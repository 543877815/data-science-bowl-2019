{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/reduce_test.csv\n",
      "data/reduce_train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot\n",
    "import shap\n",
    "\n",
    "base_dir = 'data/'\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(base_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Objective\n",
    "\n",
    "* In the last notebook we create our baseline model including a feature selection part. \n",
    "* Cohen cappa score of 0.456 (lb) with a local cv score of 0.529\n",
    "* In this notebook we are going to add more features and remove others that i think they overfitt the train set and then check if our local cv score improve.\n",
    "* Next, we will check if this improvement aligns with the lb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* Check the distribution of the target variable of the out of folds score and the prediction distribution. A good model should more or less have the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(reduce_train['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(reduce_train)\n",
    "    reduce_train['accuracy_group'].hist()\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n",
    "\n",
    "    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True\n",
    "\n",
    "def cohenkappa(ypred, y):\n",
    "    y = y.get_label().astype(\"int\")\n",
    "    ypred = ypred.reshape((4, -1)).argmax(axis = 0)\n",
    "    loss = cohenkappascore(y, y_pred, weights = 'quadratic')\n",
    "    return \"cappa\", loss, True\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod(\n",
    "            (datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv(base_dir + 'train.csv')\n",
    "    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv(base_dir + 'test.csv')\n",
    "    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv(base_dir + 'train_labels.csv')\n",
    "    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "\n",
    "    print('Reading specs.csv file....')\n",
    "    specs = pd.read_csv(base_dir + 'specs.csv')\n",
    "    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n",
    "\n",
    "    print('Reading sample_submission.csv file....')\n",
    "    sample_submission = pd.read_csv(base_dir + 'sample_submission.csv')\n",
    "    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n",
    "    return train, test, train_labels, specs, sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    \n",
    "    \n",
    "    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code\n",
    "\n",
    "media_sequence = {'Welcome to Lost Lagoon!': 19, 'Tree Top City - Level 1':17, 'Ordering Spheres':61, 'Costume Box':61,'12 Monkeys':109, 'Tree Top City - Level 2':25, \"Pirate's Tale\":80, \n",
    "                  'Treasure Map':156, 'Tree Top City - Level 3':26, 'Rulers':126, 'Magma Peak - Level 1':20, 'Slop Problem':60, 'Magma Peak - Level 2':22, 'Crystal Caves - Level 1':18, \n",
    "                  'Balancing Act':72,'Lifting Heavy Things':118, 'Crystal Caves - Level 2':24, 'Honey Cake':142, 'Crystal Caves - Level 3':19, 'Heavy, Heavier, Heaviest':61}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function that convert the raw data into processed features\n",
    "def get_data(user_sample, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "    \n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    user_clip_time = []\n",
    "    user_activity_time = []\n",
    "    user_game_time = []\n",
    "    \n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n",
    "    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n",
    "    time_spent_each_act = {t+\"_time\": 0 for t in list_of_user_activities}\n",
    "        \n",
    "    # last features\n",
    "    sessions_count = 0\n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        \n",
    "        if(session_type=='Clip'):\n",
    "            time_spent = media_sequence[session_title_text]\n",
    "            user_clip_time.append(time_spent)\n",
    "            time_spent_each_act[activities_labels[session_title] + \"_time\"] += time_spent\n",
    "            \n",
    "        elif(session_type=='Activity'):\n",
    "            time_spent = (session.iloc[-1, 2] - session.iloc[0, 2]).seconds\n",
    "            user_activity_time.append(time_spent)\n",
    "            time_spent_each_act[activities_labels[session_title] + \"_time\"] += time_spent\n",
    "        \n",
    "        elif (session_type == 'Game'):\n",
    "            time_spent = (session.iloc[-1, 2] - session.iloc[0, 2]).seconds\n",
    "            user_game_time.append(time_spent)\n",
    "            time_spent_each_act[activities_labels[session_title] + \"_time\"] += time_spent\n",
    "                    \n",
    "            \n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens: \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(title_event_code_count.copy())\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(time_spent_each_act.copy())\n",
    "            features['installation_session_count'] = sessions_count\n",
    "            \n",
    "            variety_features = [('var_event_code', event_code_count),\n",
    "                              ('var_event_id', event_id_count),\n",
    "                               ('var_title', title_count),\n",
    "                               ('var_title_event_code', title_event_code_count)]\n",
    "            \n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "                 \n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0]\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)   \n",
    "                \n",
    "            if user_clip_time == []:\n",
    "                features['clip_time_mean'] = 0\n",
    "            else:\n",
    "                features['clip_time_mean'] = np.mean(user_clip_time)\n",
    "            if user_activity_time == []:\n",
    "                features['activity_time_mean'] = 0\n",
    "            else:\n",
    "                features['activity_time_mean'] = np.mean(user_activity_time)\n",
    "            if user_game_time == []:\n",
    "                features['game_time_mean'] = 0\n",
    "            else:\n",
    "                features['game_time_mean'] = np.mean(user_game_time)\n",
    "                \n",
    "            time_spent = (session.iloc[-1, 2] - session.iloc[0, 2]).seconds\n",
    "            time_spent_each_act[activities_labels[session_title] + \"_time\"] += time_spent\n",
    "            durations.append(time_spent)\n",
    "            # the accurace is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            \n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "        \n",
    "        sessions_count += 1\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = activities_labels[k]\n",
    "                    counter[x] += num_of_session_count[k]\n",
    "                return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type \n",
    "                        \n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(train, test):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n",
    "        compiled_train += get_data(user_sample)\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n",
    "        test_data = get_data(user_sample, test_set = True)\n",
    "        compiled_test.append(test_data)\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    categoricals = ['session_title']\n",
    "    return reduce_train, reduce_test, categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(reduce_train, reduce_test):\n",
    "    for df in [reduce_train, reduce_test]:\n",
    "        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n",
    "        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n",
    "        #df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n",
    "        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n",
    "        \n",
    "        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n",
    "                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n",
    "                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n",
    "        \n",
    "        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n",
    "        #df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n",
    "        \n",
    "    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "    features = [x for x in features if x not in ['accuracy_group', 'installation_id']] + ['acc_' + title for title in assess_titles]\n",
    "   \n",
    "    return reduce_train, reduce_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train, test, train_labels, specs, sample_submission = read_data()\n",
    "# get usefull dict with maping encode\n",
    "train, test, train_labels, win_code, list_of_user_activities, \\\n",
    "list_of_event_code, activities_labels, assess_titles, list_of_event_id,\\\n",
    "all_title_event_code = encode_title(train, test, train_labels)\n",
    "# tranform function to get the train and test set\n",
    "if not os.path.exists(base_dir + 'reduce_train.csv'):\n",
    "    reduce_train, reduce_test, categoricals = get_train_and_test(train, test)\n",
    "    reduce_train.to_csv(base_dir + 'reduce_train.csv')\n",
    "    reduce_test.to_csv(base_dir + 'reduce_test.csv')\n",
    "    print(reduce_train.shape, reduce_test.shape, categoricals)\n",
    "else:\n",
    "    reduce_train = pd.read_csv(base_dir + 'reduce_train.csv')\n",
    "    reduce_test = pd.read_csv(base_dir + 'reduce_test.csv')\n",
    "    categoricals = 'session_title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call feature engineering function\n",
    "# reduce_train, reduce_test, features = preprocess(reduce_train, reduce_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call feature engineering function\n",
    "features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "features = [x for x in features if x not in ['accuracy_group', 'installation_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "to_remove = []\n",
    "for feat_a in features:\n",
    "    for feat_b in features:\n",
    "        if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
    "#             c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]   \n",
    "            c = np.abs(np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1])  \n",
    "            if c > 0.995:  # 相关性\n",
    "                counter += 1\n",
    "                to_remove.append(feat_b)\n",
    "                print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude = [] \n",
    "ajusted_test = reduce_test.copy()\n",
    "for feature in ajusted_test.columns:\n",
    "    if feature not in ['accuracy_group', 'installation_id', 'session_title']:\n",
    "        data = reduce_train[feature]\n",
    "        train_mean = data.mean()\n",
    "        data = ajusted_test[feature] \n",
    "        test_mean = data.mean()\n",
    "        try:\n",
    "            error = stract_hists(feature, adjust=True)\n",
    "            ajust_factor = train_mean / test_mean\n",
    "            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "                to_exclude.append(feature)\n",
    "                print(feature, train_mean, test_mean, error)\n",
    "            else:\n",
    "                ajusted_test[feature] *= ajust_factor\n",
    "        except:\n",
    "            to_exclude.append(feature)\n",
    "            print(feature, train_mean, test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in features if x not in (to_exclude + to_remove)]\n",
    "reduce_train[features].shape, ajusted_test.shape, len(features), categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | max_depth | min_da... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0228\tvalid_1's rmse: 1.05575\n",
      "[200]\ttraining's rmse: 0.950709\tvalid_1's rmse: 1.01151\n",
      "[300]\ttraining's rmse: 0.916753\tvalid_1's rmse: 0.997894\n",
      "[400]\ttraining's rmse: 0.893604\tvalid_1's rmse: 0.991501\n",
      "[500]\ttraining's rmse: 0.875741\tvalid_1's rmse: 0.989307\n",
      "[600]\ttraining's rmse: 0.861306\tvalid_1's rmse: 0.988479\n",
      "[700]\ttraining's rmse: 0.848213\tvalid_1's rmse: 0.988335\n",
      "Early stopping, best iteration is:\n",
      "[660]\ttraining's rmse: 0.852963\tvalid_1's rmse: 0.988214\n",
      "Partial score of fold 0 is: 0.5811144763171063\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02257\tvalid_1's rmse: 1.05066\n",
      "[200]\ttraining's rmse: 0.952284\tvalid_1's rmse: 1.00539\n",
      "[300]\ttraining's rmse: 0.91753\tvalid_1's rmse: 0.99168\n",
      "[400]\ttraining's rmse: 0.894794\tvalid_1's rmse: 0.986001\n",
      "[500]\ttraining's rmse: 0.87705\tvalid_1's rmse: 0.982344\n",
      "[600]\ttraining's rmse: 0.863144\tvalid_1's rmse: 0.980633\n",
      "[700]\ttraining's rmse: 0.850854\tvalid_1's rmse: 0.979515\n",
      "[800]\ttraining's rmse: 0.838726\tvalid_1's rmse: 0.978921\n",
      "[900]\ttraining's rmse: 0.828873\tvalid_1's rmse: 0.978867\n",
      "[1000]\ttraining's rmse: 0.819575\tvalid_1's rmse: 0.97888\n",
      "[1100]\ttraining's rmse: 0.81055\tvalid_1's rmse: 0.978751\n",
      "Early stopping, best iteration is:\n",
      "[1051]\ttraining's rmse: 0.814558\tvalid_1's rmse: 0.978599\n",
      "Partial score of fold 1 is: 0.5968607070236824\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02535\tvalid_1's rmse: 1.05135\n",
      "[200]\ttraining's rmse: 0.954931\tvalid_1's rmse: 1.00077\n",
      "[300]\ttraining's rmse: 0.92127\tvalid_1's rmse: 0.984475\n",
      "[400]\ttraining's rmse: 0.897493\tvalid_1's rmse: 0.977492\n",
      "[500]\ttraining's rmse: 0.879737\tvalid_1's rmse: 0.974727\n",
      "[600]\ttraining's rmse: 0.865569\tvalid_1's rmse: 0.973183\n",
      "[700]\ttraining's rmse: 0.85305\tvalid_1's rmse: 0.972163\n",
      "[800]\ttraining's rmse: 0.841983\tvalid_1's rmse: 0.971295\n",
      "[900]\ttraining's rmse: 0.831034\tvalid_1's rmse: 0.970824\n",
      "[1000]\ttraining's rmse: 0.820728\tvalid_1's rmse: 0.97027\n",
      "[1100]\ttraining's rmse: 0.811181\tvalid_1's rmse: 0.97024\n",
      "Early stopping, best iteration is:\n",
      "[1048]\ttraining's rmse: 0.816105\tvalid_1's rmse: 0.970164\n",
      "Partial score of fold 2 is: 0.6142173476888857\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02369\tvalid_1's rmse: 1.04501\n",
      "[200]\ttraining's rmse: 0.953513\tvalid_1's rmse: 0.994993\n",
      "[300]\ttraining's rmse: 0.918623\tvalid_1's rmse: 0.979362\n",
      "[400]\ttraining's rmse: 0.89364\tvalid_1's rmse: 0.972497\n",
      "[500]\ttraining's rmse: 0.875136\tvalid_1's rmse: 0.968766\n",
      "[600]\ttraining's rmse: 0.860441\tvalid_1's rmse: 0.966962\n",
      "[700]\ttraining's rmse: 0.847973\tvalid_1's rmse: 0.966214\n",
      "[800]\ttraining's rmse: 0.836995\tvalid_1's rmse: 0.965842\n",
      "[900]\ttraining's rmse: 0.826802\tvalid_1's rmse: 0.965283\n",
      "[1000]\ttraining's rmse: 0.817235\tvalid_1's rmse: 0.964861\n",
      "[1100]\ttraining's rmse: 0.808102\tvalid_1's rmse: 0.964528\n",
      "[1200]\ttraining's rmse: 0.799469\tvalid_1's rmse: 0.96429\n",
      "[1300]\ttraining's rmse: 0.790985\tvalid_1's rmse: 0.964275\n",
      "Early stopping, best iteration is:\n",
      "[1269]\ttraining's rmse: 0.793635\tvalid_1's rmse: 0.964096\n",
      "Partial score of fold 3 is: 0.612428003290411\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0237\tvalid_1's rmse: 1.04671\n",
      "[200]\ttraining's rmse: 0.953391\tvalid_1's rmse: 0.999045\n",
      "[300]\ttraining's rmse: 0.919685\tvalid_1's rmse: 0.984357\n",
      "[400]\ttraining's rmse: 0.895508\tvalid_1's rmse: 0.978277\n",
      "[500]\ttraining's rmse: 0.877719\tvalid_1's rmse: 0.97501\n",
      "[600]\ttraining's rmse: 0.863683\tvalid_1's rmse: 0.973897\n",
      "[700]\ttraining's rmse: 0.85087\tvalid_1's rmse: 0.973183\n",
      "[800]\ttraining's rmse: 0.838413\tvalid_1's rmse: 0.973059\n",
      "[900]\ttraining's rmse: 0.827153\tvalid_1's rmse: 0.972198\n",
      "[1000]\ttraining's rmse: 0.817243\tvalid_1's rmse: 0.972216\n",
      "Early stopping, best iteration is:\n",
      "[954]\ttraining's rmse: 0.821577\tvalid_1's rmse: 0.972077\n",
      "Partial score of fold 4 is: 0.5997138868385467\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6008  \u001b[0m | \u001b[0m 0.9013  \u001b[0m | \u001b[0m 0.9217  \u001b[0m | \u001b[0m 1.261   \u001b[0m | \u001b[0m 3.319   \u001b[0m | \u001b[0m 6.05    \u001b[0m | \u001b[0m 55.8    \u001b[0m | \u001b[0m 82.42   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03452\tvalid_1's rmse: 1.06386\n",
      "[200]\ttraining's rmse: 0.959471\tvalid_1's rmse: 1.01512\n",
      "[300]\ttraining's rmse: 0.918629\tvalid_1's rmse: 0.997917\n",
      "[400]\ttraining's rmse: 0.89052\tvalid_1's rmse: 0.991058\n",
      "[500]\ttraining's rmse: 0.868181\tvalid_1's rmse: 0.988576\n",
      "[600]\ttraining's rmse: 0.850211\tvalid_1's rmse: 0.987833\n",
      "Early stopping, best iteration is:\n",
      "[583]\ttraining's rmse: 0.853127\tvalid_1's rmse: 0.987715\n",
      "Partial score of fold 0 is: 0.5857667717531403\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03589\tvalid_1's rmse: 1.05982\n",
      "[200]\ttraining's rmse: 0.96114\tvalid_1's rmse: 1.00918\n",
      "[300]\ttraining's rmse: 0.921021\tvalid_1's rmse: 0.990869\n",
      "[400]\ttraining's rmse: 0.893747\tvalid_1's rmse: 0.984556\n",
      "[500]\ttraining's rmse: 0.871754\tvalid_1's rmse: 0.981717\n",
      "[600]\ttraining's rmse: 0.853244\tvalid_1's rmse: 0.980058\n",
      "[700]\ttraining's rmse: 0.837611\tvalid_1's rmse: 0.979807\n",
      "[800]\ttraining's rmse: 0.823151\tvalid_1's rmse: 0.979565\n",
      "[900]\ttraining's rmse: 0.809742\tvalid_1's rmse: 0.9794\n",
      "[1000]\ttraining's rmse: 0.797614\tvalid_1's rmse: 0.9792\n",
      "[1100]\ttraining's rmse: 0.786586\tvalid_1's rmse: 0.97918\n",
      "Early stopping, best iteration is:\n",
      "[1036]\ttraining's rmse: 0.793303\tvalid_1's rmse: 0.978937\n",
      "Partial score of fold 1 is: 0.6018708713394112\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0455\tvalid_1's rmse: 1.06987\n",
      "[200]\ttraining's rmse: 0.963967\tvalid_1's rmse: 1.00893\n",
      "[300]\ttraining's rmse: 0.922843\tvalid_1's rmse: 0.987544\n",
      "[400]\ttraining's rmse: 0.894862\tvalid_1's rmse: 0.978955\n",
      "[500]\ttraining's rmse: 0.873014\tvalid_1's rmse: 0.974422\n",
      "[600]\ttraining's rmse: 0.854198\tvalid_1's rmse: 0.972457\n",
      "[700]\ttraining's rmse: 0.838003\tvalid_1's rmse: 0.971717\n",
      "[800]\ttraining's rmse: 0.82371\tvalid_1's rmse: 0.971359\n",
      "[900]\ttraining's rmse: 0.810123\tvalid_1's rmse: 0.971536\n",
      "Early stopping, best iteration is:\n",
      "[809]\ttraining's rmse: 0.82218\tvalid_1's rmse: 0.97132\n",
      "Partial score of fold 2 is: 0.6131437410498009\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03767\tvalid_1's rmse: 1.05397\n",
      "[200]\ttraining's rmse: 0.963345\tvalid_1's rmse: 0.999614\n",
      "[300]\ttraining's rmse: 0.923116\tvalid_1's rmse: 0.979261\n",
      "[400]\ttraining's rmse: 0.895838\tvalid_1's rmse: 0.970794\n",
      "[500]\ttraining's rmse: 0.874562\tvalid_1's rmse: 0.96672\n",
      "[600]\ttraining's rmse: 0.856881\tvalid_1's rmse: 0.964967\n",
      "[700]\ttraining's rmse: 0.840439\tvalid_1's rmse: 0.964012\n",
      "[800]\ttraining's rmse: 0.826046\tvalid_1's rmse: 0.963522\n",
      "[900]\ttraining's rmse: 0.811977\tvalid_1's rmse: 0.963143\n",
      "[1000]\ttraining's rmse: 0.799576\tvalid_1's rmse: 0.962543\n",
      "Early stopping, best iteration is:\n",
      "[994]\ttraining's rmse: 0.800232\tvalid_1's rmse: 0.962441\n",
      "Partial score of fold 3 is: 0.6188696431249194\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03613\tvalid_1's rmse: 1.05661\n",
      "[200]\ttraining's rmse: 0.961924\tvalid_1's rmse: 1.00498\n",
      "[300]\ttraining's rmse: 0.921975\tvalid_1's rmse: 0.985592\n",
      "[400]\ttraining's rmse: 0.89465\tvalid_1's rmse: 0.97761\n",
      "[500]\ttraining's rmse: 0.87297\tvalid_1's rmse: 0.973901\n",
      "[600]\ttraining's rmse: 0.854182\tvalid_1's rmse: 0.971676\n",
      "[700]\ttraining's rmse: 0.838789\tvalid_1's rmse: 0.970848\n",
      "[800]\ttraining's rmse: 0.824949\tvalid_1's rmse: 0.970787\n",
      "Early stopping, best iteration is:\n",
      "[736]\ttraining's rmse: 0.833634\tvalid_1's rmse: 0.970685\n",
      "Partial score of fold 4 is: 0.6025775478314298\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.6038  \u001b[0m | \u001b[95m 0.7448  \u001b[0m | \u001b[95m 0.733   \u001b[0m | \u001b[95m 3.166   \u001b[0m | \u001b[95m 5.289   \u001b[0m | \u001b[95m 11.87   \u001b[0m | \u001b[95m 122.1   \u001b[0m | \u001b[95m 74.07   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 1.09511\tvalid_1's rmse: 1.10331\n",
      "[200]\ttraining's rmse: 1.04427\tvalid_1's rmse: 1.05713\n",
      "[300]\ttraining's rmse: 1.02145\tvalid_1's rmse: 1.0377\n",
      "[400]\ttraining's rmse: 1.00868\tvalid_1's rmse: 1.02732\n",
      "[500]\ttraining's rmse: 0.99957\tvalid_1's rmse: 1.01988\n",
      "[600]\ttraining's rmse: 0.992803\tvalid_1's rmse: 1.01461\n",
      "[700]\ttraining's rmse: 0.987154\tvalid_1's rmse: 1.01087\n",
      "[800]\ttraining's rmse: 0.982386\tvalid_1's rmse: 1.00773\n",
      "[900]\ttraining's rmse: 0.978271\tvalid_1's rmse: 1.00556\n",
      "[1000]\ttraining's rmse: 0.974534\tvalid_1's rmse: 1.00369\n",
      "[1100]\ttraining's rmse: 0.971155\tvalid_1's rmse: 1.00222\n",
      "[1200]\ttraining's rmse: 0.967994\tvalid_1's rmse: 1.00107\n",
      "[1300]\ttraining's rmse: 0.965055\tvalid_1's rmse: 1.0003\n",
      "[1400]\ttraining's rmse: 0.962343\tvalid_1's rmse: 0.999496\n",
      "[1500]\ttraining's rmse: 0.959709\tvalid_1's rmse: 0.998687\n",
      "[1600]\ttraining's rmse: 0.957221\tvalid_1's rmse: 0.997949\n",
      "[1700]\ttraining's rmse: 0.954767\tvalid_1's rmse: 0.997438\n",
      "[1800]\ttraining's rmse: 0.952475\tvalid_1's rmse: 0.996977\n",
      "[1900]\ttraining's rmse: 0.950247\tvalid_1's rmse: 0.996588\n",
      "[2000]\ttraining's rmse: 0.948193\tvalid_1's rmse: 0.996158\n",
      "[2100]\ttraining's rmse: 0.946142\tvalid_1's rmse: 0.995664\n",
      "[2200]\ttraining's rmse: 0.944149\tvalid_1's rmse: 0.995408\n",
      "[2300]\ttraining's rmse: 0.942151\tvalid_1's rmse: 0.995173\n",
      "[2400]\ttraining's rmse: 0.940277\tvalid_1's rmse: 0.995102\n",
      "[2500]\ttraining's rmse: 0.938351\tvalid_1's rmse: 0.994729\n",
      "[2600]\ttraining's rmse: 0.936581\tvalid_1's rmse: 0.994536\n",
      "[2700]\ttraining's rmse: 0.934836\tvalid_1's rmse: 0.994463\n",
      "[2800]\ttraining's rmse: 0.933064\tvalid_1's rmse: 0.994276\n",
      "[2900]\ttraining's rmse: 0.93138\tvalid_1's rmse: 0.994319\n",
      "Early stopping, best iteration is:\n",
      "[2806]\ttraining's rmse: 0.932972\tvalid_1's rmse: 0.994238\n",
      "Partial score of fold 0 is: 0.5757464431216828\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.09661\tvalid_1's rmse: 1.10128\n",
      "[200]\ttraining's rmse: 1.04567\tvalid_1's rmse: 1.05345\n",
      "[300]\ttraining's rmse: 1.02271\tvalid_1's rmse: 1.03342\n",
      "[400]\ttraining's rmse: 1.00997\tvalid_1's rmse: 1.02295\n",
      "[500]\ttraining's rmse: 1.00077\tvalid_1's rmse: 1.01559\n",
      "[600]\ttraining's rmse: 0.993618\tvalid_1's rmse: 1.01055\n",
      "[700]\ttraining's rmse: 0.987892\tvalid_1's rmse: 1.00671\n",
      "[800]\ttraining's rmse: 0.983163\tvalid_1's rmse: 1.00404\n",
      "[900]\ttraining's rmse: 0.97902\tvalid_1's rmse: 1.00189\n",
      "[1000]\ttraining's rmse: 0.975314\tvalid_1's rmse: 0.99998\n",
      "[1100]\ttraining's rmse: 0.972006\tvalid_1's rmse: 0.998526\n",
      "[1200]\ttraining's rmse: 0.968944\tvalid_1's rmse: 0.997201\n",
      "[1300]\ttraining's rmse: 0.96599\tvalid_1's rmse: 0.995876\n",
      "[1400]\ttraining's rmse: 0.963322\tvalid_1's rmse: 0.994744\n",
      "[1500]\ttraining's rmse: 0.960775\tvalid_1's rmse: 0.993849\n",
      "[1600]\ttraining's rmse: 0.958312\tvalid_1's rmse: 0.993005\n",
      "[1700]\ttraining's rmse: 0.956032\tvalid_1's rmse: 0.992384\n",
      "[1800]\ttraining's rmse: 0.953857\tvalid_1's rmse: 0.991736\n",
      "[1900]\ttraining's rmse: 0.951739\tvalid_1's rmse: 0.99105\n",
      "[2000]\ttraining's rmse: 0.949653\tvalid_1's rmse: 0.990571\n",
      "[2100]\ttraining's rmse: 0.94757\tvalid_1's rmse: 0.990118\n",
      "[2200]\ttraining's rmse: 0.945595\tvalid_1's rmse: 0.989688\n",
      "[2300]\ttraining's rmse: 0.943688\tvalid_1's rmse: 0.989152\n",
      "[2400]\ttraining's rmse: 0.941778\tvalid_1's rmse: 0.988889\n",
      "[2500]\ttraining's rmse: 0.939945\tvalid_1's rmse: 0.988687\n",
      "[2600]\ttraining's rmse: 0.938113\tvalid_1's rmse: 0.988326\n",
      "[2700]\ttraining's rmse: 0.936348\tvalid_1's rmse: 0.987968\n",
      "[2800]\ttraining's rmse: 0.934624\tvalid_1's rmse: 0.987698\n",
      "[2900]\ttraining's rmse: 0.932953\tvalid_1's rmse: 0.987568\n",
      "[3000]\ttraining's rmse: 0.931318\tvalid_1's rmse: 0.987483\n",
      "[3100]\ttraining's rmse: 0.92967\tvalid_1's rmse: 0.987145\n",
      "[3200]\ttraining's rmse: 0.928083\tvalid_1's rmse: 0.987021\n",
      "[3300]\ttraining's rmse: 0.9266\tvalid_1's rmse: 0.986891\n",
      "[3400]\ttraining's rmse: 0.925077\tvalid_1's rmse: 0.986656\n",
      "[3500]\ttraining's rmse: 0.923582\tvalid_1's rmse: 0.986494\n",
      "[3600]\ttraining's rmse: 0.922078\tvalid_1's rmse: 0.986264\n",
      "Early stopping, best iteration is:\n",
      "[3586]\ttraining's rmse: 0.922295\tvalid_1's rmse: 0.986241\n",
      "Partial score of fold 1 is: 0.5932820182267333\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.09682\tvalid_1's rmse: 1.10188\n",
      "[200]\ttraining's rmse: 1.04625\tvalid_1's rmse: 1.05154\n",
      "[300]\ttraining's rmse: 1.02397\tvalid_1's rmse: 1.02961\n",
      "[400]\ttraining's rmse: 1.01122\tvalid_1's rmse: 1.01839\n",
      "[500]\ttraining's rmse: 1.00186\tvalid_1's rmse: 1.01084\n",
      "[600]\ttraining's rmse: 0.99482\tvalid_1's rmse: 1.00585\n",
      "[700]\ttraining's rmse: 0.989089\tvalid_1's rmse: 1.00183\n",
      "[800]\ttraining's rmse: 0.984355\tvalid_1's rmse: 0.999079\n",
      "[900]\ttraining's rmse: 0.980172\tvalid_1's rmse: 0.997004\n",
      "[1000]\ttraining's rmse: 0.97656\tvalid_1's rmse: 0.995259\n",
      "[1100]\ttraining's rmse: 0.973203\tvalid_1's rmse: 0.99376\n",
      "[1200]\ttraining's rmse: 0.970192\tvalid_1's rmse: 0.992768\n",
      "[1300]\ttraining's rmse: 0.967328\tvalid_1's rmse: 0.991702\n",
      "[1400]\ttraining's rmse: 0.964574\tvalid_1's rmse: 0.990728\n",
      "[1500]\ttraining's rmse: 0.962093\tvalid_1's rmse: 0.990116\n",
      "[1600]\ttraining's rmse: 0.959594\tvalid_1's rmse: 0.989415\n",
      "[1700]\ttraining's rmse: 0.957293\tvalid_1's rmse: 0.988845\n",
      "[1800]\ttraining's rmse: 0.954982\tvalid_1's rmse: 0.988297\n",
      "[1900]\ttraining's rmse: 0.95283\tvalid_1's rmse: 0.987697\n",
      "[2000]\ttraining's rmse: 0.950779\tvalid_1's rmse: 0.987135\n",
      "[2100]\ttraining's rmse: 0.948722\tvalid_1's rmse: 0.98668\n",
      "[2200]\ttraining's rmse: 0.946712\tvalid_1's rmse: 0.98636\n",
      "[2300]\ttraining's rmse: 0.944826\tvalid_1's rmse: 0.985929\n",
      "[2400]\ttraining's rmse: 0.942919\tvalid_1's rmse: 0.985611\n",
      "[2500]\ttraining's rmse: 0.941034\tvalid_1's rmse: 0.985231\n",
      "[2600]\ttraining's rmse: 0.93923\tvalid_1's rmse: 0.985027\n",
      "[2700]\ttraining's rmse: 0.937453\tvalid_1's rmse: 0.984782\n",
      "[2800]\ttraining's rmse: 0.935736\tvalid_1's rmse: 0.984605\n",
      "[2900]\ttraining's rmse: 0.934051\tvalid_1's rmse: 0.984411\n",
      "[3000]\ttraining's rmse: 0.932424\tvalid_1's rmse: 0.984169\n",
      "[3100]\ttraining's rmse: 0.930738\tvalid_1's rmse: 0.983859\n",
      "[3200]\ttraining's rmse: 0.929186\tvalid_1's rmse: 0.983618\n",
      "[3300]\ttraining's rmse: 0.927697\tvalid_1's rmse: 0.983518\n",
      "[3400]\ttraining's rmse: 0.926179\tvalid_1's rmse: 0.983342\n",
      "[3500]\ttraining's rmse: 0.924648\tvalid_1's rmse: 0.983252\n",
      "[3600]\ttraining's rmse: 0.923142\tvalid_1's rmse: 0.98315\n",
      "[3700]\ttraining's rmse: 0.921687\tvalid_1's rmse: 0.983042\n",
      "[3800]\ttraining's rmse: 0.92021\tvalid_1's rmse: 0.982787\n",
      "[3900]\ttraining's rmse: 0.918769\tvalid_1's rmse: 0.982638\n",
      "[4000]\ttraining's rmse: 0.917351\tvalid_1's rmse: 0.982495\n",
      "[4100]\ttraining's rmse: 0.915902\tvalid_1's rmse: 0.982265\n",
      "[4200]\ttraining's rmse: 0.914559\tvalid_1's rmse: 0.98218\n",
      "[4300]\ttraining's rmse: 0.913179\tvalid_1's rmse: 0.982068\n",
      "[4400]\ttraining's rmse: 0.911776\tvalid_1's rmse: 0.981994\n",
      "[4500]\ttraining's rmse: 0.910467\tvalid_1's rmse: 0.981866\n",
      "[4600]\ttraining's rmse: 0.909195\tvalid_1's rmse: 0.981709\n",
      "[4700]\ttraining's rmse: 0.907942\tvalid_1's rmse: 0.981555\n",
      "[4800]\ttraining's rmse: 0.906681\tvalid_1's rmse: 0.981376\n",
      "[4900]\ttraining's rmse: 0.905421\tvalid_1's rmse: 0.981373\n",
      "Early stopping, best iteration is:\n",
      "[4807]\ttraining's rmse: 0.9066\tvalid_1's rmse: 0.981352\n",
      "Partial score of fold 2 is: 0.606523166775445\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.09867\tvalid_1's rmse: 1.09456\n",
      "[200]\ttraining's rmse: 1.04881\tvalid_1's rmse: 1.04363\n",
      "[300]\ttraining's rmse: 1.02597\tvalid_1's rmse: 1.02136\n",
      "[400]\ttraining's rmse: 1.01279\tvalid_1's rmse: 1.0106\n",
      "[500]\ttraining's rmse: 1.00342\tvalid_1's rmse: 1.00354\n",
      "[600]\ttraining's rmse: 0.996396\tvalid_1's rmse: 0.998426\n",
      "[700]\ttraining's rmse: 0.990828\tvalid_1's rmse: 0.9946\n",
      "[800]\ttraining's rmse: 0.986141\tvalid_1's rmse: 0.99164\n",
      "[900]\ttraining's rmse: 0.982001\tvalid_1's rmse: 0.988952\n",
      "[1000]\ttraining's rmse: 0.978483\tvalid_1's rmse: 0.987056\n",
      "[1100]\ttraining's rmse: 0.975075\tvalid_1's rmse: 0.985459\n",
      "[1200]\ttraining's rmse: 0.972138\tvalid_1's rmse: 0.984158\n",
      "[1300]\ttraining's rmse: 0.96924\tvalid_1's rmse: 0.982853\n",
      "[1400]\ttraining's rmse: 0.966561\tvalid_1's rmse: 0.981642\n",
      "[1500]\ttraining's rmse: 0.964042\tvalid_1's rmse: 0.980641\n",
      "[1600]\ttraining's rmse: 0.961604\tvalid_1's rmse: 0.979793\n",
      "[1700]\ttraining's rmse: 0.959221\tvalid_1's rmse: 0.979026\n",
      "[1800]\ttraining's rmse: 0.956951\tvalid_1's rmse: 0.978298\n",
      "[1900]\ttraining's rmse: 0.954724\tvalid_1's rmse: 0.977593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\ttraining's rmse: 0.9526\tvalid_1's rmse: 0.976782\n",
      "[2100]\ttraining's rmse: 0.950566\tvalid_1's rmse: 0.976147\n",
      "[2200]\ttraining's rmse: 0.948567\tvalid_1's rmse: 0.975615\n",
      "[2300]\ttraining's rmse: 0.946665\tvalid_1's rmse: 0.975043\n",
      "[2400]\ttraining's rmse: 0.944901\tvalid_1's rmse: 0.974612\n",
      "[2500]\ttraining's rmse: 0.943021\tvalid_1's rmse: 0.974135\n",
      "[2600]\ttraining's rmse: 0.941249\tvalid_1's rmse: 0.973577\n",
      "[2700]\ttraining's rmse: 0.939509\tvalid_1's rmse: 0.973134\n",
      "[2800]\ttraining's rmse: 0.937834\tvalid_1's rmse: 0.972682\n",
      "[2900]\ttraining's rmse: 0.936121\tvalid_1's rmse: 0.97233\n",
      "[3000]\ttraining's rmse: 0.934486\tvalid_1's rmse: 0.972006\n",
      "[3100]\ttraining's rmse: 0.932844\tvalid_1's rmse: 0.971763\n",
      "[3200]\ttraining's rmse: 0.931234\tvalid_1's rmse: 0.971527\n",
      "[3300]\ttraining's rmse: 0.929563\tvalid_1's rmse: 0.971265\n",
      "[3400]\ttraining's rmse: 0.928074\tvalid_1's rmse: 0.971098\n",
      "[3500]\ttraining's rmse: 0.926544\tvalid_1's rmse: 0.970847\n",
      "[3600]\ttraining's rmse: 0.925023\tvalid_1's rmse: 0.970681\n",
      "[3700]\ttraining's rmse: 0.923551\tvalid_1's rmse: 0.970529\n",
      "[3800]\ttraining's rmse: 0.922146\tvalid_1's rmse: 0.970446\n",
      "[3900]\ttraining's rmse: 0.920783\tvalid_1's rmse: 0.970114\n",
      "[4000]\ttraining's rmse: 0.919373\tvalid_1's rmse: 0.969894\n",
      "[4100]\ttraining's rmse: 0.917977\tvalid_1's rmse: 0.969629\n",
      "[4200]\ttraining's rmse: 0.916593\tvalid_1's rmse: 0.969428\n",
      "[4300]\ttraining's rmse: 0.915159\tvalid_1's rmse: 0.969171\n",
      "[4400]\ttraining's rmse: 0.913871\tvalid_1's rmse: 0.968945\n",
      "[4500]\ttraining's rmse: 0.912546\tvalid_1's rmse: 0.968847\n",
      "[4600]\ttraining's rmse: 0.91124\tvalid_1's rmse: 0.968651\n",
      "[4700]\ttraining's rmse: 0.909945\tvalid_1's rmse: 0.968532\n",
      "[4800]\ttraining's rmse: 0.908587\tvalid_1's rmse: 0.96827\n",
      "[4900]\ttraining's rmse: 0.90733\tvalid_1's rmse: 0.968078\n",
      "[5000]\ttraining's rmse: 0.906097\tvalid_1's rmse: 0.96797\n",
      "[5100]\ttraining's rmse: 0.904744\tvalid_1's rmse: 0.967693\n",
      "[5200]\ttraining's rmse: 0.903529\tvalid_1's rmse: 0.967558\n",
      "[5300]\ttraining's rmse: 0.90234\tvalid_1's rmse: 0.967346\n",
      "[5400]\ttraining's rmse: 0.901096\tvalid_1's rmse: 0.967182\n",
      "[5500]\ttraining's rmse: 0.899899\tvalid_1's rmse: 0.967186\n",
      "Early stopping, best iteration is:\n",
      "[5427]\ttraining's rmse: 0.900756\tvalid_1's rmse: 0.967103\n",
      "Partial score of fold 3 is: 0.6061652978957501\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.09767\tvalid_1's rmse: 1.09563\n",
      "[200]\ttraining's rmse: 1.04652\tvalid_1's rmse: 1.0465\n",
      "[300]\ttraining's rmse: 1.02377\tvalid_1's rmse: 1.02542\n",
      "[400]\ttraining's rmse: 1.011\tvalid_1's rmse: 1.01469\n",
      "[500]\ttraining's rmse: 1.0019\tvalid_1's rmse: 1.00779\n",
      "[600]\ttraining's rmse: 0.994807\tvalid_1's rmse: 1.00301\n",
      "[700]\ttraining's rmse: 0.989152\tvalid_1's rmse: 0.999486\n",
      "[800]\ttraining's rmse: 0.984304\tvalid_1's rmse: 0.997134\n",
      "[900]\ttraining's rmse: 0.980194\tvalid_1's rmse: 0.995172\n",
      "[1000]\ttraining's rmse: 0.976608\tvalid_1's rmse: 0.993575\n",
      "[1100]\ttraining's rmse: 0.973334\tvalid_1's rmse: 0.992212\n",
      "[1200]\ttraining's rmse: 0.97021\tvalid_1's rmse: 0.991237\n",
      "[1300]\ttraining's rmse: 0.967385\tvalid_1's rmse: 0.990504\n",
      "[1400]\ttraining's rmse: 0.96474\tvalid_1's rmse: 0.989888\n",
      "[1500]\ttraining's rmse: 0.962221\tvalid_1's rmse: 0.989253\n",
      "[1600]\ttraining's rmse: 0.959769\tvalid_1's rmse: 0.988789\n",
      "[1700]\ttraining's rmse: 0.957412\tvalid_1's rmse: 0.988335\n",
      "[1800]\ttraining's rmse: 0.955197\tvalid_1's rmse: 0.987939\n",
      "[1900]\ttraining's rmse: 0.953021\tvalid_1's rmse: 0.987449\n",
      "[2000]\ttraining's rmse: 0.950896\tvalid_1's rmse: 0.986963\n",
      "[2100]\ttraining's rmse: 0.948882\tvalid_1's rmse: 0.986562\n",
      "[2200]\ttraining's rmse: 0.946935\tvalid_1's rmse: 0.986105\n",
      "[2300]\ttraining's rmse: 0.944987\tvalid_1's rmse: 0.985786\n",
      "[2400]\ttraining's rmse: 0.943114\tvalid_1's rmse: 0.98541\n",
      "[2500]\ttraining's rmse: 0.941339\tvalid_1's rmse: 0.985161\n",
      "[2600]\ttraining's rmse: 0.939589\tvalid_1's rmse: 0.984872\n",
      "[2700]\ttraining's rmse: 0.937897\tvalid_1's rmse: 0.98474\n",
      "[2800]\ttraining's rmse: 0.936241\tvalid_1's rmse: 0.984598\n",
      "[2900]\ttraining's rmse: 0.934541\tvalid_1's rmse: 0.984345\n",
      "[3000]\ttraining's rmse: 0.932917\tvalid_1's rmse: 0.984212\n",
      "[3100]\ttraining's rmse: 0.931294\tvalid_1's rmse: 0.984114\n",
      "[3200]\ttraining's rmse: 0.929736\tvalid_1's rmse: 0.983884\n",
      "[3300]\ttraining's rmse: 0.928231\tvalid_1's rmse: 0.98372\n",
      "[3400]\ttraining's rmse: 0.926675\tvalid_1's rmse: 0.983515\n",
      "[3500]\ttraining's rmse: 0.925084\tvalid_1's rmse: 0.983486\n",
      "[3600]\ttraining's rmse: 0.923537\tvalid_1's rmse: 0.983442\n",
      "[3700]\ttraining's rmse: 0.922084\tvalid_1's rmse: 0.983289\n",
      "[3800]\ttraining's rmse: 0.920647\tvalid_1's rmse: 0.983243\n",
      "[3900]\ttraining's rmse: 0.919181\tvalid_1's rmse: 0.983067\n",
      "[4000]\ttraining's rmse: 0.917805\tvalid_1's rmse: 0.982868\n",
      "[4100]\ttraining's rmse: 0.916422\tvalid_1's rmse: 0.982711\n",
      "[4200]\ttraining's rmse: 0.91506\tvalid_1's rmse: 0.982669\n",
      "[4300]\ttraining's rmse: 0.91372\tvalid_1's rmse: 0.982486\n",
      "[4400]\ttraining's rmse: 0.912392\tvalid_1's rmse: 0.982371\n",
      "[4500]\ttraining's rmse: 0.911019\tvalid_1's rmse: 0.982328\n",
      "[4600]\ttraining's rmse: 0.909624\tvalid_1's rmse: 0.982274\n",
      "[4700]\ttraining's rmse: 0.908296\tvalid_1's rmse: 0.982209\n",
      "[4800]\ttraining's rmse: 0.90701\tvalid_1's rmse: 0.982161\n",
      "Early stopping, best iteration is:\n",
      "[4761]\ttraining's rmse: 0.907524\tvalid_1's rmse: 0.982143\n",
      "Partial score of fold 4 is: 0.595597374161277\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5951  \u001b[0m | \u001b[0m 0.8835  \u001b[0m | \u001b[0m 0.7204  \u001b[0m | \u001b[0m 4.347   \u001b[0m | \u001b[0m 1.605   \u001b[0m | \u001b[0m 7.461   \u001b[0m | \u001b[0m 149.9   \u001b[0m | \u001b[0m 5.526   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02403\tvalid_1's rmse: 1.05703\n",
      "[200]\ttraining's rmse: 0.948437\tvalid_1's rmse: 1.01078\n",
      "[300]\ttraining's rmse: 0.907761\tvalid_1's rmse: 0.99607\n",
      "[400]\ttraining's rmse: 0.879668\tvalid_1's rmse: 0.99031\n",
      "[500]\ttraining's rmse: 0.857364\tvalid_1's rmse: 0.987634\n",
      "[600]\ttraining's rmse: 0.840266\tvalid_1's rmse: 0.98703\n",
      "[700]\ttraining's rmse: 0.825075\tvalid_1's rmse: 0.987338\n",
      "Early stopping, best iteration is:\n",
      "[612]\ttraining's rmse: 0.838135\tvalid_1's rmse: 0.986887\n",
      "Partial score of fold 0 is: 0.5905980016290215\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02435\tvalid_1's rmse: 1.05331\n",
      "[200]\ttraining's rmse: 0.948846\tvalid_1's rmse: 1.00546\n",
      "[300]\ttraining's rmse: 0.907977\tvalid_1's rmse: 0.990688\n",
      "[400]\ttraining's rmse: 0.880399\tvalid_1's rmse: 0.985329\n",
      "[500]\ttraining's rmse: 0.857859\tvalid_1's rmse: 0.982674\n",
      "[600]\ttraining's rmse: 0.841049\tvalid_1's rmse: 0.981746\n",
      "[700]\ttraining's rmse: 0.825533\tvalid_1's rmse: 0.981172\n",
      "[800]\ttraining's rmse: 0.811841\tvalid_1's rmse: 0.981015\n",
      "[900]\ttraining's rmse: 0.799705\tvalid_1's rmse: 0.981082\n",
      "Early stopping, best iteration is:\n",
      "[864]\ttraining's rmse: 0.803878\tvalid_1's rmse: 0.980957\n",
      "Partial score of fold 1 is: 0.5999025925010892\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0257\tvalid_1's rmse: 1.05299\n",
      "[200]\ttraining's rmse: 0.950676\tvalid_1's rmse: 1.00146\n",
      "[300]\ttraining's rmse: 0.910392\tvalid_1's rmse: 0.983514\n",
      "[400]\ttraining's rmse: 0.882078\tvalid_1's rmse: 0.976721\n",
      "[500]\ttraining's rmse: 0.859509\tvalid_1's rmse: 0.973443\n",
      "[600]\ttraining's rmse: 0.840392\tvalid_1's rmse: 0.972008\n",
      "[700]\ttraining's rmse: 0.825335\tvalid_1's rmse: 0.971558\n",
      "[800]\ttraining's rmse: 0.811234\tvalid_1's rmse: 0.971545\n",
      "[900]\ttraining's rmse: 0.798311\tvalid_1's rmse: 0.971482\n",
      "Early stopping, best iteration is:\n",
      "[831]\ttraining's rmse: 0.806821\tvalid_1's rmse: 0.971411\n",
      "Partial score of fold 2 is: 0.6117122655310212\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02703\tvalid_1's rmse: 1.04669\n",
      "[200]\ttraining's rmse: 0.95159\tvalid_1's rmse: 0.994338\n",
      "[300]\ttraining's rmse: 0.91106\tvalid_1's rmse: 0.976891\n",
      "[400]\ttraining's rmse: 0.882783\tvalid_1's rmse: 0.969806\n",
      "[500]\ttraining's rmse: 0.860231\tvalid_1's rmse: 0.966251\n",
      "[600]\ttraining's rmse: 0.842975\tvalid_1's rmse: 0.964594\n",
      "[700]\ttraining's rmse: 0.827206\tvalid_1's rmse: 0.964208\n",
      "[800]\ttraining's rmse: 0.813673\tvalid_1's rmse: 0.963915\n",
      "[900]\ttraining's rmse: 0.801327\tvalid_1's rmse: 0.963327\n",
      "[1000]\ttraining's rmse: 0.789283\tvalid_1's rmse: 0.963047\n",
      "[1100]\ttraining's rmse: 0.777726\tvalid_1's rmse: 0.962801\n",
      "Early stopping, best iteration is:\n",
      "[1086]\ttraining's rmse: 0.779256\tvalid_1's rmse: 0.962736\n",
      "Partial score of fold 3 is: 0.6185117742452246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02566\tvalid_1's rmse: 1.04875\n",
      "[200]\ttraining's rmse: 0.950348\tvalid_1's rmse: 0.999234\n",
      "[300]\ttraining's rmse: 0.909911\tvalid_1's rmse: 0.982648\n",
      "[400]\ttraining's rmse: 0.88149\tvalid_1's rmse: 0.975291\n",
      "[500]\ttraining's rmse: 0.859338\tvalid_1's rmse: 0.972054\n",
      "[600]\ttraining's rmse: 0.841411\tvalid_1's rmse: 0.971066\n",
      "[700]\ttraining's rmse: 0.825622\tvalid_1's rmse: 0.970752\n",
      "Early stopping, best iteration is:\n",
      "[692]\ttraining's rmse: 0.82691\tvalid_1's rmse: 0.970623\n",
      "Partial score of fold 4 is: 0.6050832512002027\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.6047  \u001b[0m | \u001b[95m 0.9252  \u001b[0m | \u001b[95m 0.8149  \u001b[0m | \u001b[95m 1.334   \u001b[0m | \u001b[95m 3.47    \u001b[0m | \u001b[95m 12.6    \u001b[0m | \u001b[95m 149.4   \u001b[0m | \u001b[95m 130.0   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02977\tvalid_1's rmse: 1.06004\n",
      "[200]\ttraining's rmse: 0.954559\tvalid_1's rmse: 1.01342\n",
      "[300]\ttraining's rmse: 0.912859\tvalid_1's rmse: 0.997375\n",
      "[400]\ttraining's rmse: 0.882432\tvalid_1's rmse: 0.991157\n",
      "[500]\ttraining's rmse: 0.858114\tvalid_1's rmse: 0.988402\n",
      "[600]\ttraining's rmse: 0.838261\tvalid_1's rmse: 0.988186\n",
      "Early stopping, best iteration is:\n",
      "[555]\ttraining's rmse: 0.846675\tvalid_1's rmse: 0.988047\n",
      "Partial score of fold 0 is: 0.5841563617945131\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03053\tvalid_1's rmse: 1.0569\n",
      "[200]\ttraining's rmse: 0.955431\tvalid_1's rmse: 1.00799\n",
      "[300]\ttraining's rmse: 0.913388\tvalid_1's rmse: 0.991671\n",
      "[400]\ttraining's rmse: 0.88311\tvalid_1's rmse: 0.985218\n",
      "[500]\ttraining's rmse: 0.858799\tvalid_1's rmse: 0.982354\n",
      "[600]\ttraining's rmse: 0.838745\tvalid_1's rmse: 0.98102\n",
      "[700]\ttraining's rmse: 0.821584\tvalid_1's rmse: 0.980554\n",
      "[800]\ttraining's rmse: 0.8057\tvalid_1's rmse: 0.980268\n",
      "[900]\ttraining's rmse: 0.791012\tvalid_1's rmse: 0.980357\n",
      "Early stopping, best iteration is:\n",
      "[861]\ttraining's rmse: 0.796388\tvalid_1's rmse: 0.980215\n",
      "Partial score of fold 1 is: 0.5950713626252079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03145\tvalid_1's rmse: 1.05624\n",
      "[200]\ttraining's rmse: 0.956776\tvalid_1's rmse: 1.00415\n",
      "[300]\ttraining's rmse: 0.915744\tvalid_1's rmse: 0.984858\n",
      "[400]\ttraining's rmse: 0.88596\tvalid_1's rmse: 0.976878\n",
      "[500]\ttraining's rmse: 0.861527\tvalid_1's rmse: 0.973255\n",
      "[600]\ttraining's rmse: 0.841332\tvalid_1's rmse: 0.971953\n",
      "[700]\ttraining's rmse: 0.823399\tvalid_1's rmse: 0.971801\n",
      "[800]\ttraining's rmse: 0.806887\tvalid_1's rmse: 0.971525\n",
      "Early stopping, best iteration is:\n",
      "[777]\ttraining's rmse: 0.810567\tvalid_1's rmse: 0.971461\n",
      "Partial score of fold 2 is: 0.6118911999708687\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03288\tvalid_1's rmse: 1.05128\n",
      "[200]\ttraining's rmse: 0.958127\tvalid_1's rmse: 0.997554\n",
      "[300]\ttraining's rmse: 0.916262\tvalid_1's rmse: 0.979256\n",
      "[400]\ttraining's rmse: 0.886263\tvalid_1's rmse: 0.971286\n",
      "[500]\ttraining's rmse: 0.862086\tvalid_1's rmse: 0.967632\n",
      "[600]\ttraining's rmse: 0.842186\tvalid_1's rmse: 0.965701\n",
      "[700]\ttraining's rmse: 0.824727\tvalid_1's rmse: 0.964783\n",
      "[800]\ttraining's rmse: 0.808683\tvalid_1's rmse: 0.964404\n",
      "[900]\ttraining's rmse: 0.794334\tvalid_1's rmse: 0.964218\n",
      "[1000]\ttraining's rmse: 0.780526\tvalid_1's rmse: 0.96427\n",
      "Early stopping, best iteration is:\n",
      "[910]\ttraining's rmse: 0.792864\tvalid_1's rmse: 0.964089\n",
      "Partial score of fold 3 is: 0.6165434954069026\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03181\tvalid_1's rmse: 1.0523\n",
      "[200]\ttraining's rmse: 0.956599\tvalid_1's rmse: 1.00135\n",
      "[300]\ttraining's rmse: 0.914747\tvalid_1's rmse: 0.983308\n",
      "[400]\ttraining's rmse: 0.884836\tvalid_1's rmse: 0.975779\n",
      "[500]\ttraining's rmse: 0.860606\tvalid_1's rmse: 0.972077\n",
      "[600]\ttraining's rmse: 0.840354\tvalid_1's rmse: 0.971157\n",
      "[700]\ttraining's rmse: 0.822816\tvalid_1's rmse: 0.970531\n",
      "[800]\ttraining's rmse: 0.806611\tvalid_1's rmse: 0.970452\n",
      "Early stopping, best iteration is:\n",
      "[749]\ttraining's rmse: 0.814717\tvalid_1's rmse: 0.970326\n",
      "Partial score of fold 4 is: 0.602756526643485\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6025  \u001b[0m | \u001b[0m 0.9382  \u001b[0m | \u001b[0m 0.7854  \u001b[0m | \u001b[0m 3.306   \u001b[0m | \u001b[0m 5.464   \u001b[0m | \u001b[0m 14.82   \u001b[0m | \u001b[0m 146.8   \u001b[0m | \u001b[0m 129.9   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.960438\tvalid_1's rmse: 1.04722\n",
      "[200]\ttraining's rmse: 0.842146\tvalid_1's rmse: 1.00386\n",
      "[300]\ttraining's rmse: 0.772046\tvalid_1's rmse: 0.993993\n",
      "[400]\ttraining's rmse: 0.71946\tvalid_1's rmse: 0.991049\n",
      "[500]\ttraining's rmse: 0.677539\tvalid_1's rmse: 0.991672\n",
      "Early stopping, best iteration is:\n",
      "[449]\ttraining's rmse: 0.697925\tvalid_1's rmse: 0.990859\n",
      "Partial score of fold 0 is: 0.5823670173960386\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.96273\tvalid_1's rmse: 1.04201\n",
      "[200]\ttraining's rmse: 0.844247\tvalid_1's rmse: 0.994888\n",
      "[300]\ttraining's rmse: 0.773652\tvalid_1's rmse: 0.984222\n",
      "[400]\ttraining's rmse: 0.722769\tvalid_1's rmse: 0.981689\n",
      "[500]\ttraining's rmse: 0.680818\tvalid_1's rmse: 0.980969\n",
      "Early stopping, best iteration is:\n",
      "[496]\ttraining's rmse: 0.682371\tvalid_1's rmse: 0.980886\n",
      "Partial score of fold 1 is: 0.5959660348244451\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.963901\tvalid_1's rmse: 1.03968\n",
      "[200]\ttraining's rmse: 0.846524\tvalid_1's rmse: 0.98841\n",
      "[300]\ttraining's rmse: 0.776082\tvalid_1's rmse: 0.974246\n",
      "[400]\ttraining's rmse: 0.723926\tvalid_1's rmse: 0.970967\n",
      "[500]\ttraining's rmse: 0.68207\tvalid_1's rmse: 0.970371\n",
      "[600]\ttraining's rmse: 0.646729\tvalid_1's rmse: 0.970549\n",
      "Early stopping, best iteration is:\n",
      "[542]\ttraining's rmse: 0.666455\tvalid_1's rmse: 0.970275\n",
      "Partial score of fold 2 is: 0.6084914456137671\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.964561\tvalid_1's rmse: 1.03569\n",
      "[200]\ttraining's rmse: 0.846062\tvalid_1's rmse: 0.982385\n",
      "[300]\ttraining's rmse: 0.775606\tvalid_1's rmse: 0.968937\n",
      "[400]\ttraining's rmse: 0.723347\tvalid_1's rmse: 0.965139\n",
      "[500]\ttraining's rmse: 0.681222\tvalid_1's rmse: 0.963859\n",
      "Early stopping, best iteration is:\n",
      "[488]\ttraining's rmse: 0.68591\tvalid_1's rmse: 0.963759\n",
      "Partial score of fold 3 is: 0.6126069377302585\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.963111\tvalid_1's rmse: 1.03708\n",
      "[200]\ttraining's rmse: 0.845555\tvalid_1's rmse: 0.988348\n",
      "[300]\ttraining's rmse: 0.775733\tvalid_1's rmse: 0.976215\n",
      "[400]\ttraining's rmse: 0.723424\tvalid_1's rmse: 0.972415\n",
      "[500]\ttraining's rmse: 0.681672\tvalid_1's rmse: 0.972146\n",
      "Early stopping, best iteration is:\n",
      "[469]\ttraining's rmse: 0.693786\tvalid_1's rmse: 0.971811\n",
      "Partial score of fold 4 is: 0.6043673359519819\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6007  \u001b[0m | \u001b[0m 0.9016  \u001b[0m | \u001b[0m 0.9259  \u001b[0m | \u001b[0m 0.7169  \u001b[0m | \u001b[0m 5.422   \u001b[0m | \u001b[0m 14.07   \u001b[0m | \u001b[0m 12.58   \u001b[0m | \u001b[0m 129.6   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01958\tvalid_1's rmse: 1.0563\n",
      "[200]\ttraining's rmse: 0.940495\tvalid_1's rmse: 1.00908\n",
      "[300]\ttraining's rmse: 0.897601\tvalid_1's rmse: 0.994277\n",
      "[400]\ttraining's rmse: 0.868281\tvalid_1's rmse: 0.988764\n",
      "[500]\ttraining's rmse: 0.845195\tvalid_1's rmse: 0.987539\n",
      "[600]\ttraining's rmse: 0.826898\tvalid_1's rmse: 0.987439\n",
      "[700]\ttraining's rmse: 0.810063\tvalid_1's rmse: 0.987987\n",
      "Early stopping, best iteration is:\n",
      "[612]\ttraining's rmse: 0.824843\tvalid_1's rmse: 0.987313\n",
      "Partial score of fold 0 is: 0.5897033294297842\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02133\tvalid_1's rmse: 1.05246\n",
      "[200]\ttraining's rmse: 0.942057\tvalid_1's rmse: 1.00324\n",
      "[300]\ttraining's rmse: 0.899573\tvalid_1's rmse: 0.988148\n",
      "[400]\ttraining's rmse: 0.869844\tvalid_1's rmse: 0.9827\n",
      "[500]\ttraining's rmse: 0.846332\tvalid_1's rmse: 0.980693\n",
      "[600]\ttraining's rmse: 0.827412\tvalid_1's rmse: 0.979868\n",
      "Early stopping, best iteration is:\n",
      "[590]\ttraining's rmse: 0.82905\tvalid_1's rmse: 0.979771\n",
      "Partial score of fold 1 is: 0.5981132481026146\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02248\tvalid_1's rmse: 1.0522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.944149\tvalid_1's rmse: 1.00041\n",
      "[300]\ttraining's rmse: 0.901894\tvalid_1's rmse: 0.982861\n",
      "[400]\ttraining's rmse: 0.872138\tvalid_1's rmse: 0.97611\n",
      "[500]\ttraining's rmse: 0.848716\tvalid_1's rmse: 0.973573\n",
      "[600]\ttraining's rmse: 0.829416\tvalid_1's rmse: 0.972382\n",
      "[700]\ttraining's rmse: 0.812475\tvalid_1's rmse: 0.972298\n",
      "[800]\ttraining's rmse: 0.797888\tvalid_1's rmse: 0.972237\n",
      "Early stopping, best iteration is:\n",
      "[774]\ttraining's rmse: 0.801163\tvalid_1's rmse: 0.972085\n",
      "Partial score of fold 2 is: 0.6129648066099534\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02267\tvalid_1's rmse: 1.04563\n",
      "[200]\ttraining's rmse: 0.944182\tvalid_1's rmse: 0.993546\n",
      "[300]\ttraining's rmse: 0.901671\tvalid_1's rmse: 0.975689\n",
      "[400]\ttraining's rmse: 0.871955\tvalid_1's rmse: 0.968496\n",
      "[500]\ttraining's rmse: 0.849227\tvalid_1's rmse: 0.965662\n",
      "[600]\ttraining's rmse: 0.8311\tvalid_1's rmse: 0.96435\n",
      "[700]\ttraining's rmse: 0.814276\tvalid_1's rmse: 0.963662\n",
      "[800]\ttraining's rmse: 0.798359\tvalid_1's rmse: 0.963371\n",
      "Early stopping, best iteration is:\n",
      "[772]\ttraining's rmse: 0.802824\tvalid_1's rmse: 0.963159\n",
      "Partial score of fold 3 is: 0.6149330854482755\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02106\tvalid_1's rmse: 1.04837\n",
      "[200]\ttraining's rmse: 0.942366\tvalid_1's rmse: 0.997859\n",
      "[300]\ttraining's rmse: 0.900033\tvalid_1's rmse: 0.981361\n",
      "[400]\ttraining's rmse: 0.870584\tvalid_1's rmse: 0.974404\n",
      "[500]\ttraining's rmse: 0.846995\tvalid_1's rmse: 0.972124\n",
      "[600]\ttraining's rmse: 0.827814\tvalid_1's rmse: 0.971277\n",
      "[700]\ttraining's rmse: 0.811036\tvalid_1's rmse: 0.970694\n",
      "Early stopping, best iteration is:\n",
      "[692]\ttraining's rmse: 0.812399\tvalid_1's rmse: 0.970648\n",
      "Partial score of fold 4 is: 0.6034724418917059\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.604   \u001b[0m | \u001b[0m 0.7856  \u001b[0m | \u001b[0m 0.7902  \u001b[0m | \u001b[0m 0.05169 \u001b[0m | \u001b[0m 1.084   \u001b[0m | \u001b[0m 14.88   \u001b[0m | \u001b[0m 125.3   \u001b[0m | \u001b[0m 111.5   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.06641\tvalid_1's rmse: 1.08097\n",
      "[200]\ttraining's rmse: 1.00928\tvalid_1's rmse: 1.03376\n",
      "[300]\ttraining's rmse: 0.983941\tvalid_1's rmse: 1.01513\n",
      "[400]\ttraining's rmse: 0.968883\tvalid_1's rmse: 1.00562\n",
      "[500]\ttraining's rmse: 0.957703\tvalid_1's rmse: 0.999686\n",
      "[600]\ttraining's rmse: 0.949157\tvalid_1's rmse: 0.996104\n",
      "[700]\ttraining's rmse: 0.942146\tvalid_1's rmse: 0.994325\n",
      "[800]\ttraining's rmse: 0.935681\tvalid_1's rmse: 0.993018\n",
      "[900]\ttraining's rmse: 0.930212\tvalid_1's rmse: 0.991992\n",
      "[1000]\ttraining's rmse: 0.925326\tvalid_1's rmse: 0.991309\n",
      "[1100]\ttraining's rmse: 0.920896\tvalid_1's rmse: 0.991025\n",
      "[1200]\ttraining's rmse: 0.916612\tvalid_1's rmse: 0.990686\n",
      "[1300]\ttraining's rmse: 0.912241\tvalid_1's rmse: 0.990637\n",
      "[1400]\ttraining's rmse: 0.908069\tvalid_1's rmse: 0.990127\n",
      "[1500]\ttraining's rmse: 0.904007\tvalid_1's rmse: 0.989808\n",
      "[1600]\ttraining's rmse: 0.900184\tvalid_1's rmse: 0.989765\n",
      "[1700]\ttraining's rmse: 0.896575\tvalid_1's rmse: 0.989641\n",
      "[1800]\ttraining's rmse: 0.893351\tvalid_1's rmse: 0.989757\n",
      "Early stopping, best iteration is:\n",
      "[1701]\ttraining's rmse: 0.896509\tvalid_1's rmse: 0.989602\n",
      "Partial score of fold 0 is: 0.5796830007983267\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.06634\tvalid_1's rmse: 1.07599\n",
      "[200]\ttraining's rmse: 1.01007\tvalid_1's rmse: 1.02712\n",
      "[300]\ttraining's rmse: 0.984805\tvalid_1's rmse: 1.00904\n",
      "[400]\ttraining's rmse: 0.970106\tvalid_1's rmse: 0.999961\n",
      "[500]\ttraining's rmse: 0.959011\tvalid_1's rmse: 0.994393\n",
      "[600]\ttraining's rmse: 0.950726\tvalid_1's rmse: 0.991358\n",
      "[700]\ttraining's rmse: 0.943825\tvalid_1's rmse: 0.989057\n",
      "[800]\ttraining's rmse: 0.937964\tvalid_1's rmse: 0.987599\n",
      "[900]\ttraining's rmse: 0.932467\tvalid_1's rmse: 0.986317\n",
      "[1000]\ttraining's rmse: 0.927631\tvalid_1's rmse: 0.98521\n",
      "[1100]\ttraining's rmse: 0.923163\tvalid_1's rmse: 0.98449\n",
      "[1200]\ttraining's rmse: 0.918576\tvalid_1's rmse: 0.983834\n",
      "[1300]\ttraining's rmse: 0.914482\tvalid_1's rmse: 0.983561\n",
      "[1400]\ttraining's rmse: 0.910273\tvalid_1's rmse: 0.98303\n",
      "[1500]\ttraining's rmse: 0.906445\tvalid_1's rmse: 0.982522\n",
      "[1600]\ttraining's rmse: 0.902595\tvalid_1's rmse: 0.982251\n",
      "[1700]\ttraining's rmse: 0.899107\tvalid_1's rmse: 0.98216\n",
      "Early stopping, best iteration is:\n",
      "[1617]\ttraining's rmse: 0.901911\tvalid_1's rmse: 0.982023\n",
      "Partial score of fold 1 is: 0.5973975103432247\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.06828\tvalid_1's rmse: 1.07699\n",
      "[200]\ttraining's rmse: 1.01226\tvalid_1's rmse: 1.02579\n",
      "[300]\ttraining's rmse: 0.987695\tvalid_1's rmse: 1.00535\n",
      "[400]\ttraining's rmse: 0.972661\tvalid_1's rmse: 0.995453\n",
      "[500]\ttraining's rmse: 0.961401\tvalid_1's rmse: 0.990007\n",
      "[600]\ttraining's rmse: 0.952691\tvalid_1's rmse: 0.986836\n",
      "[700]\ttraining's rmse: 0.945196\tvalid_1's rmse: 0.984922\n",
      "[800]\ttraining's rmse: 0.939187\tvalid_1's rmse: 0.983479\n",
      "[900]\ttraining's rmse: 0.933591\tvalid_1's rmse: 0.982336\n",
      "[1000]\ttraining's rmse: 0.928556\tvalid_1's rmse: 0.98135\n",
      "[1100]\ttraining's rmse: 0.923592\tvalid_1's rmse: 0.980388\n",
      "[1200]\ttraining's rmse: 0.919084\tvalid_1's rmse: 0.979786\n",
      "[1300]\ttraining's rmse: 0.914901\tvalid_1's rmse: 0.979067\n",
      "[1400]\ttraining's rmse: 0.910555\tvalid_1's rmse: 0.978688\n",
      "[1500]\ttraining's rmse: 0.906431\tvalid_1's rmse: 0.978256\n",
      "[1600]\ttraining's rmse: 0.902513\tvalid_1's rmse: 0.977644\n",
      "[1700]\ttraining's rmse: 0.899103\tvalid_1's rmse: 0.977293\n",
      "[1800]\ttraining's rmse: 0.895438\tvalid_1's rmse: 0.977046\n",
      "[1900]\ttraining's rmse: 0.892278\tvalid_1's rmse: 0.976761\n",
      "[2000]\ttraining's rmse: 0.889021\tvalid_1's rmse: 0.976387\n",
      "[2100]\ttraining's rmse: 0.885857\tvalid_1's rmse: 0.97623\n",
      "[2200]\ttraining's rmse: 0.88266\tvalid_1's rmse: 0.976141\n",
      "[2300]\ttraining's rmse: 0.879612\tvalid_1's rmse: 0.975946\n",
      "[2400]\ttraining's rmse: 0.876362\tvalid_1's rmse: 0.975819\n",
      "[2500]\ttraining's rmse: 0.873297\tvalid_1's rmse: 0.97572\n",
      "[2600]\ttraining's rmse: 0.870462\tvalid_1's rmse: 0.975615\n",
      "[2700]\ttraining's rmse: 0.867612\tvalid_1's rmse: 0.975394\n",
      "[2800]\ttraining's rmse: 0.864898\tvalid_1's rmse: 0.975306\n",
      "Early stopping, best iteration is:\n",
      "[2784]\ttraining's rmse: 0.865297\tvalid_1's rmse: 0.975243\n",
      "Partial score of fold 2 is: 0.6090282489333094\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.06897\tvalid_1's rmse: 1.07006\n",
      "[200]\ttraining's rmse: 1.01305\tvalid_1's rmse: 1.01844\n",
      "[300]\ttraining's rmse: 0.987552\tvalid_1's rmse: 0.997926\n",
      "[400]\ttraining's rmse: 0.971963\tvalid_1's rmse: 0.98867\n",
      "[500]\ttraining's rmse: 0.960667\tvalid_1's rmse: 0.982965\n",
      "[600]\ttraining's rmse: 0.952128\tvalid_1's rmse: 0.979573\n",
      "[700]\ttraining's rmse: 0.945258\tvalid_1's rmse: 0.977215\n",
      "[800]\ttraining's rmse: 0.939198\tvalid_1's rmse: 0.975356\n",
      "[900]\ttraining's rmse: 0.933713\tvalid_1's rmse: 0.973976\n",
      "[1000]\ttraining's rmse: 0.928561\tvalid_1's rmse: 0.972822\n",
      "[1100]\ttraining's rmse: 0.923797\tvalid_1's rmse: 0.971999\n",
      "[1200]\ttraining's rmse: 0.919441\tvalid_1's rmse: 0.971047\n",
      "[1300]\ttraining's rmse: 0.915136\tvalid_1's rmse: 0.970372\n",
      "[1400]\ttraining's rmse: 0.910968\tvalid_1's rmse: 0.969707\n",
      "[1500]\ttraining's rmse: 0.907338\tvalid_1's rmse: 0.969329\n",
      "[1600]\ttraining's rmse: 0.903614\tvalid_1's rmse: 0.968783\n",
      "[1700]\ttraining's rmse: 0.899973\tvalid_1's rmse: 0.968085\n",
      "[1800]\ttraining's rmse: 0.896463\tvalid_1's rmse: 0.967724\n",
      "[1900]\ttraining's rmse: 0.893086\tvalid_1's rmse: 0.967474\n",
      "[2000]\ttraining's rmse: 0.889832\tvalid_1's rmse: 0.96706\n",
      "[2100]\ttraining's rmse: 0.886674\tvalid_1's rmse: 0.966777\n",
      "[2200]\ttraining's rmse: 0.883646\tvalid_1's rmse: 0.966611\n",
      "[2300]\ttraining's rmse: 0.880711\tvalid_1's rmse: 0.966304\n",
      "[2400]\ttraining's rmse: 0.87786\tvalid_1's rmse: 0.966141\n",
      "[2500]\ttraining's rmse: 0.874918\tvalid_1's rmse: 0.9659\n",
      "[2600]\ttraining's rmse: 0.872185\tvalid_1's rmse: 0.965822\n",
      "[2700]\ttraining's rmse: 0.869482\tvalid_1's rmse: 0.965796\n",
      "[2800]\ttraining's rmse: 0.866855\tvalid_1's rmse: 0.965656\n",
      "[2900]\ttraining's rmse: 0.864339\tvalid_1's rmse: 0.965518\n",
      "[3000]\ttraining's rmse: 0.861868\tvalid_1's rmse: 0.965305\n",
      "[3100]\ttraining's rmse: 0.85928\tvalid_1's rmse: 0.965237\n",
      "Early stopping, best iteration is:\n",
      "[3045]\ttraining's rmse: 0.860644\tvalid_1's rmse: 0.965163\n",
      "Partial score of fold 3 is: 0.6145752165685805\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 1.06824\tvalid_1's rmse: 1.07177\n",
      "[200]\ttraining's rmse: 1.01244\tvalid_1's rmse: 1.02265\n",
      "[300]\ttraining's rmse: 0.987071\tvalid_1's rmse: 1.00325\n",
      "[400]\ttraining's rmse: 0.971253\tvalid_1's rmse: 0.993135\n",
      "[500]\ttraining's rmse: 0.960271\tvalid_1's rmse: 0.987833\n",
      "[600]\ttraining's rmse: 0.952152\tvalid_1's rmse: 0.984755\n",
      "[700]\ttraining's rmse: 0.945572\tvalid_1's rmse: 0.982799\n",
      "[800]\ttraining's rmse: 0.939475\tvalid_1's rmse: 0.981402\n",
      "[900]\ttraining's rmse: 0.933909\tvalid_1's rmse: 0.980262\n",
      "[1000]\ttraining's rmse: 0.928934\tvalid_1's rmse: 0.979899\n",
      "[1100]\ttraining's rmse: 0.924177\tvalid_1's rmse: 0.979202\n",
      "[1200]\ttraining's rmse: 0.919437\tvalid_1's rmse: 0.978698\n",
      "[1300]\ttraining's rmse: 0.915367\tvalid_1's rmse: 0.97834\n",
      "[1400]\ttraining's rmse: 0.911307\tvalid_1's rmse: 0.977959\n",
      "[1500]\ttraining's rmse: 0.90729\tvalid_1's rmse: 0.977631\n",
      "[1600]\ttraining's rmse: 0.903456\tvalid_1's rmse: 0.977352\n",
      "[1700]\ttraining's rmse: 0.899815\tvalid_1's rmse: 0.977055\n",
      "Early stopping, best iteration is:\n",
      "[1698]\ttraining's rmse: 0.89992\tvalid_1's rmse: 0.977039\n",
      "Partial score of fold 4 is: 0.5968502258456634\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5991  \u001b[0m | \u001b[0m 0.8499  \u001b[0m | \u001b[0m 0.7428  \u001b[0m | \u001b[0m 0.191   \u001b[0m | \u001b[0m 5.79    \u001b[0m | \u001b[0m 4.356   \u001b[0m | \u001b[0m 148.2   \u001b[0m | \u001b[0m 129.7   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.09242\tvalid_1's rmse: 1.10119\n",
      "[200]\ttraining's rmse: 1.04167\tvalid_1's rmse: 1.05423\n",
      "[300]\ttraining's rmse: 1.02086\tvalid_1's rmse: 1.03705\n",
      "[400]\ttraining's rmse: 1.00809\tvalid_1's rmse: 1.02625\n",
      "[500]\ttraining's rmse: 0.998962\tvalid_1's rmse: 1.01864\n",
      "[600]\ttraining's rmse: 0.991947\tvalid_1's rmse: 1.01329\n",
      "[700]\ttraining's rmse: 0.986264\tvalid_1's rmse: 1.00965\n",
      "[800]\ttraining's rmse: 0.981385\tvalid_1's rmse: 1.00706\n",
      "[900]\ttraining's rmse: 0.977009\tvalid_1's rmse: 1.00502\n",
      "[1000]\ttraining's rmse: 0.973183\tvalid_1's rmse: 1.00356\n",
      "[1100]\ttraining's rmse: 0.969566\tvalid_1's rmse: 1.00242\n",
      "[1200]\ttraining's rmse: 0.966202\tvalid_1's rmse: 1.00154\n",
      "[1300]\ttraining's rmse: 0.962954\tvalid_1's rmse: 1.00074\n",
      "[1400]\ttraining's rmse: 0.959968\tvalid_1's rmse: 0.999861\n",
      "[1500]\ttraining's rmse: 0.957061\tvalid_1's rmse: 0.999139\n",
      "[1600]\ttraining's rmse: 0.954379\tvalid_1's rmse: 0.998521\n",
      "[1700]\ttraining's rmse: 0.951766\tvalid_1's rmse: 0.998128\n",
      "[1800]\ttraining's rmse: 0.949287\tvalid_1's rmse: 0.997674\n",
      "[1900]\ttraining's rmse: 0.94679\tvalid_1's rmse: 0.997226\n",
      "[2000]\ttraining's rmse: 0.94442\tvalid_1's rmse: 0.996917\n",
      "[2100]\ttraining's rmse: 0.942093\tvalid_1's rmse: 0.996527\n",
      "[2200]\ttraining's rmse: 0.939867\tvalid_1's rmse: 0.996264\n",
      "[2300]\ttraining's rmse: 0.937647\tvalid_1's rmse: 0.995935\n",
      "[2400]\ttraining's rmse: 0.935531\tvalid_1's rmse: 0.995775\n",
      "[2500]\ttraining's rmse: 0.933415\tvalid_1's rmse: 0.9956\n",
      "[2600]\ttraining's rmse: 0.931271\tvalid_1's rmse: 0.995341\n",
      "[2700]\ttraining's rmse: 0.929253\tvalid_1's rmse: 0.995126\n",
      "[2800]\ttraining's rmse: 0.927233\tvalid_1's rmse: 0.995075\n",
      "Early stopping, best iteration is:\n",
      "[2738]\ttraining's rmse: 0.928481\tvalid_1's rmse: 0.99503\n",
      "Partial score of fold 0 is: 0.5730624265239709\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.09382\tvalid_1's rmse: 1.09914\n",
      "[200]\ttraining's rmse: 1.0424\tvalid_1's rmse: 1.05179\n",
      "[300]\ttraining's rmse: 1.02187\tvalid_1's rmse: 1.03407\n",
      "[400]\ttraining's rmse: 1.00909\tvalid_1's rmse: 1.02311\n",
      "[500]\ttraining's rmse: 0.999519\tvalid_1's rmse: 1.01552\n",
      "[600]\ttraining's rmse: 0.99223\tvalid_1's rmse: 1.01012\n",
      "[700]\ttraining's rmse: 0.986404\tvalid_1's rmse: 1.00609\n",
      "[800]\ttraining's rmse: 0.981463\tvalid_1's rmse: 1.00315\n",
      "[900]\ttraining's rmse: 0.977176\tvalid_1's rmse: 1.00105\n",
      "[1000]\ttraining's rmse: 0.97329\tvalid_1's rmse: 0.999232\n",
      "[1100]\ttraining's rmse: 0.969652\tvalid_1's rmse: 0.997711\n",
      "[1200]\ttraining's rmse: 0.966309\tvalid_1's rmse: 0.996392\n",
      "[1300]\ttraining's rmse: 0.96314\tvalid_1's rmse: 0.99535\n",
      "[1400]\ttraining's rmse: 0.960242\tvalid_1's rmse: 0.994306\n",
      "[1500]\ttraining's rmse: 0.957377\tvalid_1's rmse: 0.9934\n",
      "[1600]\ttraining's rmse: 0.954658\tvalid_1's rmse: 0.992434\n",
      "[1700]\ttraining's rmse: 0.952046\tvalid_1's rmse: 0.991742\n",
      "[1800]\ttraining's rmse: 0.949622\tvalid_1's rmse: 0.991315\n",
      "[1900]\ttraining's rmse: 0.947263\tvalid_1's rmse: 0.990648\n",
      "[2000]\ttraining's rmse: 0.944848\tvalid_1's rmse: 0.990042\n",
      "[2100]\ttraining's rmse: 0.942613\tvalid_1's rmse: 0.989613\n",
      "[2200]\ttraining's rmse: 0.940345\tvalid_1's rmse: 0.98926\n",
      "[2300]\ttraining's rmse: 0.938231\tvalid_1's rmse: 0.988774\n",
      "[2400]\ttraining's rmse: 0.936154\tvalid_1's rmse: 0.988362\n",
      "[2500]\ttraining's rmse: 0.93411\tvalid_1's rmse: 0.987996\n",
      "[2600]\ttraining's rmse: 0.932177\tvalid_1's rmse: 0.987699\n",
      "[2700]\ttraining's rmse: 0.930239\tvalid_1's rmse: 0.987352\n",
      "[2800]\ttraining's rmse: 0.928301\tvalid_1's rmse: 0.987092\n",
      "[2900]\ttraining's rmse: 0.926461\tvalid_1's rmse: 0.986837\n",
      "[3000]\ttraining's rmse: 0.924518\tvalid_1's rmse: 0.986664\n",
      "[3100]\ttraining's rmse: 0.922661\tvalid_1's rmse: 0.986464\n",
      "[3200]\ttraining's rmse: 0.920828\tvalid_1's rmse: 0.98625\n",
      "[3300]\ttraining's rmse: 0.918997\tvalid_1's rmse: 0.985904\n",
      "[3400]\ttraining's rmse: 0.917225\tvalid_1's rmse: 0.98572\n",
      "[3500]\ttraining's rmse: 0.915496\tvalid_1's rmse: 0.985662\n",
      "[3600]\ttraining's rmse: 0.91371\tvalid_1's rmse: 0.985406\n",
      "[3700]\ttraining's rmse: 0.911977\tvalid_1's rmse: 0.985198\n",
      "[3800]\ttraining's rmse: 0.910327\tvalid_1's rmse: 0.985031\n",
      "[3900]\ttraining's rmse: 0.908659\tvalid_1's rmse: 0.984871\n",
      "[4000]\ttraining's rmse: 0.906972\tvalid_1's rmse: 0.984919\n",
      "Early stopping, best iteration is:\n",
      "[3925]\ttraining's rmse: 0.908222\tvalid_1's rmse: 0.98483\n",
      "Partial score of fold 1 is: 0.5941766904259707\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.09406\tvalid_1's rmse: 1.0992\n",
      "[200]\ttraining's rmse: 1.04328\tvalid_1's rmse: 1.04841\n",
      "[300]\ttraining's rmse: 1.02322\tvalid_1's rmse: 1.02882\n",
      "[400]\ttraining's rmse: 1.01054\tvalid_1's rmse: 1.01826\n",
      "[500]\ttraining's rmse: 1.00121\tvalid_1's rmse: 1.01081\n",
      "[600]\ttraining's rmse: 0.994007\tvalid_1's rmse: 1.00571\n",
      "[700]\ttraining's rmse: 0.988206\tvalid_1's rmse: 1.00202\n",
      "[800]\ttraining's rmse: 0.983338\tvalid_1's rmse: 0.999307\n",
      "[900]\ttraining's rmse: 0.979113\tvalid_1's rmse: 0.997418\n",
      "[1000]\ttraining's rmse: 0.975206\tvalid_1's rmse: 0.995627\n",
      "[1100]\ttraining's rmse: 0.971716\tvalid_1's rmse: 0.994329\n",
      "[1200]\ttraining's rmse: 0.968545\tvalid_1's rmse: 0.993192\n",
      "[1300]\ttraining's rmse: 0.965503\tvalid_1's rmse: 0.992122\n",
      "[1400]\ttraining's rmse: 0.962648\tvalid_1's rmse: 0.991342\n",
      "[1500]\ttraining's rmse: 0.959872\tvalid_1's rmse: 0.990576\n",
      "[1600]\ttraining's rmse: 0.957164\tvalid_1's rmse: 0.989731\n",
      "[1700]\ttraining's rmse: 0.954541\tvalid_1's rmse: 0.989277\n",
      "[1800]\ttraining's rmse: 0.952098\tvalid_1's rmse: 0.988818\n",
      "[1900]\ttraining's rmse: 0.949747\tvalid_1's rmse: 0.988352\n",
      "[2000]\ttraining's rmse: 0.947435\tvalid_1's rmse: 0.987786\n",
      "[2100]\ttraining's rmse: 0.945307\tvalid_1's rmse: 0.987329\n",
      "[2200]\ttraining's rmse: 0.943086\tvalid_1's rmse: 0.986987\n",
      "[2300]\ttraining's rmse: 0.940888\tvalid_1's rmse: 0.986425\n",
      "[2400]\ttraining's rmse: 0.938836\tvalid_1's rmse: 0.986107\n",
      "[2500]\ttraining's rmse: 0.936684\tvalid_1's rmse: 0.985617\n",
      "[2600]\ttraining's rmse: 0.934615\tvalid_1's rmse: 0.985396\n",
      "[2700]\ttraining's rmse: 0.93272\tvalid_1's rmse: 0.985027\n",
      "[2800]\ttraining's rmse: 0.930772\tvalid_1's rmse: 0.984726\n",
      "[2900]\ttraining's rmse: 0.928855\tvalid_1's rmse: 0.98451\n",
      "[3000]\ttraining's rmse: 0.926933\tvalid_1's rmse: 0.984178\n",
      "[3100]\ttraining's rmse: 0.925027\tvalid_1's rmse: 0.983922\n",
      "[3200]\ttraining's rmse: 0.923183\tvalid_1's rmse: 0.983729\n",
      "[3300]\ttraining's rmse: 0.921342\tvalid_1's rmse: 0.983346\n",
      "[3400]\ttraining's rmse: 0.919524\tvalid_1's rmse: 0.983093\n",
      "[3500]\ttraining's rmse: 0.917776\tvalid_1's rmse: 0.983001\n",
      "[3600]\ttraining's rmse: 0.916125\tvalid_1's rmse: 0.982748\n",
      "[3700]\ttraining's rmse: 0.914359\tvalid_1's rmse: 0.982521\n",
      "[3800]\ttraining's rmse: 0.912686\tvalid_1's rmse: 0.982288\n",
      "[3900]\ttraining's rmse: 0.910956\tvalid_1's rmse: 0.98214\n",
      "[4000]\ttraining's rmse: 0.909242\tvalid_1's rmse: 0.981902\n",
      "[4100]\ttraining's rmse: 0.907563\tvalid_1's rmse: 0.981811\n",
      "[4200]\ttraining's rmse: 0.905928\tvalid_1's rmse: 0.981614\n",
      "[4300]\ttraining's rmse: 0.904326\tvalid_1's rmse: 0.981383\n",
      "[4400]\ttraining's rmse: 0.902827\tvalid_1's rmse: 0.9813\n",
      "[4500]\ttraining's rmse: 0.901235\tvalid_1's rmse: 0.981124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4600]\ttraining's rmse: 0.899745\tvalid_1's rmse: 0.980956\n",
      "[4700]\ttraining's rmse: 0.898214\tvalid_1's rmse: 0.980851\n",
      "[4800]\ttraining's rmse: 0.896635\tvalid_1's rmse: 0.980679\n",
      "[4900]\ttraining's rmse: 0.895158\tvalid_1's rmse: 0.980537\n",
      "[5000]\ttraining's rmse: 0.893587\tvalid_1's rmse: 0.980385\n",
      "[5100]\ttraining's rmse: 0.892078\tvalid_1's rmse: 0.980243\n",
      "[5200]\ttraining's rmse: 0.890608\tvalid_1's rmse: 0.980084\n",
      "[5300]\ttraining's rmse: 0.889151\tvalid_1's rmse: 0.979961\n",
      "[5400]\ttraining's rmse: 0.887773\tvalid_1's rmse: 0.979836\n",
      "[5500]\ttraining's rmse: 0.886315\tvalid_1's rmse: 0.979759\n",
      "[5600]\ttraining's rmse: 0.884876\tvalid_1's rmse: 0.979692\n",
      "[5700]\ttraining's rmse: 0.883444\tvalid_1's rmse: 0.979528\n",
      "[5800]\ttraining's rmse: 0.881966\tvalid_1's rmse: 0.979329\n",
      "[5900]\ttraining's rmse: 0.880484\tvalid_1's rmse: 0.979242\n",
      "[6000]\ttraining's rmse: 0.879041\tvalid_1's rmse: 0.979182\n",
      "[6100]\ttraining's rmse: 0.877693\tvalid_1's rmse: 0.9791\n",
      "[6200]\ttraining's rmse: 0.876271\tvalid_1's rmse: 0.978956\n",
      "[6300]\ttraining's rmse: 0.87493\tvalid_1's rmse: 0.978908\n",
      "[6400]\ttraining's rmse: 0.87355\tvalid_1's rmse: 0.978844\n",
      "[6500]\ttraining's rmse: 0.872199\tvalid_1's rmse: 0.97874\n",
      "[6600]\ttraining's rmse: 0.8708\tvalid_1's rmse: 0.978583\n",
      "[6700]\ttraining's rmse: 0.869438\tvalid_1's rmse: 0.97858\n",
      "[6800]\ttraining's rmse: 0.868136\tvalid_1's rmse: 0.978419\n",
      "[6900]\ttraining's rmse: 0.866813\tvalid_1's rmse: 0.978349\n",
      "[7000]\ttraining's rmse: 0.865477\tvalid_1's rmse: 0.978194\n",
      "[7100]\ttraining's rmse: 0.864214\tvalid_1's rmse: 0.978113\n",
      "[7200]\ttraining's rmse: 0.862925\tvalid_1's rmse: 0.977905\n",
      "[7300]\ttraining's rmse: 0.861581\tvalid_1's rmse: 0.977744\n",
      "[7400]\ttraining's rmse: 0.860257\tvalid_1's rmse: 0.977732\n",
      "[7500]\ttraining's rmse: 0.859013\tvalid_1's rmse: 0.977666\n",
      "[7600]\ttraining's rmse: 0.857805\tvalid_1's rmse: 0.977643\n",
      "[7700]\ttraining's rmse: 0.856502\tvalid_1's rmse: 0.977582\n",
      "[7800]\ttraining's rmse: 0.855281\tvalid_1's rmse: 0.977522\n",
      "[7900]\ttraining's rmse: 0.854078\tvalid_1's rmse: 0.977425\n",
      "[8000]\ttraining's rmse: 0.852904\tvalid_1's rmse: 0.977371\n",
      "[8100]\ttraining's rmse: 0.851734\tvalid_1's rmse: 0.977324\n",
      "[8200]\ttraining's rmse: 0.850524\tvalid_1's rmse: 0.977289\n",
      "Early stopping, best iteration is:\n",
      "[8197]\ttraining's rmse: 0.850558\tvalid_1's rmse: 0.977282\n",
      "Partial score of fold 2 is: 0.604554887937123\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0959\tvalid_1's rmse: 1.09145\n",
      "[200]\ttraining's rmse: 1.04559\tvalid_1's rmse: 1.0401\n",
      "[300]\ttraining's rmse: 1.02506\tvalid_1's rmse: 1.02066\n",
      "[400]\ttraining's rmse: 1.01204\tvalid_1's rmse: 1.01051\n",
      "[500]\ttraining's rmse: 1.00255\tvalid_1's rmse: 1.00354\n",
      "[600]\ttraining's rmse: 0.995533\tvalid_1's rmse: 0.998434\n",
      "[700]\ttraining's rmse: 0.98982\tvalid_1's rmse: 0.994651\n",
      "[800]\ttraining's rmse: 0.984934\tvalid_1's rmse: 0.991479\n",
      "[900]\ttraining's rmse: 0.980681\tvalid_1's rmse: 0.98893\n",
      "[1000]\ttraining's rmse: 0.976994\tvalid_1's rmse: 0.987136\n",
      "[1100]\ttraining's rmse: 0.973515\tvalid_1's rmse: 0.985673\n",
      "[1200]\ttraining's rmse: 0.970214\tvalid_1's rmse: 0.984274\n",
      "[1300]\ttraining's rmse: 0.967089\tvalid_1's rmse: 0.983057\n",
      "[1400]\ttraining's rmse: 0.964143\tvalid_1's rmse: 0.981889\n",
      "[1500]\ttraining's rmse: 0.961379\tvalid_1's rmse: 0.980867\n",
      "[1600]\ttraining's rmse: 0.958751\tvalid_1's rmse: 0.980131\n",
      "[1700]\ttraining's rmse: 0.956208\tvalid_1's rmse: 0.97933\n",
      "[1800]\ttraining's rmse: 0.953675\tvalid_1's rmse: 0.978623\n",
      "[1900]\ttraining's rmse: 0.951261\tvalid_1's rmse: 0.978179\n",
      "[2000]\ttraining's rmse: 0.948973\tvalid_1's rmse: 0.977582\n",
      "[2100]\ttraining's rmse: 0.946621\tvalid_1's rmse: 0.97697\n",
      "[2200]\ttraining's rmse: 0.944441\tvalid_1's rmse: 0.97641\n",
      "[2300]\ttraining's rmse: 0.942267\tvalid_1's rmse: 0.975936\n",
      "[2400]\ttraining's rmse: 0.940175\tvalid_1's rmse: 0.975515\n",
      "[2500]\ttraining's rmse: 0.938251\tvalid_1's rmse: 0.975245\n",
      "[2600]\ttraining's rmse: 0.936267\tvalid_1's rmse: 0.974909\n",
      "[2700]\ttraining's rmse: 0.93433\tvalid_1's rmse: 0.974642\n",
      "[2800]\ttraining's rmse: 0.932357\tvalid_1's rmse: 0.974254\n",
      "[2900]\ttraining's rmse: 0.93049\tvalid_1's rmse: 0.973976\n",
      "[3000]\ttraining's rmse: 0.92859\tvalid_1's rmse: 0.973613\n",
      "[3100]\ttraining's rmse: 0.926733\tvalid_1's rmse: 0.973319\n",
      "[3200]\ttraining's rmse: 0.924869\tvalid_1's rmse: 0.972914\n",
      "[3300]\ttraining's rmse: 0.923037\tvalid_1's rmse: 0.972644\n",
      "[3400]\ttraining's rmse: 0.921318\tvalid_1's rmse: 0.972367\n",
      "[3500]\ttraining's rmse: 0.91959\tvalid_1's rmse: 0.972147\n",
      "[3600]\ttraining's rmse: 0.91791\tvalid_1's rmse: 0.971956\n",
      "[3700]\ttraining's rmse: 0.916243\tvalid_1's rmse: 0.971719\n",
      "[3800]\ttraining's rmse: 0.914612\tvalid_1's rmse: 0.971611\n",
      "[3900]\ttraining's rmse: 0.912954\tvalid_1's rmse: 0.971481\n",
      "[4000]\ttraining's rmse: 0.911326\tvalid_1's rmse: 0.971437\n",
      "[4100]\ttraining's rmse: 0.909605\tvalid_1's rmse: 0.971277\n",
      "[4200]\ttraining's rmse: 0.908034\tvalid_1's rmse: 0.971183\n",
      "[4300]\ttraining's rmse: 0.906409\tvalid_1's rmse: 0.970902\n",
      "[4400]\ttraining's rmse: 0.904785\tvalid_1's rmse: 0.970768\n",
      "[4500]\ttraining's rmse: 0.903167\tvalid_1's rmse: 0.970593\n",
      "[4600]\ttraining's rmse: 0.901576\tvalid_1's rmse: 0.970549\n",
      "[4700]\ttraining's rmse: 0.900097\tvalid_1's rmse: 0.970342\n",
      "[4800]\ttraining's rmse: 0.898581\tvalid_1's rmse: 0.970251\n",
      "[4900]\ttraining's rmse: 0.897075\tvalid_1's rmse: 0.970163\n",
      "[5000]\ttraining's rmse: 0.895605\tvalid_1's rmse: 0.970115\n",
      "[5100]\ttraining's rmse: 0.894115\tvalid_1's rmse: 0.969992\n",
      "[5200]\ttraining's rmse: 0.892614\tvalid_1's rmse: 0.969832\n",
      "[5300]\ttraining's rmse: 0.891205\tvalid_1's rmse: 0.969747\n",
      "[5400]\ttraining's rmse: 0.889765\tvalid_1's rmse: 0.969653\n",
      "[5500]\ttraining's rmse: 0.888349\tvalid_1's rmse: 0.969614\n",
      "[5600]\ttraining's rmse: 0.886869\tvalid_1's rmse: 0.969453\n",
      "[5700]\ttraining's rmse: 0.885444\tvalid_1's rmse: 0.969458\n",
      "Early stopping, best iteration is:\n",
      "[5603]\ttraining's rmse: 0.886827\tvalid_1's rmse: 0.969438\n",
      "Partial score of fold 3 is: 0.6054495601363603\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.09473\tvalid_1's rmse: 1.09243\n",
      "[200]\ttraining's rmse: 1.04354\tvalid_1's rmse: 1.04323\n",
      "[300]\ttraining's rmse: 1.02328\tvalid_1's rmse: 1.02481\n",
      "[400]\ttraining's rmse: 1.01022\tvalid_1's rmse: 1.01464\n",
      "[500]\ttraining's rmse: 1.00097\tvalid_1's rmse: 1.00793\n",
      "[600]\ttraining's rmse: 0.993706\tvalid_1's rmse: 1.00329\n",
      "[700]\ttraining's rmse: 0.987929\tvalid_1's rmse: 1.00002\n",
      "[800]\ttraining's rmse: 0.982999\tvalid_1's rmse: 0.997493\n",
      "[900]\ttraining's rmse: 0.978692\tvalid_1's rmse: 0.995467\n",
      "[1000]\ttraining's rmse: 0.974858\tvalid_1's rmse: 0.994016\n",
      "[1100]\ttraining's rmse: 0.971412\tvalid_1's rmse: 0.992622\n",
      "[1200]\ttraining's rmse: 0.968094\tvalid_1's rmse: 0.991435\n",
      "[1300]\ttraining's rmse: 0.96497\tvalid_1's rmse: 0.990374\n",
      "[1400]\ttraining's rmse: 0.961985\tvalid_1's rmse: 0.989504\n",
      "[1500]\ttraining's rmse: 0.959139\tvalid_1's rmse: 0.988735\n",
      "[1600]\ttraining's rmse: 0.956525\tvalid_1's rmse: 0.988069\n",
      "[1700]\ttraining's rmse: 0.953881\tvalid_1's rmse: 0.987411\n",
      "[1800]\ttraining's rmse: 0.951457\tvalid_1's rmse: 0.986987\n",
      "[1900]\ttraining's rmse: 0.949133\tvalid_1's rmse: 0.986335\n",
      "[2000]\ttraining's rmse: 0.946822\tvalid_1's rmse: 0.985831\n",
      "[2100]\ttraining's rmse: 0.944596\tvalid_1's rmse: 0.985445\n",
      "[2200]\ttraining's rmse: 0.942457\tvalid_1's rmse: 0.985061\n",
      "[2300]\ttraining's rmse: 0.940361\tvalid_1's rmse: 0.984787\n",
      "[2400]\ttraining's rmse: 0.938236\tvalid_1's rmse: 0.984383\n",
      "[2500]\ttraining's rmse: 0.936153\tvalid_1's rmse: 0.983999\n",
      "[2600]\ttraining's rmse: 0.93425\tvalid_1's rmse: 0.983619\n",
      "[2700]\ttraining's rmse: 0.932278\tvalid_1's rmse: 0.983273\n",
      "[2800]\ttraining's rmse: 0.930284\tvalid_1's rmse: 0.982877\n",
      "[2900]\ttraining's rmse: 0.928402\tvalid_1's rmse: 0.982656\n",
      "[3000]\ttraining's rmse: 0.926535\tvalid_1's rmse: 0.982343\n",
      "[3100]\ttraining's rmse: 0.924656\tvalid_1's rmse: 0.982114\n",
      "[3200]\ttraining's rmse: 0.922788\tvalid_1's rmse: 0.981928\n",
      "[3300]\ttraining's rmse: 0.921031\tvalid_1's rmse: 0.981807\n",
      "[3400]\ttraining's rmse: 0.919315\tvalid_1's rmse: 0.981576\n",
      "[3500]\ttraining's rmse: 0.917565\tvalid_1's rmse: 0.981424\n",
      "[3600]\ttraining's rmse: 0.915872\tvalid_1's rmse: 0.98119\n",
      "[3700]\ttraining's rmse: 0.914136\tvalid_1's rmse: 0.981056\n",
      "[3800]\ttraining's rmse: 0.912464\tvalid_1's rmse: 0.980908\n",
      "[3900]\ttraining's rmse: 0.9108\tvalid_1's rmse: 0.980655\n",
      "[4000]\ttraining's rmse: 0.909111\tvalid_1's rmse: 0.98039\n",
      "[4100]\ttraining's rmse: 0.907466\tvalid_1's rmse: 0.980288\n",
      "[4200]\ttraining's rmse: 0.905868\tvalid_1's rmse: 0.980278\n",
      "Early stopping, best iteration is:\n",
      "[4124]\ttraining's rmse: 0.907069\tvalid_1's rmse: 0.980192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial score of fold 4 is: 0.5966712470336082\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.596   \u001b[0m | \u001b[0m 0.9718  \u001b[0m | \u001b[0m 0.877   \u001b[0m | \u001b[0m 0.7481  \u001b[0m | \u001b[0m 4.894   \u001b[0m | \u001b[0m 14.86   \u001b[0m | \u001b[0m 10.99   \u001b[0m | \u001b[0m 5.383   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02212\tvalid_1's rmse: 1.05371\n",
      "[200]\ttraining's rmse: 0.949743\tvalid_1's rmse: 1.00886\n",
      "[300]\ttraining's rmse: 0.910017\tvalid_1's rmse: 0.994768\n",
      "[400]\ttraining's rmse: 0.878985\tvalid_1's rmse: 0.989638\n",
      "[500]\ttraining's rmse: 0.853701\tvalid_1's rmse: 0.987479\n",
      "[600]\ttraining's rmse: 0.831763\tvalid_1's rmse: 0.986885\n",
      "Early stopping, best iteration is:\n",
      "[560]\ttraining's rmse: 0.840126\tvalid_1's rmse: 0.98667\n",
      "Partial score of fold 0 is: 0.5898822638696317\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02387\tvalid_1's rmse: 1.05003\n",
      "[200]\ttraining's rmse: 0.951399\tvalid_1's rmse: 1.00353\n",
      "[300]\ttraining's rmse: 0.911155\tvalid_1's rmse: 0.989695\n",
      "[400]\ttraining's rmse: 0.880432\tvalid_1's rmse: 0.984144\n",
      "[500]\ttraining's rmse: 0.855127\tvalid_1's rmse: 0.982009\n",
      "[600]\ttraining's rmse: 0.833183\tvalid_1's rmse: 0.980784\n",
      "[700]\ttraining's rmse: 0.814019\tvalid_1's rmse: 0.980112\n",
      "[800]\ttraining's rmse: 0.796648\tvalid_1's rmse: 0.980486\n",
      "Early stopping, best iteration is:\n",
      "[728]\ttraining's rmse: 0.809026\tvalid_1's rmse: 0.979917\n",
      "Partial score of fold 1 is: 0.6015130024597163\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02491\tvalid_1's rmse: 1.04893\n",
      "[200]\ttraining's rmse: 0.95325\tvalid_1's rmse: 0.998539\n",
      "[300]\ttraining's rmse: 0.913514\tvalid_1's rmse: 0.982425\n",
      "[400]\ttraining's rmse: 0.882983\tvalid_1's rmse: 0.975377\n",
      "[500]\ttraining's rmse: 0.857476\tvalid_1's rmse: 0.971962\n",
      "[600]\ttraining's rmse: 0.835677\tvalid_1's rmse: 0.970211\n",
      "[700]\ttraining's rmse: 0.816243\tvalid_1's rmse: 0.969857\n",
      "[800]\ttraining's rmse: 0.798713\tvalid_1's rmse: 0.969691\n",
      "Early stopping, best iteration is:\n",
      "[743]\ttraining's rmse: 0.808712\tvalid_1's rmse: 0.969564\n",
      "Partial score of fold 2 is: 0.6097439866926992\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02572\tvalid_1's rmse: 1.04374\n",
      "[200]\ttraining's rmse: 0.95323\tvalid_1's rmse: 0.993271\n",
      "[300]\ttraining's rmse: 0.912979\tvalid_1's rmse: 0.977396\n",
      "[400]\ttraining's rmse: 0.88207\tvalid_1's rmse: 0.970396\n",
      "[500]\ttraining's rmse: 0.857234\tvalid_1's rmse: 0.96692\n",
      "[600]\ttraining's rmse: 0.835875\tvalid_1's rmse: 0.965293\n",
      "[700]\ttraining's rmse: 0.817359\tvalid_1's rmse: 0.96437\n",
      "[800]\ttraining's rmse: 0.800129\tvalid_1's rmse: 0.963961\n",
      "[900]\ttraining's rmse: 0.784208\tvalid_1's rmse: 0.963979\n",
      "Early stopping, best iteration is:\n",
      "[843]\ttraining's rmse: 0.793192\tvalid_1's rmse: 0.963725\n",
      "Partial score of fold 3 is: 0.6138594788091907\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02413\tvalid_1's rmse: 1.04487\n",
      "[200]\ttraining's rmse: 0.952273\tvalid_1's rmse: 0.99666\n",
      "[300]\ttraining's rmse: 0.912054\tvalid_1's rmse: 0.981895\n",
      "[400]\ttraining's rmse: 0.881632\tvalid_1's rmse: 0.975861\n",
      "[500]\ttraining's rmse: 0.856657\tvalid_1's rmse: 0.972963\n",
      "[600]\ttraining's rmse: 0.834942\tvalid_1's rmse: 0.971792\n",
      "[700]\ttraining's rmse: 0.816064\tvalid_1's rmse: 0.971459\n",
      "Early stopping, best iteration is:\n",
      "[674]\ttraining's rmse: 0.820733\tvalid_1's rmse: 0.971246\n",
      "Partial score of fold 4 is: 0.602756526643485\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6037  \u001b[0m | \u001b[0m 0.7315  \u001b[0m | \u001b[0m 0.9618  \u001b[0m | \u001b[0m 5.82    \u001b[0m | \u001b[0m 0.2962  \u001b[0m | \u001b[0m 14.89   \u001b[0m | \u001b[0m 103.7   \u001b[0m | \u001b[0m 105.1   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02681\tvalid_1's rmse: 1.06307\n",
      "[200]\ttraining's rmse: 0.94122\tvalid_1's rmse: 1.01083\n",
      "[300]\ttraining's rmse: 0.896513\tvalid_1's rmse: 0.995357\n",
      "[400]\ttraining's rmse: 0.865815\tvalid_1's rmse: 0.989716\n",
      "[500]\ttraining's rmse: 0.84113\tvalid_1's rmse: 0.988296\n",
      "[600]\ttraining's rmse: 0.821303\tvalid_1's rmse: 0.98846\n",
      "Early stopping, best iteration is:\n",
      "[538]\ttraining's rmse: 0.833469\tvalid_1's rmse: 0.988054\n",
      "Partial score of fold 0 is: 0.5854089028734453\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02842\tvalid_1's rmse: 1.05869\n",
      "[200]\ttraining's rmse: 0.943085\tvalid_1's rmse: 1.00404\n",
      "[300]\ttraining's rmse: 0.898186\tvalid_1's rmse: 0.987998\n",
      "[400]\ttraining's rmse: 0.866859\tvalid_1's rmse: 0.982069\n",
      "[500]\ttraining's rmse: 0.842181\tvalid_1's rmse: 0.979836\n",
      "[600]\ttraining's rmse: 0.822224\tvalid_1's rmse: 0.978646\n",
      "[700]\ttraining's rmse: 0.80447\tvalid_1's rmse: 0.978723\n",
      "[800]\ttraining's rmse: 0.788655\tvalid_1's rmse: 0.978915\n",
      "Early stopping, best iteration is:\n",
      "[736]\ttraining's rmse: 0.798577\tvalid_1's rmse: 0.978558\n",
      "Partial score of fold 1 is: 0.598650051422157\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03001\tvalid_1's rmse: 1.05872\n",
      "[200]\ttraining's rmse: 0.945157\tvalid_1's rmse: 1.00116\n",
      "[300]\ttraining's rmse: 0.901157\tvalid_1's rmse: 0.982621\n",
      "[400]\ttraining's rmse: 0.870181\tvalid_1's rmse: 0.975707\n",
      "[500]\ttraining's rmse: 0.845648\tvalid_1's rmse: 0.972587\n",
      "[600]\ttraining's rmse: 0.824403\tvalid_1's rmse: 0.971381\n",
      "[700]\ttraining's rmse: 0.806004\tvalid_1's rmse: 0.970978\n",
      "[800]\ttraining's rmse: 0.790034\tvalid_1's rmse: 0.971272\n",
      "Early stopping, best iteration is:\n",
      "[721]\ttraining's rmse: 0.802868\tvalid_1's rmse: 0.970816\n",
      "Partial score of fold 2 is: 0.614754151008428\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03033\tvalid_1's rmse: 1.05382\n",
      "[200]\ttraining's rmse: 0.94488\tvalid_1's rmse: 0.994804\n",
      "[300]\ttraining's rmse: 0.900383\tvalid_1's rmse: 0.97673\n",
      "[400]\ttraining's rmse: 0.868786\tvalid_1's rmse: 0.969404\n",
      "[500]\ttraining's rmse: 0.844268\tvalid_1's rmse: 0.965815\n",
      "[600]\ttraining's rmse: 0.824801\tvalid_1's rmse: 0.964828\n",
      "[700]\ttraining's rmse: 0.806688\tvalid_1's rmse: 0.964283\n",
      "[800]\ttraining's rmse: 0.791031\tvalid_1's rmse: 0.9644\n",
      "Early stopping, best iteration is:\n",
      "[710]\ttraining's rmse: 0.804866\tvalid_1's rmse: 0.964212\n",
      "Partial score of fold 3 is: 0.6129648066099534\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02407\tvalid_1's rmse: 1.05033\n",
      "[200]\ttraining's rmse: 0.943386\tvalid_1's rmse: 0.999388\n",
      "[300]\ttraining's rmse: 0.899525\tvalid_1's rmse: 0.982433\n",
      "[400]\ttraining's rmse: 0.868701\tvalid_1's rmse: 0.975326\n",
      "[500]\ttraining's rmse: 0.844701\tvalid_1's rmse: 0.972843\n",
      "[600]\ttraining's rmse: 0.824453\tvalid_1's rmse: 0.971858\n",
      "[700]\ttraining's rmse: 0.806209\tvalid_1's rmse: 0.971431\n",
      "[800]\ttraining's rmse: 0.790035\tvalid_1's rmse: 0.97129\n",
      "[900]\ttraining's rmse: 0.774891\tvalid_1's rmse: 0.971285\n",
      "Early stopping, best iteration is:\n",
      "[884]\ttraining's rmse: 0.777336\tvalid_1's rmse: 0.971164\n",
      "Partial score of fold 4 is: 0.6020406113952642\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6029  \u001b[0m | \u001b[0m 0.7091  \u001b[0m | \u001b[0m 0.7892  \u001b[0m | \u001b[0m 0.2929  \u001b[0m | \u001b[0m 5.878   \u001b[0m | \u001b[0m 14.92   \u001b[0m | \u001b[0m 104.6   \u001b[0m | \u001b[0m 99.17   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01601\tvalid_1's rmse: 1.0518\n",
      "[200]\ttraining's rmse: 0.939467\tvalid_1's rmse: 1.00842\n",
      "[300]\ttraining's rmse: 0.895601\tvalid_1's rmse: 0.99554\n",
      "[400]\ttraining's rmse: 0.861226\tvalid_1's rmse: 0.989358\n",
      "[500]\ttraining's rmse: 0.832968\tvalid_1's rmse: 0.987863\n",
      "[600]\ttraining's rmse: 0.809054\tvalid_1's rmse: 0.988016\n",
      "Early stopping, best iteration is:\n",
      "[542]\ttraining's rmse: 0.822527\tvalid_1's rmse: 0.987554\n",
      "Partial score of fold 0 is: 0.5873771817117673\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01714\tvalid_1's rmse: 1.04858\n",
      "[200]\ttraining's rmse: 0.940505\tvalid_1's rmse: 1.00271\n",
      "[300]\ttraining's rmse: 0.89661\tvalid_1's rmse: 0.989813\n",
      "[400]\ttraining's rmse: 0.863219\tvalid_1's rmse: 0.984842\n",
      "[500]\ttraining's rmse: 0.834596\tvalid_1's rmse: 0.981899\n",
      "[600]\ttraining's rmse: 0.810897\tvalid_1's rmse: 0.981028\n",
      "[700]\ttraining's rmse: 0.789583\tvalid_1's rmse: 0.981119\n",
      "Early stopping, best iteration is:\n",
      "[650]\ttraining's rmse: 0.799927\tvalid_1's rmse: 0.980906\n",
      "Partial score of fold 1 is: 0.5956081659447503\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01808\tvalid_1's rmse: 1.0472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.942995\tvalid_1's rmse: 0.997254\n",
      "[300]\ttraining's rmse: 0.899141\tvalid_1's rmse: 0.980829\n",
      "[400]\ttraining's rmse: 0.865096\tvalid_1's rmse: 0.974571\n",
      "[500]\ttraining's rmse: 0.837421\tvalid_1's rmse: 0.972182\n",
      "[600]\ttraining's rmse: 0.813688\tvalid_1's rmse: 0.971164\n",
      "[700]\ttraining's rmse: 0.792346\tvalid_1's rmse: 0.970849\n",
      "[800]\ttraining's rmse: 0.772989\tvalid_1's rmse: 0.970852\n",
      "Early stopping, best iteration is:\n",
      "[758]\ttraining's rmse: 0.780908\tvalid_1's rmse: 0.970713\n",
      "Partial score of fold 2 is: 0.6120701344107162\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01916\tvalid_1's rmse: 1.04222\n",
      "[200]\ttraining's rmse: 0.943438\tvalid_1's rmse: 0.991804\n",
      "[300]\ttraining's rmse: 0.899247\tvalid_1's rmse: 0.976365\n",
      "[400]\ttraining's rmse: 0.86456\tvalid_1's rmse: 0.969866\n",
      "[500]\ttraining's rmse: 0.836586\tvalid_1's rmse: 0.966203\n",
      "[600]\ttraining's rmse: 0.812701\tvalid_1's rmse: 0.964712\n",
      "[700]\ttraining's rmse: 0.79153\tvalid_1's rmse: 0.964092\n",
      "[800]\ttraining's rmse: 0.772004\tvalid_1's rmse: 0.963843\n",
      "[900]\ttraining's rmse: 0.75487\tvalid_1's rmse: 0.963669\n",
      "[1000]\ttraining's rmse: 0.738334\tvalid_1's rmse: 0.963724\n",
      "Early stopping, best iteration is:\n",
      "[933]\ttraining's rmse: 0.749305\tvalid_1's rmse: 0.963474\n",
      "Partial score of fold 3 is: 0.614754151008428\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01818\tvalid_1's rmse: 1.04271\n",
      "[200]\ttraining's rmse: 0.94258\tvalid_1's rmse: 0.99535\n",
      "[300]\ttraining's rmse: 0.898575\tvalid_1's rmse: 0.981303\n",
      "[400]\ttraining's rmse: 0.864646\tvalid_1's rmse: 0.975081\n",
      "[500]\ttraining's rmse: 0.836699\tvalid_1's rmse: 0.972496\n",
      "[600]\ttraining's rmse: 0.812778\tvalid_1's rmse: 0.971971\n",
      "[700]\ttraining's rmse: 0.790917\tvalid_1's rmse: 0.971758\n",
      "[800]\ttraining's rmse: 0.772044\tvalid_1's rmse: 0.971762\n",
      "Early stopping, best iteration is:\n",
      "[770]\ttraining's rmse: 0.777315\tvalid_1's rmse: 0.971563\n",
      "Partial score of fold 4 is: 0.6015036749590986\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6021  \u001b[0m | \u001b[0m 0.8771  \u001b[0m | \u001b[0m 0.9634  \u001b[0m | \u001b[0m 5.984   \u001b[0m | \u001b[0m 0.3819  \u001b[0m | \u001b[0m 14.55   \u001b[0m | \u001b[0m 101.9   \u001b[0m | \u001b[0m 128.2   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02458\tvalid_1's rmse: 1.05873\n",
      "[200]\ttraining's rmse: 0.947256\tvalid_1's rmse: 1.0112\n",
      "[300]\ttraining's rmse: 0.905448\tvalid_1's rmse: 0.995635\n",
      "[400]\ttraining's rmse: 0.876698\tvalid_1's rmse: 0.990003\n",
      "[500]\ttraining's rmse: 0.854452\tvalid_1's rmse: 0.988012\n",
      "[600]\ttraining's rmse: 0.836167\tvalid_1's rmse: 0.987862\n",
      "Early stopping, best iteration is:\n",
      "[547]\ttraining's rmse: 0.84531\tvalid_1's rmse: 0.98751\n",
      "Partial score of fold 0 is: 0.5893454605500893\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0258\tvalid_1's rmse: 1.055\n",
      "[200]\ttraining's rmse: 0.947594\tvalid_1's rmse: 1.00535\n",
      "[300]\ttraining's rmse: 0.906138\tvalid_1's rmse: 0.990036\n",
      "[400]\ttraining's rmse: 0.878028\tvalid_1's rmse: 0.985153\n",
      "[500]\ttraining's rmse: 0.855151\tvalid_1's rmse: 0.982515\n",
      "[600]\ttraining's rmse: 0.836928\tvalid_1's rmse: 0.981359\n",
      "[700]\ttraining's rmse: 0.820799\tvalid_1's rmse: 0.980559\n",
      "[800]\ttraining's rmse: 0.806603\tvalid_1's rmse: 0.980807\n",
      "Early stopping, best iteration is:\n",
      "[752]\ttraining's rmse: 0.813092\tvalid_1's rmse: 0.980492\n",
      "Partial score of fold 1 is: 0.59703964146353\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02659\tvalid_1's rmse: 1.05434\n",
      "[200]\ttraining's rmse: 0.949465\tvalid_1's rmse: 1.00206\n",
      "[300]\ttraining's rmse: 0.908942\tvalid_1's rmse: 0.98341\n",
      "[400]\ttraining's rmse: 0.880929\tvalid_1's rmse: 0.976327\n",
      "[500]\ttraining's rmse: 0.858258\tvalid_1's rmse: 0.973106\n",
      "[600]\ttraining's rmse: 0.839579\tvalid_1's rmse: 0.971482\n",
      "[700]\ttraining's rmse: 0.822448\tvalid_1's rmse: 0.970793\n",
      "Early stopping, best iteration is:\n",
      "[686]\ttraining's rmse: 0.824964\tvalid_1's rmse: 0.970767\n",
      "Partial score of fold 2 is: 0.6111754622114789\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02753\tvalid_1's rmse: 1.04864\n",
      "[200]\ttraining's rmse: 0.950322\tvalid_1's rmse: 0.995301\n",
      "[300]\ttraining's rmse: 0.90869\tvalid_1's rmse: 0.977109\n",
      "[400]\ttraining's rmse: 0.880367\tvalid_1's rmse: 0.970069\n",
      "[500]\ttraining's rmse: 0.858045\tvalid_1's rmse: 0.966475\n",
      "[600]\ttraining's rmse: 0.839799\tvalid_1's rmse: 0.96487\n",
      "[700]\ttraining's rmse: 0.823912\tvalid_1's rmse: 0.96434\n",
      "[800]\ttraining's rmse: 0.809765\tvalid_1's rmse: 0.964236\n",
      "[900]\ttraining's rmse: 0.796154\tvalid_1's rmse: 0.963518\n",
      "[1000]\ttraining's rmse: 0.784318\tvalid_1's rmse: 0.963657\n",
      "Early stopping, best iteration is:\n",
      "[952]\ttraining's rmse: 0.789983\tvalid_1's rmse: 0.963453\n",
      "Partial score of fold 3 is: 0.6149330854482755\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02616\tvalid_1's rmse: 1.05156\n",
      "[200]\ttraining's rmse: 0.948902\tvalid_1's rmse: 0.999847\n",
      "[300]\ttraining's rmse: 0.907976\tvalid_1's rmse: 0.982623\n",
      "[400]\ttraining's rmse: 0.879332\tvalid_1's rmse: 0.976033\n",
      "[500]\ttraining's rmse: 0.85698\tvalid_1's rmse: 0.972571\n",
      "[600]\ttraining's rmse: 0.838764\tvalid_1's rmse: 0.971895\n",
      "[700]\ttraining's rmse: 0.823173\tvalid_1's rmse: 0.971478\n",
      "[800]\ttraining's rmse: 0.809095\tvalid_1's rmse: 0.971805\n",
      "Early stopping, best iteration is:\n",
      "[726]\ttraining's rmse: 0.819503\tvalid_1's rmse: 0.971377\n",
      "Partial score of fold 4 is: 0.5997138868385467\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6034  \u001b[0m | \u001b[0m 0.8688  \u001b[0m | \u001b[0m 0.7617  \u001b[0m | \u001b[0m 0.219   \u001b[0m | \u001b[0m 0.2499  \u001b[0m | \u001b[0m 14.91   \u001b[0m | \u001b[0m 149.2   \u001b[0m | \u001b[0m 107.0   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02321\tvalid_1's rmse: 1.06787\n",
      "[200]\ttraining's rmse: 0.926522\tvalid_1's rmse: 1.01339\n",
      "[300]\ttraining's rmse: 0.876045\tvalid_1's rmse: 0.997544\n",
      "[400]\ttraining's rmse: 0.840296\tvalid_1's rmse: 0.99201\n",
      "[500]\ttraining's rmse: 0.812049\tvalid_1's rmse: 0.989815\n",
      "[600]\ttraining's rmse: 0.788751\tvalid_1's rmse: 0.99009\n",
      "Early stopping, best iteration is:\n",
      "[523]\ttraining's rmse: 0.80626\tvalid_1's rmse: 0.989674\n",
      "Partial score of fold 0 is: 0.5837984929148182\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01704\tvalid_1's rmse: 1.05718\n",
      "[200]\ttraining's rmse: 0.92727\tvalid_1's rmse: 1.00524\n",
      "[300]\ttraining's rmse: 0.875442\tvalid_1's rmse: 0.989245\n",
      "[400]\ttraining's rmse: 0.840252\tvalid_1's rmse: 0.984646\n",
      "[500]\ttraining's rmse: 0.81415\tvalid_1's rmse: 0.982727\n",
      "[600]\ttraining's rmse: 0.792021\tvalid_1's rmse: 0.981859\n",
      "[700]\ttraining's rmse: 0.772008\tvalid_1's rmse: 0.981677\n",
      "Early stopping, best iteration is:\n",
      "[693]\ttraining's rmse: 0.773244\tvalid_1's rmse: 0.981633\n",
      "Partial score of fold 1 is: 0.592745214907191\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02545\tvalid_1's rmse: 1.06415\n",
      "[200]\ttraining's rmse: 0.930133\tvalid_1's rmse: 1.00275\n",
      "[300]\ttraining's rmse: 0.878704\tvalid_1's rmse: 0.982383\n",
      "[400]\ttraining's rmse: 0.843386\tvalid_1's rmse: 0.975406\n",
      "[500]\ttraining's rmse: 0.815988\tvalid_1's rmse: 0.972603\n",
      "[600]\ttraining's rmse: 0.790964\tvalid_1's rmse: 0.971722\n",
      "[700]\ttraining's rmse: 0.771541\tvalid_1's rmse: 0.971403\n",
      "Early stopping, best iteration is:\n",
      "[674]\ttraining's rmse: 0.775834\tvalid_1's rmse: 0.97125\n",
      "Partial score of fold 2 is: 0.6113543966513264\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01881\tvalid_1's rmse: 1.05029\n",
      "[200]\ttraining's rmse: 0.929145\tvalid_1's rmse: 0.994654\n",
      "[300]\ttraining's rmse: 0.877753\tvalid_1's rmse: 0.975762\n",
      "[400]\ttraining's rmse: 0.842491\tvalid_1's rmse: 0.968917\n",
      "[500]\ttraining's rmse: 0.814118\tvalid_1's rmse: 0.965605\n",
      "[600]\ttraining's rmse: 0.790753\tvalid_1's rmse: 0.964163\n",
      "[700]\ttraining's rmse: 0.771142\tvalid_1's rmse: 0.964012\n",
      "Early stopping, best iteration is:\n",
      "[667]\ttraining's rmse: 0.777409\tvalid_1's rmse: 0.963874\n",
      "Partial score of fold 3 is: 0.6126069377302585\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01785\tvalid_1's rmse: 1.05282\n",
      "[200]\ttraining's rmse: 0.928159\tvalid_1's rmse: 1.00073\n",
      "[300]\ttraining's rmse: 0.877167\tvalid_1's rmse: 0.982131\n",
      "[400]\ttraining's rmse: 0.842495\tvalid_1's rmse: 0.975789\n",
      "[500]\ttraining's rmse: 0.81524\tvalid_1's rmse: 0.973435\n",
      "[600]\ttraining's rmse: 0.7919\tvalid_1's rmse: 0.972472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\ttraining's rmse: 0.772176\tvalid_1's rmse: 0.97226\n",
      "[800]\ttraining's rmse: 0.754954\tvalid_1's rmse: 0.972295\n",
      "Early stopping, best iteration is:\n",
      "[713]\ttraining's rmse: 0.769705\tvalid_1's rmse: 0.97213\n",
      "Partial score of fold 4 is: 0.6023985690193746\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6017  \u001b[0m | \u001b[0m 0.9894  \u001b[0m | \u001b[0m 0.7119  \u001b[0m | \u001b[0m 0.1701  \u001b[0m | \u001b[0m 5.928   \u001b[0m | \u001b[0m 14.93   \u001b[0m | \u001b[0m 111.3   \u001b[0m | \u001b[0m 100.6   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.993981\tvalid_1's rmse: 1.04559\n",
      "[200]\ttraining's rmse: 0.908024\tvalid_1's rmse: 1.00362\n",
      "[300]\ttraining's rmse: 0.860414\tvalid_1's rmse: 0.992347\n",
      "[400]\ttraining's rmse: 0.825084\tvalid_1's rmse: 0.988897\n",
      "[500]\ttraining's rmse: 0.798292\tvalid_1's rmse: 0.988109\n",
      "Early stopping, best iteration is:\n",
      "[470]\ttraining's rmse: 0.805481\tvalid_1's rmse: 0.987985\n",
      "Partial score of fold 0 is: 0.5830827551554284\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.995162\tvalid_1's rmse: 1.04145\n",
      "[200]\ttraining's rmse: 0.907946\tvalid_1's rmse: 0.99757\n",
      "[300]\ttraining's rmse: 0.860297\tvalid_1's rmse: 0.986872\n",
      "[400]\ttraining's rmse: 0.825969\tvalid_1's rmse: 0.983511\n",
      "[500]\ttraining's rmse: 0.799326\tvalid_1's rmse: 0.981972\n",
      "[600]\ttraining's rmse: 0.777253\tvalid_1's rmse: 0.981635\n",
      "Early stopping, best iteration is:\n",
      "[588]\ttraining's rmse: 0.779867\tvalid_1's rmse: 0.981497\n",
      "Partial score of fold 1 is: 0.5938188215462756\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.996322\tvalid_1's rmse: 1.03936\n",
      "[200]\ttraining's rmse: 0.91083\tvalid_1's rmse: 0.991085\n",
      "[300]\ttraining's rmse: 0.863515\tvalid_1's rmse: 0.977607\n",
      "[400]\ttraining's rmse: 0.828939\tvalid_1's rmse: 0.973187\n",
      "[500]\ttraining's rmse: 0.801909\tvalid_1's rmse: 0.971983\n",
      "[600]\ttraining's rmse: 0.77947\tvalid_1's rmse: 0.971501\n",
      "[700]\ttraining's rmse: 0.759434\tvalid_1's rmse: 0.971361\n",
      "Early stopping, best iteration is:\n",
      "[655]\ttraining's rmse: 0.768327\tvalid_1's rmse: 0.97117\n",
      "Partial score of fold 2 is: 0.612785872170106\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.997588\tvalid_1's rmse: 1.03415\n",
      "[200]\ttraining's rmse: 0.91108\tvalid_1's rmse: 0.985683\n",
      "[300]\ttraining's rmse: 0.863126\tvalid_1's rmse: 0.97191\n",
      "[400]\ttraining's rmse: 0.827889\tvalid_1's rmse: 0.967371\n",
      "[500]\ttraining's rmse: 0.801206\tvalid_1's rmse: 0.96566\n",
      "[600]\ttraining's rmse: 0.780193\tvalid_1's rmse: 0.965531\n",
      "Early stopping, best iteration is:\n",
      "[585]\ttraining's rmse: 0.7832\tvalid_1's rmse: 0.965457\n",
      "Partial score of fold 3 is: 0.6113543966513264\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.996586\tvalid_1's rmse: 1.0347\n",
      "[200]\ttraining's rmse: 0.910935\tvalid_1's rmse: 0.988448\n",
      "[300]\ttraining's rmse: 0.863113\tvalid_1's rmse: 0.975785\n",
      "[400]\ttraining's rmse: 0.82823\tvalid_1's rmse: 0.972128\n",
      "[500]\ttraining's rmse: 0.801047\tvalid_1's rmse: 0.969766\n",
      "Early stopping, best iteration is:\n",
      "[498]\ttraining's rmse: 0.801662\tvalid_1's rmse: 0.969743\n",
      "Partial score of fold 4 is: 0.6015036749590986\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6005  \u001b[0m | \u001b[0m 0.8296  \u001b[0m | \u001b[0m 0.9888  \u001b[0m | \u001b[0m 0.09439 \u001b[0m | \u001b[0m 0.3821  \u001b[0m | \u001b[0m 14.9    \u001b[0m | \u001b[0m 92.67   \u001b[0m | \u001b[0m 112.5   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03373\tvalid_1's rmse: 1.06175\n",
      "[200]\ttraining's rmse: 0.961638\tvalid_1's rmse: 1.01437\n",
      "[300]\ttraining's rmse: 0.921734\tvalid_1's rmse: 0.998287\n",
      "[400]\ttraining's rmse: 0.892096\tvalid_1's rmse: 0.991543\n",
      "[500]\ttraining's rmse: 0.868123\tvalid_1's rmse: 0.988379\n",
      "[600]\ttraining's rmse: 0.847769\tvalid_1's rmse: 0.987435\n",
      "[700]\ttraining's rmse: 0.830324\tvalid_1's rmse: 0.987593\n",
      "Early stopping, best iteration is:\n",
      "[642]\ttraining's rmse: 0.840076\tvalid_1's rmse: 0.987312\n",
      "Partial score of fold 0 is: 0.5866614439523775\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0345\tvalid_1's rmse: 1.0587\n",
      "[200]\ttraining's rmse: 0.961758\tvalid_1's rmse: 1.00924\n",
      "[300]\ttraining's rmse: 0.92173\tvalid_1's rmse: 0.992287\n",
      "[400]\ttraining's rmse: 0.892694\tvalid_1's rmse: 0.985707\n",
      "[500]\ttraining's rmse: 0.868878\tvalid_1's rmse: 0.982535\n",
      "[600]\ttraining's rmse: 0.848825\tvalid_1's rmse: 0.981063\n",
      "[700]\ttraining's rmse: 0.831369\tvalid_1's rmse: 0.980305\n",
      "[800]\ttraining's rmse: 0.815288\tvalid_1's rmse: 0.980094\n",
      "[900]\ttraining's rmse: 0.800776\tvalid_1's rmse: 0.980004\n",
      "[1000]\ttraining's rmse: 0.787305\tvalid_1's rmse: 0.980342\n",
      "Early stopping, best iteration is:\n",
      "[908]\ttraining's rmse: 0.79968\tvalid_1's rmse: 0.979947\n",
      "Partial score of fold 1 is: 0.59703964146353\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03524\tvalid_1's rmse: 1.05797\n",
      "[200]\ttraining's rmse: 0.963464\tvalid_1's rmse: 1.00574\n",
      "[300]\ttraining's rmse: 0.923963\tvalid_1's rmse: 0.986103\n",
      "[400]\ttraining's rmse: 0.89505\tvalid_1's rmse: 0.977194\n",
      "[500]\ttraining's rmse: 0.871183\tvalid_1's rmse: 0.973235\n",
      "[600]\ttraining's rmse: 0.85046\tvalid_1's rmse: 0.971575\n",
      "[700]\ttraining's rmse: 0.832847\tvalid_1's rmse: 0.970801\n",
      "[800]\ttraining's rmse: 0.816645\tvalid_1's rmse: 0.970775\n",
      "Early stopping, best iteration is:\n",
      "[721]\ttraining's rmse: 0.829453\tvalid_1's rmse: 0.970652\n",
      "Partial score of fold 2 is: 0.6111754622114789\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03675\tvalid_1's rmse: 1.05244\n",
      "[200]\ttraining's rmse: 0.964609\tvalid_1's rmse: 0.999002\n",
      "[300]\ttraining's rmse: 0.924706\tvalid_1's rmse: 0.980364\n",
      "[400]\ttraining's rmse: 0.895634\tvalid_1's rmse: 0.971921\n",
      "[500]\ttraining's rmse: 0.871518\tvalid_1's rmse: 0.96765\n",
      "[600]\ttraining's rmse: 0.851529\tvalid_1's rmse: 0.965577\n",
      "[700]\ttraining's rmse: 0.834052\tvalid_1's rmse: 0.964758\n",
      "[800]\ttraining's rmse: 0.817685\tvalid_1's rmse: 0.964291\n",
      "[900]\ttraining's rmse: 0.80292\tvalid_1's rmse: 0.964201\n",
      "[1000]\ttraining's rmse: 0.789053\tvalid_1's rmse: 0.964109\n",
      "Early stopping, best iteration is:\n",
      "[959]\ttraining's rmse: 0.794573\tvalid_1's rmse: 0.963834\n",
      "Partial score of fold 3 is: 0.6158277576475127\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03573\tvalid_1's rmse: 1.05395\n",
      "[200]\ttraining's rmse: 0.963453\tvalid_1's rmse: 1.00329\n",
      "[300]\ttraining's rmse: 0.92362\tvalid_1's rmse: 0.985124\n",
      "[400]\ttraining's rmse: 0.894941\tvalid_1's rmse: 0.977611\n",
      "[500]\ttraining's rmse: 0.870948\tvalid_1's rmse: 0.973996\n",
      "[600]\ttraining's rmse: 0.850808\tvalid_1's rmse: 0.972437\n",
      "[700]\ttraining's rmse: 0.832863\tvalid_1's rmse: 0.971509\n",
      "[800]\ttraining's rmse: 0.816904\tvalid_1's rmse: 0.971381\n",
      "Early stopping, best iteration is:\n",
      "[745]\ttraining's rmse: 0.825402\tvalid_1's rmse: 0.97129\n",
      "Partial score of fold 4 is: 0.5998928656506018\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.6031  \u001b[0m | \u001b[0m 0.931   \u001b[0m | \u001b[0m 0.7945  \u001b[0m | \u001b[0m 5.56    \u001b[0m | \u001b[0m 4.312   \u001b[0m | \u001b[0m 14.88   \u001b[0m | \u001b[0m 148.3   \u001b[0m | \u001b[0m 95.91   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02814\tvalid_1's rmse: 1.05657\n",
      "[200]\ttraining's rmse: 0.958164\tvalid_1's rmse: 1.01162\n",
      "[300]\ttraining's rmse: 0.91937\tvalid_1's rmse: 0.996581\n",
      "[400]\ttraining's rmse: 0.889694\tvalid_1's rmse: 0.990157\n",
      "[500]\ttraining's rmse: 0.865895\tvalid_1's rmse: 0.987707\n",
      "[600]\ttraining's rmse: 0.846404\tvalid_1's rmse: 0.986966\n",
      "[700]\ttraining's rmse: 0.82851\tvalid_1's rmse: 0.986893\n",
      "Early stopping, best iteration is:\n",
      "[629]\ttraining's rmse: 0.840988\tvalid_1's rmse: 0.986555\n",
      "Partial score of fold 0 is: 0.590419067189174\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02905\tvalid_1's rmse: 1.05245\n",
      "[200]\ttraining's rmse: 0.958393\tvalid_1's rmse: 1.0057\n",
      "[300]\ttraining's rmse: 0.919986\tvalid_1's rmse: 0.991307\n",
      "[400]\ttraining's rmse: 0.891062\tvalid_1's rmse: 0.985004\n",
      "[500]\ttraining's rmse: 0.867372\tvalid_1's rmse: 0.981758\n",
      "[600]\ttraining's rmse: 0.847118\tvalid_1's rmse: 0.980283\n",
      "[700]\ttraining's rmse: 0.829643\tvalid_1's rmse: 0.980148\n",
      "[800]\ttraining's rmse: 0.813617\tvalid_1's rmse: 0.979936\n",
      "Early stopping, best iteration is:\n",
      "[776]\ttraining's rmse: 0.81727\tvalid_1's rmse: 0.979801\n",
      "Partial score of fold 1 is: 0.5993592357712716\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0298\tvalid_1's rmse: 1.0515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.960133\tvalid_1's rmse: 1.00099\n",
      "[300]\ttraining's rmse: 0.922236\tvalid_1's rmse: 0.984267\n",
      "[400]\ttraining's rmse: 0.89324\tvalid_1's rmse: 0.976935\n",
      "[500]\ttraining's rmse: 0.868894\tvalid_1's rmse: 0.973516\n",
      "[600]\ttraining's rmse: 0.84872\tvalid_1's rmse: 0.972228\n",
      "[700]\ttraining's rmse: 0.831125\tvalid_1's rmse: 0.971194\n",
      "[800]\ttraining's rmse: 0.814704\tvalid_1's rmse: 0.970701\n",
      "Early stopping, best iteration is:\n",
      "[795]\ttraining's rmse: 0.815357\tvalid_1's rmse: 0.970622\n",
      "Partial score of fold 2 is: 0.6129648066099534\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03118\tvalid_1's rmse: 1.04697\n",
      "[200]\ttraining's rmse: 0.961205\tvalid_1's rmse: 0.996251\n",
      "[300]\ttraining's rmse: 0.922608\tvalid_1's rmse: 0.979433\n",
      "[400]\ttraining's rmse: 0.892794\tvalid_1's rmse: 0.97205\n",
      "[500]\ttraining's rmse: 0.869303\tvalid_1's rmse: 0.968469\n",
      "[600]\ttraining's rmse: 0.849649\tvalid_1's rmse: 0.966829\n",
      "[700]\ttraining's rmse: 0.831587\tvalid_1's rmse: 0.966032\n",
      "[800]\ttraining's rmse: 0.815689\tvalid_1's rmse: 0.965889\n",
      "[900]\ttraining's rmse: 0.800961\tvalid_1's rmse: 0.965717\n",
      "[1000]\ttraining's rmse: 0.786776\tvalid_1's rmse: 0.965327\n",
      "Early stopping, best iteration is:\n",
      "[979]\ttraining's rmse: 0.789876\tvalid_1's rmse: 0.965203\n",
      "Partial score of fold 3 is: 0.612785872170106\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0306\tvalid_1's rmse: 1.04783\n",
      "[200]\ttraining's rmse: 0.96073\tvalid_1's rmse: 0.999365\n",
      "[300]\ttraining's rmse: 0.922379\tvalid_1's rmse: 0.983865\n",
      "[400]\ttraining's rmse: 0.893207\tvalid_1's rmse: 0.977003\n",
      "[500]\ttraining's rmse: 0.869447\tvalid_1's rmse: 0.97423\n",
      "[600]\ttraining's rmse: 0.849548\tvalid_1's rmse: 0.973534\n",
      "[700]\ttraining's rmse: 0.832011\tvalid_1's rmse: 0.972965\n",
      "Early stopping, best iteration is:\n",
      "[696]\ttraining's rmse: 0.832717\tvalid_1's rmse: 0.972928\n",
      "Partial score of fold 4 is: 0.6020406113952642\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.6028  \u001b[0m | \u001b[0m 0.9052  \u001b[0m | \u001b[0m 0.9753  \u001b[0m | \u001b[0m 5.377   \u001b[0m | \u001b[0m 5.467   \u001b[0m | \u001b[0m 14.98   \u001b[0m | \u001b[0m 146.8   \u001b[0m | \u001b[0m 115.9   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03226\tvalid_1's rmse: 1.06094\n",
      "[200]\ttraining's rmse: 0.960225\tvalid_1's rmse: 1.01422\n",
      "[300]\ttraining's rmse: 0.920627\tvalid_1's rmse: 0.998437\n",
      "[400]\ttraining's rmse: 0.891036\tvalid_1's rmse: 0.991266\n",
      "[500]\ttraining's rmse: 0.867233\tvalid_1's rmse: 0.988045\n",
      "[600]\ttraining's rmse: 0.847412\tvalid_1's rmse: 0.987273\n",
      "[700]\ttraining's rmse: 0.829146\tvalid_1's rmse: 0.987087\n",
      "Early stopping, best iteration is:\n",
      "[662]\ttraining's rmse: 0.835679\tvalid_1's rmse: 0.986899\n",
      "Partial score of fold 0 is: 0.5886297227906995\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03315\tvalid_1's rmse: 1.05766\n",
      "[200]\ttraining's rmse: 0.961035\tvalid_1's rmse: 1.00909\n",
      "[300]\ttraining's rmse: 0.921707\tvalid_1's rmse: 0.992871\n",
      "[400]\ttraining's rmse: 0.892841\tvalid_1's rmse: 0.986355\n",
      "[500]\ttraining's rmse: 0.869163\tvalid_1's rmse: 0.982939\n",
      "[600]\ttraining's rmse: 0.849075\tvalid_1's rmse: 0.981698\n",
      "[700]\ttraining's rmse: 0.831594\tvalid_1's rmse: 0.981397\n",
      "[800]\ttraining's rmse: 0.815497\tvalid_1's rmse: 0.981358\n",
      "Early stopping, best iteration is:\n",
      "[751]\ttraining's rmse: 0.823047\tvalid_1's rmse: 0.98123\n",
      "Partial score of fold 1 is: 0.595848687495135\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03383\tvalid_1's rmse: 1.05719\n",
      "[200]\ttraining's rmse: 0.962279\tvalid_1's rmse: 1.0056\n",
      "[300]\ttraining's rmse: 0.923359\tvalid_1's rmse: 0.98648\n",
      "[400]\ttraining's rmse: 0.894293\tvalid_1's rmse: 0.978151\n",
      "[500]\ttraining's rmse: 0.870744\tvalid_1's rmse: 0.97424\n",
      "[600]\ttraining's rmse: 0.850726\tvalid_1's rmse: 0.972507\n",
      "[700]\ttraining's rmse: 0.833333\tvalid_1's rmse: 0.971899\n",
      "[800]\ttraining's rmse: 0.817443\tvalid_1's rmse: 0.971864\n",
      "[900]\ttraining's rmse: 0.802393\tvalid_1's rmse: 0.971694\n",
      "Early stopping, best iteration is:\n",
      "[853]\ttraining's rmse: 0.809123\tvalid_1's rmse: 0.971556\n",
      "Partial score of fold 2 is: 0.6099229211325468\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03506\tvalid_1's rmse: 1.05137\n",
      "[200]\ttraining's rmse: 0.963361\tvalid_1's rmse: 0.998323\n",
      "[300]\ttraining's rmse: 0.924031\tvalid_1's rmse: 0.979807\n",
      "[400]\ttraining's rmse: 0.895476\tvalid_1's rmse: 0.971524\n",
      "[500]\ttraining's rmse: 0.871547\tvalid_1's rmse: 0.967501\n",
      "[600]\ttraining's rmse: 0.852025\tvalid_1's rmse: 0.965517\n",
      "[700]\ttraining's rmse: 0.834541\tvalid_1's rmse: 0.964557\n",
      "[800]\ttraining's rmse: 0.818708\tvalid_1's rmse: 0.964114\n",
      "[900]\ttraining's rmse: 0.803829\tvalid_1's rmse: 0.963474\n",
      "[1000]\ttraining's rmse: 0.789776\tvalid_1's rmse: 0.963277\n",
      "Early stopping, best iteration is:\n",
      "[947]\ttraining's rmse: 0.796958\tvalid_1's rmse: 0.963124\n",
      "Partial score of fold 3 is: 0.6156488232076653\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03389\tvalid_1's rmse: 1.05366\n",
      "[200]\ttraining's rmse: 0.962173\tvalid_1's rmse: 1.00343\n",
      "[300]\ttraining's rmse: 0.923031\tvalid_1's rmse: 0.985877\n",
      "[400]\ttraining's rmse: 0.894261\tvalid_1's rmse: 0.978175\n",
      "[500]\ttraining's rmse: 0.870552\tvalid_1's rmse: 0.974335\n",
      "[600]\ttraining's rmse: 0.850504\tvalid_1's rmse: 0.972906\n",
      "[700]\ttraining's rmse: 0.832675\tvalid_1's rmse: 0.972369\n",
      "[800]\ttraining's rmse: 0.816643\tvalid_1's rmse: 0.972194\n",
      "Early stopping, best iteration is:\n",
      "[780]\ttraining's rmse: 0.8196\tvalid_1's rmse: 0.972027\n",
      "Partial score of fold 4 is: 0.600071844462657\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.6034  \u001b[0m | \u001b[0m 0.9193  \u001b[0m | \u001b[0m 0.781   \u001b[0m | \u001b[0m 5.965   \u001b[0m | \u001b[0m 0.1736  \u001b[0m | \u001b[0m 14.96   \u001b[0m | \u001b[0m 149.0   \u001b[0m | \u001b[0m 114.5   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03619\tvalid_1's rmse: 1.0646\n",
      "[200]\ttraining's rmse: 0.962091\tvalid_1's rmse: 1.01516\n",
      "[300]\ttraining's rmse: 0.921234\tvalid_1's rmse: 0.998193\n",
      "[400]\ttraining's rmse: 0.891367\tvalid_1's rmse: 0.990914\n",
      "[500]\ttraining's rmse: 0.86767\tvalid_1's rmse: 0.988367\n",
      "[600]\ttraining's rmse: 0.846722\tvalid_1's rmse: 0.987448\n",
      "[700]\ttraining's rmse: 0.82813\tvalid_1's rmse: 0.987481\n",
      "Early stopping, best iteration is:\n",
      "[622]\ttraining's rmse: 0.842406\tvalid_1's rmse: 0.987321\n",
      "Partial score of fold 0 is: 0.584872099553903\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03761\tvalid_1's rmse: 1.06123\n",
      "[200]\ttraining's rmse: 0.963108\tvalid_1's rmse: 1.01011\n",
      "[300]\ttraining's rmse: 0.922466\tvalid_1's rmse: 0.992408\n",
      "[400]\ttraining's rmse: 0.89305\tvalid_1's rmse: 0.985707\n",
      "[500]\ttraining's rmse: 0.868705\tvalid_1's rmse: 0.982361\n",
      "[600]\ttraining's rmse: 0.847875\tvalid_1's rmse: 0.980568\n",
      "[700]\ttraining's rmse: 0.829564\tvalid_1's rmse: 0.980217\n",
      "[800]\ttraining's rmse: 0.812681\tvalid_1's rmse: 0.980126\n",
      "[900]\ttraining's rmse: 0.797355\tvalid_1's rmse: 0.980277\n",
      "Early stopping, best iteration is:\n",
      "[836]\ttraining's rmse: 0.806978\tvalid_1's rmse: 0.979961\n",
      "Partial score of fold 1 is: 0.5954292315049028\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03836\tvalid_1's rmse: 1.06074\n",
      "[200]\ttraining's rmse: 0.96475\tvalid_1's rmse: 1.00674\n",
      "[300]\ttraining's rmse: 0.924768\tvalid_1's rmse: 0.987148\n",
      "[400]\ttraining's rmse: 0.895166\tvalid_1's rmse: 0.978281\n",
      "[500]\ttraining's rmse: 0.870977\tvalid_1's rmse: 0.97387\n",
      "[600]\ttraining's rmse: 0.850216\tvalid_1's rmse: 0.971902\n",
      "[700]\ttraining's rmse: 0.83127\tvalid_1's rmse: 0.971142\n",
      "[800]\ttraining's rmse: 0.81508\tvalid_1's rmse: 0.970763\n",
      "[900]\ttraining's rmse: 0.799796\tvalid_1's rmse: 0.970605\n",
      "Early stopping, best iteration is:\n",
      "[893]\ttraining's rmse: 0.800838\tvalid_1's rmse: 0.970587\n",
      "Partial score of fold 2 is: 0.6142173476888857\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03969\tvalid_1's rmse: 1.05508\n",
      "[200]\ttraining's rmse: 0.965589\tvalid_1's rmse: 0.999894\n",
      "[300]\ttraining's rmse: 0.92504\tvalid_1's rmse: 0.979803\n",
      "[400]\ttraining's rmse: 0.895959\tvalid_1's rmse: 0.970874\n",
      "[500]\ttraining's rmse: 0.87189\tvalid_1's rmse: 0.96664\n",
      "[600]\ttraining's rmse: 0.850918\tvalid_1's rmse: 0.964246\n",
      "[700]\ttraining's rmse: 0.832742\tvalid_1's rmse: 0.963077\n",
      "[800]\ttraining's rmse: 0.816226\tvalid_1's rmse: 0.962665\n",
      "[900]\ttraining's rmse: 0.80084\tvalid_1's rmse: 0.96255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's rmse: 0.786603\tvalid_1's rmse: 0.962515\n",
      "Early stopping, best iteration is:\n",
      "[958]\ttraining's rmse: 0.792512\tvalid_1's rmse: 0.962346\n",
      "Partial score of fold 3 is: 0.6160066920873601\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03826\tvalid_1's rmse: 1.05723\n",
      "[200]\ttraining's rmse: 0.96441\tvalid_1's rmse: 1.00471\n",
      "[300]\ttraining's rmse: 0.924263\tvalid_1's rmse: 0.986238\n",
      "[400]\ttraining's rmse: 0.89506\tvalid_1's rmse: 0.97806\n",
      "[500]\ttraining's rmse: 0.871315\tvalid_1's rmse: 0.974433\n",
      "[600]\ttraining's rmse: 0.850359\tvalid_1's rmse: 0.972622\n",
      "[700]\ttraining's rmse: 0.831465\tvalid_1's rmse: 0.97186\n",
      "[800]\ttraining's rmse: 0.815156\tvalid_1's rmse: 0.971695\n",
      "Early stopping, best iteration is:\n",
      "[777]\ttraining's rmse: 0.818622\tvalid_1's rmse: 0.971528\n",
      "Partial score of fold 4 is: 0.6023985690193746\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.6027  \u001b[0m | \u001b[0m 0.8779  \u001b[0m | \u001b[0m 0.7434  \u001b[0m | \u001b[0m 5.956   \u001b[0m | \u001b[0m 5.715   \u001b[0m | \u001b[0m 14.84   \u001b[0m | \u001b[0m 132.3   \u001b[0m | \u001b[0m 117.6   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01746\tvalid_1's rmse: 1.05257\n",
      "[200]\ttraining's rmse: 0.943626\tvalid_1's rmse: 1.00879\n",
      "[300]\ttraining's rmse: 0.903758\tvalid_1's rmse: 0.994661\n",
      "[400]\ttraining's rmse: 0.875666\tvalid_1's rmse: 0.989602\n",
      "[500]\ttraining's rmse: 0.853037\tvalid_1's rmse: 0.987815\n",
      "[600]\ttraining's rmse: 0.834551\tvalid_1's rmse: 0.987555\n",
      "Early stopping, best iteration is:\n",
      "[549]\ttraining's rmse: 0.843537\tvalid_1's rmse: 0.987436\n",
      "Partial score of fold 0 is: 0.5873771817117673\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01866\tvalid_1's rmse: 1.04868\n",
      "[200]\ttraining's rmse: 0.944359\tvalid_1's rmse: 1.00259\n",
      "[300]\ttraining's rmse: 0.90413\tvalid_1's rmse: 0.989633\n",
      "[400]\ttraining's rmse: 0.875859\tvalid_1's rmse: 0.98484\n",
      "[500]\ttraining's rmse: 0.853389\tvalid_1's rmse: 0.982331\n",
      "[600]\ttraining's rmse: 0.835631\tvalid_1's rmse: 0.981369\n",
      "[700]\ttraining's rmse: 0.819719\tvalid_1's rmse: 0.981425\n",
      "Early stopping, best iteration is:\n",
      "[654]\ttraining's rmse: 0.826853\tvalid_1's rmse: 0.981063\n",
      "Partial score of fold 1 is: 0.5968607070236824\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01972\tvalid_1's rmse: 1.04744\n",
      "[200]\ttraining's rmse: 0.946543\tvalid_1's rmse: 0.997915\n",
      "[300]\ttraining's rmse: 0.907242\tvalid_1's rmse: 0.982042\n",
      "[400]\ttraining's rmse: 0.878649\tvalid_1's rmse: 0.975791\n",
      "[500]\ttraining's rmse: 0.856635\tvalid_1's rmse: 0.973319\n",
      "[600]\ttraining's rmse: 0.838531\tvalid_1's rmse: 0.971962\n",
      "[700]\ttraining's rmse: 0.822403\tvalid_1's rmse: 0.971706\n",
      "[800]\ttraining's rmse: 0.807563\tvalid_1's rmse: 0.971551\n",
      "Early stopping, best iteration is:\n",
      "[790]\ttraining's rmse: 0.808937\tvalid_1's rmse: 0.971522\n",
      "Partial score of fold 2 is: 0.612428003290411\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02062\tvalid_1's rmse: 1.04154\n",
      "[200]\ttraining's rmse: 0.946729\tvalid_1's rmse: 0.992569\n",
      "[300]\ttraining's rmse: 0.906362\tvalid_1's rmse: 0.976598\n",
      "[400]\ttraining's rmse: 0.877447\tvalid_1's rmse: 0.970757\n",
      "[500]\ttraining's rmse: 0.85474\tvalid_1's rmse: 0.967506\n",
      "[600]\ttraining's rmse: 0.836857\tvalid_1's rmse: 0.965905\n",
      "[700]\ttraining's rmse: 0.821423\tvalid_1's rmse: 0.965135\n",
      "[800]\ttraining's rmse: 0.806678\tvalid_1's rmse: 0.964571\n",
      "[900]\ttraining's rmse: 0.792786\tvalid_1's rmse: 0.96428\n",
      "[1000]\ttraining's rmse: 0.780111\tvalid_1's rmse: 0.963819\n",
      "Early stopping, best iteration is:\n",
      "[999]\ttraining's rmse: 0.780234\tvalid_1's rmse: 0.963806\n",
      "Partial score of fold 3 is: 0.6177960364858348\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01984\tvalid_1's rmse: 1.04363\n",
      "[200]\ttraining's rmse: 0.946047\tvalid_1's rmse: 0.995881\n",
      "[300]\ttraining's rmse: 0.906388\tvalid_1's rmse: 0.980627\n",
      "[400]\ttraining's rmse: 0.877887\tvalid_1's rmse: 0.974391\n",
      "[500]\ttraining's rmse: 0.855159\tvalid_1's rmse: 0.971973\n",
      "[600]\ttraining's rmse: 0.836938\tvalid_1's rmse: 0.97163\n",
      "[700]\ttraining's rmse: 0.820377\tvalid_1's rmse: 0.971171\n",
      "[800]\ttraining's rmse: 0.806185\tvalid_1's rmse: 0.971018\n",
      "Early stopping, best iteration is:\n",
      "[781]\ttraining's rmse: 0.808543\tvalid_1's rmse: 0.970975\n",
      "Partial score of fold 4 is: 0.602756526643485\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.6038  \u001b[0m | \u001b[0m 0.8737  \u001b[0m | \u001b[0m 0.9279  \u001b[0m | \u001b[0m 0.5101  \u001b[0m | \u001b[0m 0.4054  \u001b[0m | \u001b[0m 14.87   \u001b[0m | \u001b[0m 149.5   \u001b[0m | \u001b[0m 128.7   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01856\tvalid_1's rmse: 1.05685\n",
      "[200]\ttraining's rmse: 0.937941\tvalid_1's rmse: 1.01108\n",
      "[300]\ttraining's rmse: 0.893812\tvalid_1's rmse: 0.996071\n",
      "[400]\ttraining's rmse: 0.863854\tvalid_1's rmse: 0.99091\n",
      "[500]\ttraining's rmse: 0.840495\tvalid_1's rmse: 0.9891\n",
      "[600]\ttraining's rmse: 0.821803\tvalid_1's rmse: 0.988927\n",
      "Early stopping, best iteration is:\n",
      "[523]\ttraining's rmse: 0.835869\tvalid_1's rmse: 0.988575\n",
      "Partial score of fold 0 is: 0.5866614439523775\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.01968\tvalid_1's rmse: 1.05348\n",
      "[200]\ttraining's rmse: 0.939337\tvalid_1's rmse: 1.00468\n",
      "[300]\ttraining's rmse: 0.895309\tvalid_1's rmse: 0.989827\n",
      "[400]\ttraining's rmse: 0.865086\tvalid_1's rmse: 0.98461\n",
      "[500]\ttraining's rmse: 0.84127\tvalid_1's rmse: 0.982551\n",
      "[600]\ttraining's rmse: 0.822229\tvalid_1's rmse: 0.98197\n",
      "Early stopping, best iteration is:\n",
      "[574]\ttraining's rmse: 0.826972\tvalid_1's rmse: 0.981774\n",
      "Partial score of fold 1 is: 0.5982921825424621\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02097\tvalid_1's rmse: 1.05302\n",
      "[200]\ttraining's rmse: 0.94094\tvalid_1's rmse: 1.00061\n",
      "[300]\ttraining's rmse: 0.897182\tvalid_1's rmse: 0.982271\n",
      "[400]\ttraining's rmse: 0.866886\tvalid_1's rmse: 0.975701\n",
      "[500]\ttraining's rmse: 0.843399\tvalid_1's rmse: 0.973677\n",
      "[600]\ttraining's rmse: 0.823427\tvalid_1's rmse: 0.972559\n",
      "[700]\ttraining's rmse: 0.807353\tvalid_1's rmse: 0.972175\n",
      "Early stopping, best iteration is:\n",
      "[651]\ttraining's rmse: 0.814628\tvalid_1's rmse: 0.971937\n",
      "Partial score of fold 2 is: 0.610817593331784\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02216\tvalid_1's rmse: 1.04693\n",
      "[200]\ttraining's rmse: 0.941635\tvalid_1's rmse: 0.993315\n",
      "[300]\ttraining's rmse: 0.898007\tvalid_1's rmse: 0.975674\n",
      "[400]\ttraining's rmse: 0.867957\tvalid_1's rmse: 0.968555\n",
      "[500]\ttraining's rmse: 0.844626\tvalid_1's rmse: 0.965261\n",
      "[600]\ttraining's rmse: 0.826001\tvalid_1's rmse: 0.963975\n",
      "[700]\ttraining's rmse: 0.809056\tvalid_1's rmse: 0.963673\n",
      "[800]\ttraining's rmse: 0.794421\tvalid_1's rmse: 0.963857\n",
      "Early stopping, best iteration is:\n",
      "[712]\ttraining's rmse: 0.807173\tvalid_1's rmse: 0.963602\n",
      "Partial score of fold 3 is: 0.6143962821287331\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.0204\tvalid_1's rmse: 1.04904\n",
      "[200]\ttraining's rmse: 0.939872\tvalid_1's rmse: 0.997787\n",
      "[300]\ttraining's rmse: 0.896425\tvalid_1's rmse: 0.980815\n",
      "[400]\ttraining's rmse: 0.865722\tvalid_1's rmse: 0.974468\n",
      "[500]\ttraining's rmse: 0.842542\tvalid_1's rmse: 0.971619\n",
      "[600]\ttraining's rmse: 0.822823\tvalid_1's rmse: 0.970777\n",
      "[700]\ttraining's rmse: 0.806143\tvalid_1's rmse: 0.970604\n",
      "Early stopping, best iteration is:\n",
      "[631]\ttraining's rmse: 0.817487\tvalid_1's rmse: 0.970549\n",
      "Partial score of fold 4 is: 0.600966738522933\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.6027  \u001b[0m | \u001b[0m 0.9215  \u001b[0m | \u001b[0m 0.7687  \u001b[0m | \u001b[0m 0.1053  \u001b[0m | \u001b[0m 0.009263\u001b[0m | \u001b[0m 14.97   \u001b[0m | \u001b[0m 139.3   \u001b[0m | \u001b[0m 127.3   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02728\tvalid_1's rmse: 1.05622\n",
      "[200]\ttraining's rmse: 0.957252\tvalid_1's rmse: 1.0118\n",
      "[300]\ttraining's rmse: 0.918879\tvalid_1's rmse: 0.996596\n",
      "[400]\ttraining's rmse: 0.88936\tvalid_1's rmse: 0.989966\n",
      "[500]\ttraining's rmse: 0.865072\tvalid_1's rmse: 0.987274\n",
      "[600]\ttraining's rmse: 0.845265\tvalid_1's rmse: 0.987084\n",
      "Early stopping, best iteration is:\n",
      "[547]\ttraining's rmse: 0.855575\tvalid_1's rmse: 0.986812\n",
      "Partial score of fold 0 is: 0.5900611983094791\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02795\tvalid_1's rmse: 1.05228\n",
      "[200]\ttraining's rmse: 0.957943\tvalid_1's rmse: 1.0061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttraining's rmse: 0.919584\tvalid_1's rmse: 0.99154\n",
      "[400]\ttraining's rmse: 0.890468\tvalid_1's rmse: 0.985711\n",
      "[500]\ttraining's rmse: 0.866606\tvalid_1's rmse: 0.982499\n",
      "[600]\ttraining's rmse: 0.847139\tvalid_1's rmse: 0.98114\n",
      "[700]\ttraining's rmse: 0.829294\tvalid_1's rmse: 0.980329\n",
      "[800]\ttraining's rmse: 0.813175\tvalid_1's rmse: 0.98033\n",
      "Early stopping, best iteration is:\n",
      "[747]\ttraining's rmse: 0.821445\tvalid_1's rmse: 0.980178\n",
      "Partial score of fold 1 is: 0.5988289858620044\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02887\tvalid_1's rmse: 1.05161\n",
      "[200]\ttraining's rmse: 0.959535\tvalid_1's rmse: 1.00138\n",
      "[300]\ttraining's rmse: 0.921692\tvalid_1's rmse: 0.984267\n",
      "[400]\ttraining's rmse: 0.892801\tvalid_1's rmse: 0.976846\n",
      "[500]\ttraining's rmse: 0.868905\tvalid_1's rmse: 0.973314\n",
      "[600]\ttraining's rmse: 0.848808\tvalid_1's rmse: 0.971817\n",
      "[700]\ttraining's rmse: 0.831036\tvalid_1's rmse: 0.971131\n",
      "[800]\ttraining's rmse: 0.814471\tvalid_1's rmse: 0.970898\n",
      "[900]\ttraining's rmse: 0.799696\tvalid_1's rmse: 0.970988\n",
      "Early stopping, best iteration is:\n",
      "[814]\ttraining's rmse: 0.81214\tvalid_1's rmse: 0.97081\n",
      "Partial score of fold 2 is: 0.6083125111739196\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03008\tvalid_1's rmse: 1.0463\n",
      "[200]\ttraining's rmse: 0.960228\tvalid_1's rmse: 0.995961\n",
      "[300]\ttraining's rmse: 0.921951\tvalid_1's rmse: 0.979111\n",
      "[400]\ttraining's rmse: 0.892618\tvalid_1's rmse: 0.972152\n",
      "[500]\ttraining's rmse: 0.869\tvalid_1's rmse: 0.968528\n",
      "[600]\ttraining's rmse: 0.849539\tvalid_1's rmse: 0.966846\n",
      "[700]\ttraining's rmse: 0.83175\tvalid_1's rmse: 0.965903\n",
      "[800]\ttraining's rmse: 0.816151\tvalid_1's rmse: 0.965293\n",
      "[900]\ttraining's rmse: 0.801311\tvalid_1's rmse: 0.964836\n",
      "[1000]\ttraining's rmse: 0.787373\tvalid_1's rmse: 0.964441\n",
      "Early stopping, best iteration is:\n",
      "[992]\ttraining's rmse: 0.788429\tvalid_1's rmse: 0.964404\n",
      "Partial score of fold 3 is: 0.6113543966513264\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02923\tvalid_1's rmse: 1.04761\n",
      "[200]\ttraining's rmse: 0.959631\tvalid_1's rmse: 0.999441\n",
      "[300]\ttraining's rmse: 0.921121\tvalid_1's rmse: 0.98326\n",
      "[400]\ttraining's rmse: 0.89201\tvalid_1's rmse: 0.976672\n",
      "[500]\ttraining's rmse: 0.868254\tvalid_1's rmse: 0.973506\n",
      "[600]\ttraining's rmse: 0.848069\tvalid_1's rmse: 0.972375\n",
      "[700]\ttraining's rmse: 0.830034\tvalid_1's rmse: 0.972021\n",
      "[800]\ttraining's rmse: 0.813994\tvalid_1's rmse: 0.971776\n",
      "[900]\ttraining's rmse: 0.7993\tvalid_1's rmse: 0.971573\n",
      "[1000]\ttraining's rmse: 0.785586\tvalid_1's rmse: 0.97186\n",
      "Early stopping, best iteration is:\n",
      "[918]\ttraining's rmse: 0.796526\tvalid_1's rmse: 0.971438\n",
      "Partial score of fold 4 is: 0.6038303995158163\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.603   \u001b[0m | \u001b[0m 0.9228  \u001b[0m | \u001b[0m 0.9213  \u001b[0m | \u001b[0m 5.825   \u001b[0m | \u001b[0m 0.9796  \u001b[0m | \u001b[0m 14.6    \u001b[0m | \u001b[0m 149.3   \u001b[0m | \u001b[0m 129.0   \u001b[0m |\n",
      "=============================================================================================================\n",
      "\n",
      " Time taken: 0 hours 26 minutes and 37.01 seconds.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def LGB_CV(\n",
    "          max_depth,\n",
    "          num_leaves,\n",
    "          min_data_in_leaf,\n",
    "          feature_fraction,\n",
    "          bagging_fraction,\n",
    "          lambda_l1,\n",
    "          lambda_l2,\n",
    "          target='accuracy_group',\n",
    "          categoricals=['session_title']\n",
    "         ):\n",
    "    train_df = reduce_train\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    cv = folds.split(reduce_train, reduce_train[target])\n",
    "    \n",
    "    param = {\n",
    "        'num_leaves': int(num_leaves),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf), \n",
    "        'objective':'regression',\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': 0.01,\n",
    "        \"boosting\": \"gbdt\",\n",
    "        \"feature_fraction\": feature_fraction,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"bagging_fraction\": bagging_fraction ,\n",
    "        \"bagging_seed\": 11,\n",
    "        \"metric\": 'rmse',\n",
    "        \"lambda_l1\": lambda_l1,\n",
    "        \"lambda_l2\": lambda_l2,\n",
    "        \"verbosity\": -1\n",
    "    }\n",
    "    \n",
    "    \n",
    "    oof_pred = np.zeros(train_df.shape[0])\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv):\n",
    "        \n",
    "        x_train, x_val = train_df[features].iloc[train_idx], train_df[features].iloc[val_idx]   \n",
    "        y_train, y_val = train_df[target][train_idx], train_df[target][val_idx]\n",
    "\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature=categoricals)\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature=categoricals)\n",
    "        \n",
    "        clf = lgb.train(param, train_set, 10000, valid_sets = [train_set, val_set], verbose_eval=100, early_stopping_rounds=100)\n",
    "        \n",
    "        oof_pred[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "        \n",
    "        print('Partial score of fold {} is: {}'.format(fold, eval_qwk_lgb_regr(y_val, oof_pred[val_idx])[1]))\n",
    "        \n",
    "        del clf, train_idx, val_idx\n",
    "        gc.collect()\n",
    "    _, loss_score, _ = eval_qwk_lgb_regr(train_df[target], oof_pred)    \n",
    "#     return -mean_squared_error(oof, target)**0.5\n",
    "    return loss_score\n",
    "\n",
    "LGB_BO = BayesianOptimization(LGB_CV, {\n",
    "    'max_depth': (4, 15),\n",
    "    'num_leaves': (5, 130),\n",
    "    'min_data_in_leaf': (10, 150),\n",
    "    'feature_fraction': (0.7, 1.0),\n",
    "    'bagging_fraction': (0.7, 1.0),\n",
    "    'lambda_l1': (0, 6),\n",
    "    'lambda_l2': (0, 6)\n",
    "    })\n",
    "\n",
    "print('-'*126)\n",
    "\n",
    "start_time = timer(None)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=2, n_iter=20, acq='ei', xi=0.0)\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Final Results\n",
      "Maximum  value: 0.604730\n",
      "Best  parameters:  {'bagging_fraction': 0.9251509026878358, 'feature_fraction': 0.814913080520384, 'lambda_l1': 1.3338841930686365, 'lambda_l2': 3.4699139216739683, 'max_depth': 12.603557133281978, 'min_data_in_leaf': 149.41288968501712, 'num_leaves': 129.95148487549415}\n"
     ]
    }
   ],
   "source": [
    "print('-'*130)\n",
    "print('Final Results')\n",
    "print('Maximum  value: %f' % LGB_BO.max['target'])\n",
    "print('Best  parameters:', LGB_BO.max['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame([x['params'] for x in LGB_BO.res])\n",
    "history_df2 = pd.DataFrame([x['target'] for x in LGB_BO.res])\n",
    "history_df = pd.concat((history_df, history_df2), axis=1)\n",
    "history_df.rename(columns = { 0 : 'gini'}, inplace=True)\n",
    "history_df.to_csv('ParametersOptimization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bagging_fraction</th>\n",
       "      <th>feature_fraction</th>\n",
       "      <th>lambda_l1</th>\n",
       "      <th>lambda_l2</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_data_in_leaf</th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>gini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.901259</td>\n",
       "      <td>0.921653</td>\n",
       "      <td>1.260654</td>\n",
       "      <td>3.318987</td>\n",
       "      <td>6.049654</td>\n",
       "      <td>55.797823</td>\n",
       "      <td>82.421428</td>\n",
       "      <td>0.600793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.744794</td>\n",
       "      <td>0.733007</td>\n",
       "      <td>3.165668</td>\n",
       "      <td>5.289036</td>\n",
       "      <td>11.870796</td>\n",
       "      <td>122.105567</td>\n",
       "      <td>74.065406</td>\n",
       "      <td>0.603836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.883539</td>\n",
       "      <td>0.720388</td>\n",
       "      <td>4.346971</td>\n",
       "      <td>1.604639</td>\n",
       "      <td>7.460692</td>\n",
       "      <td>149.923846</td>\n",
       "      <td>5.525863</td>\n",
       "      <td>0.595103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.925151</td>\n",
       "      <td>0.814913</td>\n",
       "      <td>1.333884</td>\n",
       "      <td>3.469914</td>\n",
       "      <td>12.603557</td>\n",
       "      <td>149.412890</td>\n",
       "      <td>129.951485</td>\n",
       "      <td>0.604730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.938173</td>\n",
       "      <td>0.785393</td>\n",
       "      <td>3.305657</td>\n",
       "      <td>5.463538</td>\n",
       "      <td>14.816669</td>\n",
       "      <td>146.838060</td>\n",
       "      <td>129.870075</td>\n",
       "      <td>0.602547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.901620</td>\n",
       "      <td>0.925947</td>\n",
       "      <td>0.716856</td>\n",
       "      <td>5.422264</td>\n",
       "      <td>14.067028</td>\n",
       "      <td>12.582834</td>\n",
       "      <td>129.565001</td>\n",
       "      <td>0.600686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.785599</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.051691</td>\n",
       "      <td>1.083516</td>\n",
       "      <td>14.883723</td>\n",
       "      <td>125.312007</td>\n",
       "      <td>111.477987</td>\n",
       "      <td>0.604015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.849854</td>\n",
       "      <td>0.742762</td>\n",
       "      <td>0.190979</td>\n",
       "      <td>5.790203</td>\n",
       "      <td>4.356047</td>\n",
       "      <td>148.237429</td>\n",
       "      <td>129.656504</td>\n",
       "      <td>0.599147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.971809</td>\n",
       "      <td>0.876962</td>\n",
       "      <td>0.748148</td>\n",
       "      <td>4.893738</td>\n",
       "      <td>14.863714</td>\n",
       "      <td>10.994428</td>\n",
       "      <td>5.383123</td>\n",
       "      <td>0.596033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.731539</td>\n",
       "      <td>0.961839</td>\n",
       "      <td>5.820059</td>\n",
       "      <td>0.296213</td>\n",
       "      <td>14.888716</td>\n",
       "      <td>103.712152</td>\n",
       "      <td>105.116428</td>\n",
       "      <td>0.603692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.709081</td>\n",
       "      <td>0.789187</td>\n",
       "      <td>0.292919</td>\n",
       "      <td>5.877816</td>\n",
       "      <td>14.917433</td>\n",
       "      <td>104.646171</td>\n",
       "      <td>99.170815</td>\n",
       "      <td>0.602941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.877093</td>\n",
       "      <td>0.963433</td>\n",
       "      <td>5.983822</td>\n",
       "      <td>0.381920</td>\n",
       "      <td>14.547255</td>\n",
       "      <td>101.933575</td>\n",
       "      <td>128.238838</td>\n",
       "      <td>0.602082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.868824</td>\n",
       "      <td>0.761728</td>\n",
       "      <td>0.219036</td>\n",
       "      <td>0.249902</td>\n",
       "      <td>14.905497</td>\n",
       "      <td>149.195216</td>\n",
       "      <td>106.972392</td>\n",
       "      <td>0.603370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.711946</td>\n",
       "      <td>0.170118</td>\n",
       "      <td>5.928311</td>\n",
       "      <td>14.933898</td>\n",
       "      <td>111.253587</td>\n",
       "      <td>100.556020</td>\n",
       "      <td>0.601724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.829568</td>\n",
       "      <td>0.988799</td>\n",
       "      <td>0.094389</td>\n",
       "      <td>0.382148</td>\n",
       "      <td>14.898063</td>\n",
       "      <td>92.674217</td>\n",
       "      <td>112.489130</td>\n",
       "      <td>0.600471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.794487</td>\n",
       "      <td>5.560497</td>\n",
       "      <td>4.311592</td>\n",
       "      <td>14.882310</td>\n",
       "      <td>148.344112</td>\n",
       "      <td>95.913356</td>\n",
       "      <td>0.603084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.905202</td>\n",
       "      <td>0.975294</td>\n",
       "      <td>5.377291</td>\n",
       "      <td>5.466528</td>\n",
       "      <td>14.975712</td>\n",
       "      <td>146.793585</td>\n",
       "      <td>115.874917</td>\n",
       "      <td>0.602834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.919292</td>\n",
       "      <td>0.781019</td>\n",
       "      <td>5.964534</td>\n",
       "      <td>0.173634</td>\n",
       "      <td>14.955256</td>\n",
       "      <td>148.992271</td>\n",
       "      <td>114.547603</td>\n",
       "      <td>0.603370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.877905</td>\n",
       "      <td>0.743373</td>\n",
       "      <td>5.956321</td>\n",
       "      <td>5.714908</td>\n",
       "      <td>14.839080</td>\n",
       "      <td>132.296833</td>\n",
       "      <td>117.606129</td>\n",
       "      <td>0.602690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.873741</td>\n",
       "      <td>0.927932</td>\n",
       "      <td>0.510095</td>\n",
       "      <td>0.405402</td>\n",
       "      <td>14.873450</td>\n",
       "      <td>149.468411</td>\n",
       "      <td>128.713515</td>\n",
       "      <td>0.603836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.921511</td>\n",
       "      <td>0.768663</td>\n",
       "      <td>0.105299</td>\n",
       "      <td>0.009263</td>\n",
       "      <td>14.970906</td>\n",
       "      <td>139.267309</td>\n",
       "      <td>127.284686</td>\n",
       "      <td>0.602655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.922810</td>\n",
       "      <td>0.921254</td>\n",
       "      <td>5.824923</td>\n",
       "      <td>0.979614</td>\n",
       "      <td>14.595925</td>\n",
       "      <td>149.309188</td>\n",
       "      <td>128.995386</td>\n",
       "      <td>0.602977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bagging_fraction  feature_fraction  lambda_l1  lambda_l2  max_depth  \\\n",
       "0           0.901259          0.921653   1.260654   3.318987   6.049654   \n",
       "1           0.744794          0.733007   3.165668   5.289036  11.870796   \n",
       "2           0.883539          0.720388   4.346971   1.604639   7.460692   \n",
       "3           0.925151          0.814913   1.333884   3.469914  12.603557   \n",
       "4           0.938173          0.785393   3.305657   5.463538  14.816669   \n",
       "5           0.901620          0.925947   0.716856   5.422264  14.067028   \n",
       "6           0.785599          0.790244   0.051691   1.083516  14.883723   \n",
       "7           0.849854          0.742762   0.190979   5.790203   4.356047   \n",
       "8           0.971809          0.876962   0.748148   4.893738  14.863714   \n",
       "9           0.731539          0.961839   5.820059   0.296213  14.888716   \n",
       "10          0.709081          0.789187   0.292919   5.877816  14.917433   \n",
       "11          0.877093          0.963433   5.983822   0.381920  14.547255   \n",
       "12          0.868824          0.761728   0.219036   0.249902  14.905497   \n",
       "13          0.989418          0.711946   0.170118   5.928311  14.933898   \n",
       "14          0.829568          0.988799   0.094389   0.382148  14.898063   \n",
       "15          0.931034          0.794487   5.560497   4.311592  14.882310   \n",
       "16          0.905202          0.975294   5.377291   5.466528  14.975712   \n",
       "17          0.919292          0.781019   5.964534   0.173634  14.955256   \n",
       "18          0.877905          0.743373   5.956321   5.714908  14.839080   \n",
       "19          0.873741          0.927932   0.510095   0.405402  14.873450   \n",
       "20          0.921511          0.768663   0.105299   0.009263  14.970906   \n",
       "21          0.922810          0.921254   5.824923   0.979614  14.595925   \n",
       "\n",
       "    min_data_in_leaf  num_leaves      gini  \n",
       "0          55.797823   82.421428  0.600793  \n",
       "1         122.105567   74.065406  0.603836  \n",
       "2         149.923846    5.525863  0.595103  \n",
       "3         149.412890  129.951485  0.604730  \n",
       "4         146.838060  129.870075  0.602547  \n",
       "5          12.582834  129.565001  0.600686  \n",
       "6         125.312007  111.477987  0.604015  \n",
       "7         148.237429  129.656504  0.599147  \n",
       "8          10.994428    5.383123  0.596033  \n",
       "9         103.712152  105.116428  0.603692  \n",
       "10        104.646171   99.170815  0.602941  \n",
       "11        101.933575  128.238838  0.602082  \n",
       "12        149.195216  106.972392  0.603370  \n",
       "13        111.253587  100.556020  0.601724  \n",
       "14         92.674217  112.489130  0.600471  \n",
       "15        148.344112   95.913356  0.603084  \n",
       "16        146.793585  115.874917  0.602834  \n",
       "17        148.992271  114.547603  0.603370  \n",
       "18        132.296833  117.606129  0.602690  \n",
       "19        149.468411  128.713515  0.603836  \n",
       "20        139.267309  127.284686  0.602655  \n",
       "21        149.309188  128.995386  0.602977  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True, param=None):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.categoricals = categoricals\n",
    "        self.target = 'accuracy_group'\n",
    "        self.cv = self.get_cv()\n",
    "        self.verbose = verbose\n",
    "        self.params = self.get_params(param)\n",
    "        self.y_pred, self.oof_pred, self.score, self.model = self.fit()\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_cv(self):\n",
    "        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        return cv.split(self.train_df, self.train_df[self.target])\n",
    "    \n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "        \n",
    "    def fit(self):\n",
    "        oof_pred = np.zeros((len(reduce_train), ))\n",
    "        y_pred = np.zeros((len(reduce_test), ))\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv):\n",
    "            # 相当于降维\n",
    "            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]   \n",
    "            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n",
    "            # 针对不同模型处理数据\n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            # 针对不同模型训练数据\n",
    "            model = self.train_model(train_set, val_set) \n",
    "            # 得到验证数据的预测值\n",
    "            conv_x_val = self.convert_x(x_val)\n",
    "            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n",
    "            # 得到测试数据的预测值\n",
    "            x_test = self.convert_x(self.test_df[self.features])\n",
    "            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "            print('Partial score of fold {} is: {}'.format(fold, eval_qwk_lgb_regr(y_val, oof_pred[val_idx])[1]))\n",
    "        _, loss_score, _ = eval_qwk_lgb_regr(self.train_df[self.target], oof_pred)\n",
    "        if self.verbose:\n",
    "            print('Our oof cohen kappa score is: ', loss_score)\n",
    "        return y_pred, oof_pred, loss_score, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lgb_Model(Base_Model):\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self, params=None):\n",
    "        if params is None :\n",
    "            params = {\n",
    "                'n_estimators':2000,\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'subsample': 0.75,\n",
    "                'subsample_freq': 1,\n",
    "                'learning_rate': 0.01,\n",
    "                'feature_fraction': 0.9,\n",
    "                'max_depth': 15,\n",
    "                'lambda_l1': 1,  \n",
    "                'lambda_l2': 1,\n",
    "                'early_stopping_rounds': 100}\n",
    "        else:\n",
    "            params = {\n",
    "                'n_estimators':2000,\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'subsample': 0.75,\n",
    "                'subsample_freq': 1,\n",
    "                'learning_rate': 0.01,\n",
    "                'min_data_in_leaf': params.min_data_in_leaf, \n",
    "                'bagging_fraction': params.bagging_fraction,\n",
    "                'feature_fraction': params.bagging_fraction,\n",
    "                'max_depth': params.max_depth,\n",
    "                'lambda_l1': params.lambda_l1,  \n",
    "                'lambda_l2': params.lambda_l2,\n",
    "                'early_stopping_rounds': 100}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xgb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return xgb.train(self.params, \n",
    "                         train_set, \n",
    "                         num_boost_round=5000, \n",
    "                         evals=[(train_set, 'train'), \n",
    "                                (val_set, 'val')], \n",
    "                         verbose_eval=verbosity,\n",
    "                         early_stopping_rounds=100)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = xgb.DMatrix(x_train, y_train)\n",
    "        val_set = xgb.DMatrix(x_val, y_val)\n",
    "        return train_set, val_set\n",
    "    \n",
    "    def convert_x(self, x):\n",
    "        return xgb.DMatrix(x)\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'colsample_bytree': 0.8,                 \n",
    "                  'learning_rate': 0.01,\n",
    "                  'max_depth': 10,\n",
    "                  'subsample': 1,\n",
    "                  'objective':'reg:squarederror',\n",
    "                  #'eval_metric':'rmse',\n",
    "                  'min_child_weight':3,\n",
    "                  'gamma':0.25,\n",
    "                  'n_estimators':2000}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catb_Model(Base_Model):\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        clf = CatBoostRegressor(**self.params)\n",
    "        clf.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                eval_set=(val_set['X'], val_set['y']),\n",
    "                verbose=verbosity, \n",
    "                cat_features=self.categoricals)\n",
    "        return clf\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'loss_function': 'RMSE',\n",
    "                   'task_type': \"CPU\",\n",
    "                   'iterations': 2000,\n",
    "                   'od_type': \"Iter\",\n",
    "                   'depth': 10,\n",
    "                   'colsample_bylevel': 0.5, \n",
    "                   'early_stopping_rounds': 300,\n",
    "                   'l2_leaf_reg': 18,\n",
    "                   'random_seed': 42,\n",
    "                   'use_best_model': True}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "class Nn_Model(Base_Model):\n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(train_df[[cat]])\n",
    "                test_cats = enc.transform(test_df[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                train_df = pd.concat([train_df, train_cats], axis=1)\n",
    "                test_df = pd.concat([test_df, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        train_df[features] = scalar.fit_transform(train_df[features])\n",
    "        test_df[features] = scalar.transform(test_df[features])\n",
    "        print(train_df[features].shape)\n",
    "        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(train_set['X'].shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(100, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "class Cnn_Model(Base_Model):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(train_df[[cat]])\n",
    "                test_cats = enc.transform(test_df[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                train_df = pd.concat([train_df, train_cats], axis=1)\n",
    "                test_df = pd.concat([test_df, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        train_df[features] = scalar.fit_transform(train_df[features])\n",
    "        test_df[features] = scalar.transform(test_df[features])\n",
    "        self.create_feat_2d(features)\n",
    "        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n",
    "        \n",
    "    def create_feat_2d(self, features, n_feats_repeat=50):\n",
    "        self.n_feats = len(features)\n",
    "        self.n_feats_repeat = n_feats_repeat\n",
    "        self.mask = np.zeros((self.n_feats_repeat, self.n_feats), dtype=np.int32)\n",
    "        for i in range(self.n_feats_repeat):\n",
    "            l = list(range(self.n_feats))\n",
    "            for j in range(self.n_feats):\n",
    "                c = l.pop(choice(range(len(l))))\n",
    "                self.mask[i, j] = c\n",
    "        self.mask = tf.convert_to_tensor(self.mask)\n",
    "        print(self.mask.shape)\n",
    "       \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "\n",
    "        inp = tf.keras.layers.Input(shape=(self.n_feats))\n",
    "        x = tf.keras.layers.Lambda(lambda x: tf.gather(x, self.mask, axis=1))(inp)\n",
    "        x = tf.keras.layers.Reshape((self.n_feats_repeat, self.n_feats, 1))(x)\n",
    "        x = tf.keras.layers.Conv2D(18, (50, 50), strides=50, activation='relu')(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        #x = tf.keras.layers.Dense(200, activation='relu')(x)\n",
    "        #x = tf.keras.layers.LayerNormalization()(x)\n",
    "        #x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        out = tf.keras.layers.Dense(1)(x)\n",
    "        \n",
    "        model = tf.keras.Model(inp, out)\n",
    "    \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028296339811674016\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.028296339811674016"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stract_hists(feature, train=reduce_train, test=reduce_test, adjust=False, plot=False):\n",
    "    n_bins = 10\n",
    "    train_data = train[feature]\n",
    "    test_data = test[feature]\n",
    "    if adjust:\n",
    "        test_data *= train_data.mean() / test_data.mean()\n",
    "    perc_90 = np.percentile(train_data, 95)\n",
    "    train_data = np.clip(train_data, 0, perc_90)\n",
    "    test_data = np.clip(test_data, 0, perc_90)\n",
    "    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n",
    "    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n",
    "    msre = mean_squared_error(train_hist, test_hist)\n",
    "    if plot:\n",
    "        print(msre)\n",
    "        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n",
    "        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n",
    "        plt.show()\n",
    "    return msre\n",
    "stract_hists('Magma Peak - Level 1_2000', adjust=True, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17690, 952), (1000, 952), 943)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_train.shape, reduce_test.shape, len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2473118\ttest: 1.2478775\tbest: 1.2478775 (0)\ttotal: 2.27s\tremaining: 1h 15m 28s\n",
      "100:\tlearn: 0.9907757\ttest: 1.0304158\tbest: 1.0304158 (100)\ttotal: 31.6s\tremaining: 9m 53s\n",
      "200:\tlearn: 0.9414181\ttest: 1.0097588\tbest: 1.0097588 (200)\ttotal: 52.3s\tremaining: 7m 48s\n",
      "300:\tlearn: 0.9086435\ttest: 1.0025655\tbest: 1.0025655 (300)\ttotal: 1m 16s\tremaining: 7m 14s\n",
      "400:\tlearn: 0.8721896\ttest: 0.9973666\tbest: 0.9973666 (400)\ttotal: 1m 43s\tremaining: 6m 51s\n",
      "500:\tlearn: 0.8458273\ttest: 0.9951154\tbest: 0.9951090 (498)\ttotal: 2m 8s\tremaining: 6m 23s\n",
      "600:\tlearn: 0.8235988\ttest: 0.9935908\tbest: 0.9935908 (600)\ttotal: 2m 29s\tremaining: 5m 48s\n",
      "700:\tlearn: 0.8065238\ttest: 0.9933210\tbest: 0.9932864 (695)\ttotal: 2m 46s\tremaining: 5m 9s\n",
      "800:\tlearn: 0.7902526\ttest: 0.9927438\tbest: 0.9927438 (800)\ttotal: 3m 8s\tremaining: 4m 41s\n",
      "900:\tlearn: 0.7758287\ttest: 0.9925924\tbest: 0.9925376 (898)\ttotal: 3m 33s\tremaining: 4m 20s\n",
      "1000:\tlearn: 0.7606254\ttest: 0.9928594\tbest: 0.9925298 (915)\ttotal: 3m 58s\tremaining: 3m 57s\n",
      "1100:\tlearn: 0.7452393\ttest: 0.9924005\tbest: 0.9924005 (1100)\ttotal: 4m 20s\tremaining: 3m 32s\n",
      "1200:\tlearn: 0.7312287\ttest: 0.9923223\tbest: 0.9923108 (1188)\ttotal: 4m 44s\tremaining: 3m 9s\n",
      "1300:\tlearn: 0.7154446\ttest: 0.9928771\tbest: 0.9923052 (1205)\ttotal: 5m 9s\tremaining: 2m 46s\n",
      "1400:\tlearn: 0.7031160\ttest: 0.9930273\tbest: 0.9923052 (1205)\ttotal: 5m 26s\tremaining: 2m 19s\n",
      "1500:\tlearn: 0.6896663\ttest: 0.9939051\tbest: 0.9923052 (1205)\ttotal: 5m 43s\tremaining: 1m 54s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9923051743\n",
      "bestIteration = 1205\n",
      "\n",
      "Shrink model to first 1206 iterations.\n",
      "Partial score of fold 0 is: 0.5818302140764962\n",
      "0:\tlearn: 1.2477565\ttest: 1.2480153\tbest: 1.2480153 (0)\ttotal: 809ms\tremaining: 26m 56s\n",
      "100:\tlearn: 0.9911805\ttest: 1.0185338\tbest: 1.0185338 (100)\ttotal: 25.9s\tremaining: 8m 7s\n",
      "200:\tlearn: 0.9448901\ttest: 0.9992128\tbest: 0.9992128 (200)\ttotal: 50.7s\tremaining: 7m 33s\n",
      "300:\tlearn: 0.9138206\ttest: 0.9920026\tbest: 0.9920026 (300)\ttotal: 1m 14s\tremaining: 7m 2s\n",
      "400:\tlearn: 0.8837112\ttest: 0.9865855\tbest: 0.9865855 (400)\ttotal: 1m 40s\tremaining: 6m 42s\n",
      "500:\tlearn: 0.8592699\ttest: 0.9842453\tbest: 0.9842453 (500)\ttotal: 2m 5s\tremaining: 6m 16s\n",
      "600:\tlearn: 0.8382813\ttest: 0.9825621\tbest: 0.9825621 (600)\ttotal: 2m 31s\tremaining: 5m 53s\n",
      "700:\tlearn: 0.8186024\ttest: 0.9815920\tbest: 0.9815862 (699)\ttotal: 2m 54s\tremaining: 5m 22s\n",
      "800:\tlearn: 0.8012046\ttest: 0.9806319\tbest: 0.9805983 (794)\ttotal: 3m 18s\tremaining: 4m 56s\n",
      "900:\tlearn: 0.7819198\ttest: 0.9805153\tbest: 0.9803551 (886)\ttotal: 3m 43s\tremaining: 4m 33s\n",
      "1000:\tlearn: 0.7664031\ttest: 0.9801336\tbest: 0.9801279 (985)\ttotal: 4m 10s\tremaining: 4m 9s\n",
      "1100:\tlearn: 0.7511189\ttest: 0.9803429\tbest: 0.9800786 (1036)\ttotal: 4m 32s\tremaining: 3m 42s\n",
      "1200:\tlearn: 0.7362698\ttest: 0.9803411\tbest: 0.9800786 (1036)\ttotal: 4m 49s\tremaining: 3m 12s\n",
      "1300:\tlearn: 0.7223841\ttest: 0.9808186\tbest: 0.9800786 (1036)\ttotal: 5m 8s\tremaining: 2m 45s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9800785946\n",
      "bestIteration = 1036\n",
      "\n",
      "Shrink model to first 1037 iterations.\n",
      "Partial score of fold 1 is: 0.6042429727505687\n",
      "0:\tlearn: 1.2471789\ttest: 1.2480231\tbest: 1.2480231 (0)\ttotal: 482ms\tremaining: 16m 2s\n",
      "100:\tlearn: 0.9912419\ttest: 1.0317342\tbest: 1.0317342 (100)\ttotal: 27.6s\tremaining: 8m 38s\n",
      "200:\tlearn: 0.9438769\ttest: 1.0080787\tbest: 1.0080787 (200)\ttotal: 54s\tremaining: 8m 3s\n",
      "300:\tlearn: 0.9114555\ttest: 0.9982000\tbest: 0.9982000 (300)\ttotal: 1m 21s\tremaining: 7m 42s\n",
      "400:\tlearn: 0.8822375\ttest: 0.9932323\tbest: 0.9932323 (400)\ttotal: 1m 49s\tremaining: 7m 16s\n",
      "500:\tlearn: 0.8599618\ttest: 0.9899061\tbest: 0.9899061 (499)\ttotal: 2m 17s\tremaining: 6m 49s\n",
      "600:\tlearn: 0.8415861\ttest: 0.9887349\tbest: 0.9886649 (594)\ttotal: 2m 37s\tremaining: 6m 5s\n",
      "700:\tlearn: 0.8237368\ttest: 0.9875893\tbest: 0.9875893 (700)\ttotal: 2m 59s\tremaining: 5m 33s\n",
      "800:\tlearn: 0.8048874\ttest: 0.9867595\tbest: 0.9867490 (799)\ttotal: 3m 22s\tremaining: 5m 2s\n",
      "900:\tlearn: 0.7861433\ttest: 0.9862449\tbest: 0.9862449 (900)\ttotal: 3m 38s\tremaining: 4m 26s\n",
      "1000:\tlearn: 0.7702217\ttest: 0.9865477\tbest: 0.9862035 (902)\ttotal: 3m 56s\tremaining: 3m 56s\n",
      "1100:\tlearn: 0.7552480\ttest: 0.9862282\tbest: 0.9861766 (1081)\ttotal: 4m 18s\tremaining: 3m 31s\n",
      "1200:\tlearn: 0.7433417\ttest: 0.9863075\tbest: 0.9861766 (1081)\ttotal: 4m 36s\tremaining: 3m 3s\n",
      "1300:\tlearn: 0.7311561\ttest: 0.9863459\tbest: 0.9861766 (1081)\ttotal: 4m 57s\tremaining: 2m 39s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9861765608\n",
      "bestIteration = 1081\n",
      "\n",
      "Shrink model to first 1082 iterations.\n",
      "Partial score of fold 2 is: 0.5913137393884114\n",
      "0:\tlearn: 1.2477153\ttest: 1.2473330\tbest: 1.2473330 (0)\ttotal: 377ms\tremaining: 12m 33s\n",
      "100:\tlearn: 0.9954803\ttest: 1.0103720\tbest: 1.0103720 (100)\ttotal: 20.4s\tremaining: 6m 23s\n",
      "200:\tlearn: 0.9479384\ttest: 0.9921740\tbest: 0.9921740 (200)\ttotal: 43.3s\tremaining: 6m 27s\n",
      "300:\tlearn: 0.9181652\ttest: 0.9860461\tbest: 0.9860461 (300)\ttotal: 1m 10s\tremaining: 6m 39s\n",
      "400:\tlearn: 0.8862329\ttest: 0.9824984\tbest: 0.9824984 (400)\ttotal: 1m 35s\tremaining: 6m 22s\n",
      "500:\tlearn: 0.8640526\ttest: 0.9805841\tbest: 0.9805841 (500)\ttotal: 2m\tremaining: 6m\n",
      "600:\tlearn: 0.8394904\ttest: 0.9794551\tbest: 0.9794551 (600)\ttotal: 2m 27s\tremaining: 5m 42s\n",
      "700:\tlearn: 0.8198412\ttest: 0.9790365\tbest: 0.9790226 (692)\ttotal: 2m 51s\tremaining: 5m 17s\n",
      "800:\tlearn: 0.8034552\ttest: 0.9790597\tbest: 0.9789720 (793)\ttotal: 3m 18s\tremaining: 4m 56s\n",
      "900:\tlearn: 0.7859721\ttest: 0.9796516\tbest: 0.9789720 (793)\ttotal: 3m 39s\tremaining: 4m 27s\n",
      "1000:\tlearn: 0.7713016\ttest: 0.9794872\tbest: 0.9789720 (793)\ttotal: 4m 2s\tremaining: 4m 2s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9789719962\n",
      "bestIteration = 793\n",
      "\n",
      "Shrink model to first 794 iterations.\n",
      "Partial score of fold 3 is: 0.6000815269409366\n",
      "0:\tlearn: 1.2476206\ttest: 1.2471783\tbest: 1.2471783 (0)\ttotal: 196ms\tremaining: 6m 32s\n",
      "100:\tlearn: 0.9917255\ttest: 1.0145255\tbest: 1.0145255 (100)\ttotal: 24.2s\tremaining: 7m 34s\n",
      "200:\tlearn: 0.9435991\ttest: 0.9952985\tbest: 0.9952985 (200)\ttotal: 46.4s\tremaining: 6m 55s\n",
      "300:\tlearn: 0.9125229\ttest: 0.9876321\tbest: 0.9876321 (300)\ttotal: 1m 15s\tremaining: 7m 3s\n",
      "400:\tlearn: 0.8814323\ttest: 0.9839952\tbest: 0.9839952 (400)\ttotal: 1m 42s\tremaining: 6m 48s\n",
      "500:\tlearn: 0.8556449\ttest: 0.9816328\tbest: 0.9816328 (500)\ttotal: 2m 6s\tremaining: 6m 18s\n",
      "600:\tlearn: 0.8363682\ttest: 0.9806121\tbest: 0.9803183 (592)\ttotal: 2m 27s\tremaining: 5m 43s\n",
      "700:\tlearn: 0.8202262\ttest: 0.9803144\tbest: 0.9802693 (686)\ttotal: 2m 48s\tremaining: 5m 12s\n",
      "800:\tlearn: 0.8033254\ttest: 0.9800067\tbest: 0.9797228 (752)\ttotal: 3m 10s\tremaining: 4m 44s\n",
      "900:\tlearn: 0.7929963\ttest: 0.9796839\tbest: 0.9796769 (896)\ttotal: 3m 32s\tremaining: 4m 18s\n",
      "1000:\tlearn: 0.7787734\ttest: 0.9792412\tbest: 0.9791845 (994)\ttotal: 3m 54s\tremaining: 3m 54s\n",
      "1100:\tlearn: 0.7657026\ttest: 0.9794260\tbest: 0.9791073 (1022)\ttotal: 4m 20s\tremaining: 3m 32s\n",
      "1200:\tlearn: 0.7541581\ttest: 0.9799693\tbest: 0.9791073 (1022)\ttotal: 4m 37s\tremaining: 3m 4s\n",
      "1300:\tlearn: 0.7408515\ttest: 0.9799932\tbest: 0.9791073 (1022)\ttotal: 4m 52s\tremaining: 2m 37s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.9791072814\n",
      "bestIteration = 1022\n",
      "\n",
      "Shrink model to first 1023 iterations.\n",
      "Partial score of fold 4 is: 0.5927337131683938\n",
      "Our oof cohen kappa score is:  0.5942438124383298\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_model = Catb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found `n_estimators` in params. Will use it instead of argument\n",
      "Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "Using categorical_feature in Dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02801\tvalid_1's rmse: 1.05628\n",
      "[200]\ttraining's rmse: 0.959353\tvalid_1's rmse: 1.01192\n",
      "[300]\ttraining's rmse: 0.923879\tvalid_1's rmse: 0.996885\n",
      "[400]\ttraining's rmse: 0.899404\tvalid_1's rmse: 0.99037\n",
      "[500]\ttraining's rmse: 0.879336\tvalid_1's rmse: 0.986673\n",
      "[600]\ttraining's rmse: 0.861837\tvalid_1's rmse: 0.984225\n",
      "[700]\ttraining's rmse: 0.846229\tvalid_1's rmse: 0.983197\n",
      "[800]\ttraining's rmse: 0.831994\tvalid_1's rmse: 0.982488\n",
      "[900]\ttraining's rmse: 0.818487\tvalid_1's rmse: 0.981512\n",
      "[1000]\ttraining's rmse: 0.805375\tvalid_1's rmse: 0.981374\n",
      "[1100]\ttraining's rmse: 0.79329\tvalid_1's rmse: 0.981369\n",
      "Early stopping, best iteration is:\n",
      "[1058]\ttraining's rmse: 0.79821\tvalid_1's rmse: 0.98127\n",
      "Partial score of fold 0 is: 0.5936398871064282\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03107\tvalid_1's rmse: 1.04745\n",
      "[200]\ttraining's rmse: 0.961839\tvalid_1's rmse: 0.998728\n",
      "[300]\ttraining's rmse: 0.926267\tvalid_1's rmse: 0.982469\n",
      "[400]\ttraining's rmse: 0.901554\tvalid_1's rmse: 0.976445\n",
      "[500]\ttraining's rmse: 0.881309\tvalid_1's rmse: 0.973956\n",
      "[600]\ttraining's rmse: 0.863872\tvalid_1's rmse: 0.972768\n",
      "[700]\ttraining's rmse: 0.848067\tvalid_1's rmse: 0.972315\n",
      "[800]\ttraining's rmse: 0.833599\tvalid_1's rmse: 0.972474\n",
      "Early stopping, best iteration is:\n",
      "[730]\ttraining's rmse: 0.843601\tvalid_1's rmse: 0.9721\n",
      "Partial score of fold 1 is: 0.6079546422942247\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03066\tvalid_1's rmse: 1.05705\n",
      "[200]\ttraining's rmse: 0.960762\tvalid_1's rmse: 1.0072\n",
      "[300]\ttraining's rmse: 0.925704\tvalid_1's rmse: 0.990919\n",
      "[400]\ttraining's rmse: 0.900832\tvalid_1's rmse: 0.98364\n",
      "[500]\ttraining's rmse: 0.880791\tvalid_1's rmse: 0.980162\n",
      "[600]\ttraining's rmse: 0.863076\tvalid_1's rmse: 0.978409\n",
      "[700]\ttraining's rmse: 0.847333\tvalid_1's rmse: 0.977206\n",
      "[800]\ttraining's rmse: 0.832822\tvalid_1's rmse: 0.976723\n",
      "[900]\ttraining's rmse: 0.81923\tvalid_1's rmse: 0.976558\n",
      "Early stopping, best iteration is:\n",
      "[838]\ttraining's rmse: 0.827631\tvalid_1's rmse: 0.976441\n",
      "Partial score of fold 2 is: 0.6004393958206315\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03225\tvalid_1's rmse: 1.05193\n",
      "[200]\ttraining's rmse: 0.96286\tvalid_1's rmse: 1.00305\n",
      "[300]\ttraining's rmse: 0.926997\tvalid_1's rmse: 0.98715\n",
      "[400]\ttraining's rmse: 0.901686\tvalid_1's rmse: 0.980583\n",
      "[500]\ttraining's rmse: 0.881661\tvalid_1's rmse: 0.978391\n",
      "[600]\ttraining's rmse: 0.863942\tvalid_1's rmse: 0.976905\n",
      "[700]\ttraining's rmse: 0.848292\tvalid_1's rmse: 0.976234\n",
      "[800]\ttraining's rmse: 0.833797\tvalid_1's rmse: 0.975473\n",
      "Early stopping, best iteration is:\n",
      "[779]\ttraining's rmse: 0.836732\tvalid_1's rmse: 0.975383\n",
      "Partial score of fold 3 is: 0.6000815269409366\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03002\tvalid_1's rmse: 1.04717\n",
      "[200]\ttraining's rmse: 0.961542\tvalid_1's rmse: 1.00032\n",
      "[300]\ttraining's rmse: 0.926417\tvalid_1's rmse: 0.985673\n",
      "[400]\ttraining's rmse: 0.90223\tvalid_1's rmse: 0.979432\n",
      "[500]\ttraining's rmse: 0.882463\tvalid_1's rmse: 0.975872\n",
      "[600]\ttraining's rmse: 0.865465\tvalid_1's rmse: 0.974035\n",
      "[700]\ttraining's rmse: 0.849942\tvalid_1's rmse: 0.973008\n",
      "[800]\ttraining's rmse: 0.835832\tvalid_1's rmse: 0.97202\n",
      "[900]\ttraining's rmse: 0.822609\tvalid_1's rmse: 0.97152\n",
      "[1000]\ttraining's rmse: 0.810246\tvalid_1's rmse: 0.971178\n",
      "[1100]\ttraining's rmse: 0.798405\tvalid_1's rmse: 0.971265\n",
      "Early stopping, best iteration is:\n",
      "[1062]\ttraining's rmse: 0.802773\tvalid_1's rmse: 0.970926\n",
      "Partial score of fold 4 is: 0.598282056342105\n",
      "Our oof cohen kappa score is:  0.599111311909829\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# solve: LightGBMError: Do not support special JSON characters in feature name.\n",
    "reduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\n",
    "ajusted_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in ajusted_test.columns]\n",
    "features = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in features]\n",
    "\n",
    "lgb_model = Lgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 390,  105,  119,   73,  107,  148,  136,  193,   28,  275,   88,\n",
       "         40,   42,   47,   35,   55,   57,   41,   60,  331,   88,  478,\n",
       "        443,  372,  120,  149,  224,  305,   49,  278,  215,   68,  304,\n",
       "         21,  107,  264,  581,   66,   20,   43,  113,   76,   31,   97,\n",
       "        190,   44,   69,   39,  100,   24,   41,  124,   35,    0,    1,\n",
       "         31,   32,    2,   94,   18,   35,  151,   70,   68,   53,   21,\n",
       "         12,   78,    9,   41,  177,   35,  163,   23,   70,  125,   10,\n",
       "          3,   68,   29,   78,    1,  146,   43,   39,   69,   35,   53,\n",
       "         51,   80,   58,   64,   53,   79,   17,    8,   50,   10,   59,\n",
       "         66,    1,    4,   28,    2,   21,   33,  162,   43,   65,    2,\n",
       "         32,  278,   60,  124,   36,   20,  163,   14,   21,   19,   11,\n",
       "          9,   58,   71,   22,   97,   81,   66,   44,   65,   47,   21,\n",
       "        106,   71,   50,   25,   24,   31,  109,   44,   10,   38,   46,\n",
       "         34,   36,    9,   50,   25,   44,   54,   24,   42,  213,   20,\n",
       "          5,   22,    0,   37,   30,    9,   37,    0,  123,   20,   96,\n",
       "          5,   22,   26,   55,   11,   34,   33,   36,  190,   28,   23,\n",
       "          0,   31,   63,   70,   38,   63,   10,   22,   25,   50,  114,\n",
       "         46,   76,   34,   12,  131,   41,   61,  105,    6,   61,   18,\n",
       "         47,   12,   12,   25,   54,   19,   10,   59,  174,  119,  209,\n",
       "         63,    1,   63,   67,  201,   17,   10,    0,   26,   78,   47,\n",
       "         85,  105,  118,    8,   54,   52,  129,   48,    8,   21,  139,\n",
       "         43,   95,   11,    8,   89,    5,   63,   48,    8,  103,   90,\n",
       "          8,   64,    5,   27,    7,   52,   83,    3,   42,    9,   23,\n",
       "        159,   13,   13,    7,   14,   14,   92,   27,   88,    0,   47,\n",
       "         98,   32,  106,  108,    0,   41,  146,    5,   36,   70,   17,\n",
       "          9,   53,   11,    2,   33,    5,   47,   15,   62,  158,   13,\n",
       "         30,  102,    7,   12,   19,   11,   25,   31,    0,   45,   61,\n",
       "         18,    9,   12,    4,   89,   12,   58,    0,    5,   46,    7,\n",
       "         40,   81,   81,   37,   76,   67,   98,   55,  103,   68,   40,\n",
       "         49,   39,   31,   62,   53,  115,   40,   48,  140,   32,   74,\n",
       "         79,   41,   22,   50,   39,   85,   36,   47,   49,   63,   73,\n",
       "        106,   74,   58,   46,   15,  118,   67,   43,   75,  104,   92,\n",
       "        122,   77,   84,   83,   74,   67,  188,   58,   92,  158,   84,\n",
       "         77,  133,   50,   69,  140,  145,   56,   48,   77,  133,  259,\n",
       "        232,  940,   56,  201,  449,  346,  484,  438,  468,  539,   93,\n",
       "         25,   28,   51,  496,  203, 1049,  317,  480], dtype=int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.model.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xgb_model = Xgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model = Cnn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_model = Nn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking\n",
    "# stacker = LinearRegression()\n",
    "# S_train = np.concatenate(\n",
    "#     (lgb_model.oof_pred.reshape(-1,1), \\\n",
    "#      xgb_model.oof_pred.reshape(-1,1), \\\n",
    "#      cat_model.oof_pred.reshape(-1,1)),\\\n",
    "#     axis=1)\n",
    "# S_test = np.concatenate(\n",
    "#     (lgb_model.y_pred.reshape(-1,1), \\\n",
    "#      xgb_model.y_pred.reshape(-1,1), \\\n",
    "#      cat_model.y_pred.reshape(-1,1)),\\\n",
    "#     axis=1)\n",
    "# stacker.fit(S_train, reduce_train['accuracy_group'])\n",
    "# final_pred = stacker.predict(S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# weights = {'lgb': 0.60, 'cat': 0.20, 'xgb': 0.20, 'nn': 0.20}\n",
    "\n",
    "# final_pred = (lgb_model.y_pred * weights['lgb']) + (xgb_model.y_pred * weights['xgb']) + (cat_model.y_pred * weights['cat'])\n",
    "# print(final_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = lgb_model.y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.2515291496653005, 1: 1.6753615239825623, 2: 1.8973574885382039}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    0.500\n",
       "0    0.239\n",
       "1    0.136\n",
       "2    0.125\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "reduce_train['accuracy_group'].hist()\n",
    "\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(final_pred, acum * 100)\n",
    "print(bound)\n",
    "\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "final_pred = np.array(list(map(classify, final_pred)))\n",
    "\n",
    "sample_submission['accuracy_group'] = final_pred.astype(int)\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission['accuracy_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
